OOS_params$nFac <- 5
# Number of target lags
OOS_params$lagY <- 2
# Number of regressors lags (factors included)
OOS_params$lagX <- 2
# Create MARX
OOS_params$lagMARX <- NA
# Number of folds for CV
OOS_params$nfolds <- 5
# How many quarters between hyperparameters CV (in quarters)
OOS_params$reEstimate <- 20 # each 5 years
# Which models to used ? Possible choice c("AR, BIC", "ARDI, BIC","LASSO","RIDGE","ELASTIC-NET","RF","GBM","NN,"AR-RF")
OOS_params$model_list <- c("AR, BIC","AR-RF") #"ARDI, BIC","LASSO","RIDGE","RF","GBM","NN"
# Folder name in 50_results
OOS_params$save_path = "demo_v2"
# Elastic - Net hyperparameters (CV)
OOS_params$EN_hyps <- list(alpha_range = round(seq(0.01,0.99, length = 100),4))
# Boosting hyperparameters (CV)
OOS_params$Boosting_hyps <- list(man_grid = expand.grid(n.trees = c(seq(25, 700, by = 100)),
interaction.depth = c(3,5),
shrinkage = c(0.01),
n.minobsinnode = c(10)),
fitControl = trainControl(method = "cv",
number = OOS_params$nfolds,
search = "grid"))
# Random Forest hyperparameters
OOS_params$RF_hyps <- list(num.trees = 500,
min.node.size = 3,
mtry = 1/3)
# Macro Random Forest hyperparamaters
OOS_params$MacroRF_hyps <- list(x_pos = c(2,3),
B = 20,
mtry_frac = 0.15,
minsize = 15,
block_size = 8)
# Neural network hyperparameters
OOS_params$nn_hyps <- list(n_features=NA,
nodes=rep(100,5),      # same number of nodes in every layers
patience=10,           # Return the best model
epochs=100,
lr=0.001,
tol=0.01,
show_train=3,          # 1=show each bootstrap loss, 2=progress bar, 3+=show nothing
num_average=5,
dropout_rate=0.2,
sampling_rate = 0.75,
batch_size = 32,
num_batches = NA)
# Create all possible of targets and horizons
combn <- list(var = c(1:length(OOS_params$targetName)),
hor = OOS_params$horizon)
all_options <- expand.grid(combn)
all_options <- all_options[order(all_options$var,all_options$hor, decreasing = F),]
rownames(all_options) <- c()
# Choice of variable and horizon
var <- 1
hor <- 4
results <- process_results(paths,OOS_params = OOS_params, benchmark = "AR, BIC") # To use plain MSE put benchmark = NA
# Show MSE ratio
round(results$mse_table,3)
round(results$mse_table_2019,3)
# MSE ratio barplots and predictions plots (the plots are saved in 20_Figures)
mse_barplot_h1 <- list()
mse_barplot_h4 <- list()
pred_plot_h1 <- list()
pred_plot_h4 <- list()
for(var in 1:dim(results$mse_table)[3]) {
# MSE
mse_barplot_h1[[var]] <- quick_barplot(results, hor = 1, var = var)
mse_barplot_h4[[var]] <- quick_barplot(results, hor = 4, var = var)
# Predictions
pred_plot_h1[[var]] <- quick_plot(results, hor = 1, var = var)
pred_plot_h4[[var]] <- quick_plot(results, hor = 4, var = var)
# Put the 2 graphs together
p <- arrangeGrob(pred_plot_h1[[var]],mse_barplot_h1[[var]],
nrow = 2, ncol = 1)
ptitle = paste0(paths$fig,"/",OOS_params$targetName[var],"_h",1,".png")
ggsave(ptitle, plot = p, dpi=72, dev='png', height=600, width=450, units="mm")
p <- arrangeGrob(pred_plot_h4[[var]],mse_barplot_h4[[var]],
nrow = 2, ncol = 1)
ptitle = paste0(paths$fig,"/",OOS_params$targetName[var],"_h",4,".png")
ggsave(ptitle, plot = p, dpi=72, dev='png', height=600, width=450, units="mm")
}
# Quick view, you need to choose the postion of the target you want to see
# targets order : (1) "CPIAUCSL", (2) "UNRATE", (3) "HOUST", (4) "PAYEMS", (5) "GDPC1"
mse_barplot_h1[[1]]
pred_plot_h1[[1]]
# Clear all
rm(list = ls())
set.seed(1234)
# Set paths
path <- 'C:/Users/avalder/OneDrive - WU Wien/Documents/Study/SoSe_24/Statistical Learning/assignments/StatL_5454/project/empirical/'
setwd(path)
paths <- list(pro = "00_prog",
dat = "10_data",
fig = "20_figures",
tab = "30_tables",
too = "40_tools",
rst = "50_results")
# Install needed packages
myPKGs <- c('torch', 'pracma','glmnet','ranger',
'ggplot2','reshape2', 'stringr','MacroRF',
'caret','doParallel','gridExtra','gbm')
InstalledPKGs <- names(installed.packages()[,'Package'])
InstallThesePKGs <- myPKGs[!myPKGs %in% InstalledPKGs]
if (length(InstallThesePKGs) > 0) install.packages(InstallThesePKGs, repos = "http://cran.us.r-project.org")
# Load libraries
library(torch)      # Neural Networks
library(glmnet)     # Lasso, Ridge and Elastic-Net
library(ranger)     # Random Forest
library(MacroRF)    # Macro Random Forest
library(caret)      # Gradient boosting machine (GBM)
library(ggplot2)    # Graphs
library(reshape2)   # Data manipulation
library(gridExtra)  # To organize results graphs
library(pracma)     # Utilities (for PCA)
library(stringr)    # String manipulation
library(doParallel) # Parallel estimation
library(foreach)    # Parallel estimation
# Load US data retreiver
source(paste(paths$too, 'MakeDataUK_function.R', sep='/'))
# Factor analysis (PCA)
source(paste(paths$too, 'EM_sw.R', sep='/'))
source(paste(paths$too, 'factor.R', sep='/'))
source(paste(paths$too, 'ICp2.R', sep='/'))
# Neural Network function
source(paste(paths$too, 'MLP_function_v6b.R', sep='/'))
# Out-of-sample functions
source(paste(paths$pro, 'OOS_models.R', sep='/'))
# Number of CPU for the estimation (if you don't want to use the parallel setting : ncores <- NA)
ncores <- 4
torch_set_num_threads(1)
## OOS Parameters ----------------------------------------------------------------
OOS_params <- list()
OOS_params$targetName <- c("IOP_PROD","UNEMP_RATE","CPI_ALL")[1]
# Change the transformation code of the target, "NA" to keep FRED's code
OOS_params$target_tcode <- c(5,2,5,NA,NA)
# Forecasting horizons (in quarter), month here
OOS_params$horizon <- c(3,12)
# Out-of-sample starting date
OOS_params$OOS_starting_date <- "2008-01-01"
# Number of FRED's factors
OOS_params$nFac <- 5
# Number of target lags
OOS_params$lagY <- 24
# Number of regressors lags (factors included)
OOS_params$lagX <- 24
# Create MARX
OOS_params$lagMARX <- NA
# Number of folds for CV
OOS_params$nfolds <- 5
# How many quarters between hyperparameters CV (in quarters)
OOS_params$reEstimate <- 20 # each 5 years
OOS_params$model_list <- c("AR, BIC","AR-RF")
# Folder name in 50_results
OOS_params$save_path = "demo"
# Elastic - Net hyperparameters (CV)
OOS_params$EN_hyps <- list(alpha_range = round(seq(0.01,0.99, length = 100),4))
# Boosting hyperparameters (CV)
OOS_params$Boosting_hyps <- list(man_grid = expand.grid(n.trees = c(seq(25, 700, by = 100)),
interaction.depth = c(3,5),
shrinkage = c(0.01),
n.minobsinnode = c(10)),
fitControl = trainControl(method = "cv",
number = OOS_params$nfolds,
search = "grid"))
# Random Forest hyperparameters
OOS_params$RF_hyps <- list(num.trees = 500,
min.node.size = 3,
mtry = 1/3)
# Create all possible of targets and horizons
combn <- list(var = c(1:length(OOS_params$targetName)),
hor = OOS_params$horizon)
all_options <- expand.grid(combn)
all_options <- all_options[order(all_options$var,all_options$hor, decreasing = F),]
rownames(all_options) <- c()
# Get the data
UKdata <- MakeDataUK(path = paste0(path,paths$dat,"/"), targetName = OOS_params$targetName[var], h = hor,
nFac = OOS_params$nFac, lag_y = OOS_params$lagY, lag_f = OOS_params$lagX, lag_marx = OOS_params$lagMARX,
versionName = "current",
download = F, EM_algorithm=T, EM_last_date=NA,
frequency = 1, target_new_tcode=OOS_params$target_tcode[var])
OOS_params$targetName[var]
View(OOS_params)
# Get the data
UKdata <- MakeDataUK(path = paste0(path,paths$dat,"/"), targetName = OOS_params$targetName, h = hor,
nFac = OOS_params$nFac, lag_y = OOS_params$lagY, lag_f = OOS_params$lagX, lag_marx = OOS_params$lagMARX,
versionName = "current",
download = F, EM_algorithm=T, EM_last_date=NA,
frequency = 1, target_new_tcode=OOS_params$target_tcode[var])
# Target names from FRED DB
OOS_params$targetName <- c("IOP_PROD","UNEMP_RATE","CPI_ALL")[1]
# Change the transformation code of the target, "NA" to keep FRED's code
OOS_params$target_tcode <- c(5,5)
# Forecasting horizons (in quarter)
OOS_params$horizon <- c(1,4)
# Out-of-sample starting date
OOS_params$OOS_starting_date <- "2015-03-01"
# Number of FRED's factors
OOS_params$nFac <- 5
# Number of target lags
OOS_params$lagY <- 2
# Number of regressors lags (factors included)
OOS_params$lagX <- 2
# Create MARX
OOS_params$lagMARX <- NA
# Number of folds for CV
OOS_params$nfolds <- 5
# How many quarters between hyperparameters CV (in quarters)
OOS_params$reEstimate <- 20 # each 5 years
# Which models to used ? Possible choice c("AR, BIC", "ARDI, BIC","LASSO","RIDGE","ELASTIC-NET","RF","GBM","NN,"AR-RF")
OOS_params$model_list <- c("AR, BIC","AR-RF") #"ARDI, BIC","LASSO","RIDGE","RF","GBM","NN"
# Folder name in 50_results
OOS_params$save_path = "demo_v2"
# Clear all
rm(list = ls())
set.seed(1234)
# Set paths
path <- 'C:/Users/avalder/OneDrive - WU Wien/Documents/Study/SoSe_24/Statistical Learning/assignments/StatL_5454/project/empirical/'
setwd(path)
paths <- list(pro = "00_prog",
dat = "10_data",
fig = "20_figures",
tab = "30_tables",
too = "40_tools",
rst = "50_results")
# Install needed packages
myPKGs <- c('torch', 'pracma','glmnet','ranger',
'ggplot2','reshape2', 'stringr','MacroRF',
'caret','doParallel','gridExtra','gbm')
InstalledPKGs <- names(installed.packages()[,'Package'])
InstallThesePKGs <- myPKGs[!myPKGs %in% InstalledPKGs]
if (length(InstallThesePKGs) > 0) install.packages(InstallThesePKGs, repos = "http://cran.us.r-project.org")
# Load libraries
library(torch)      # Neural Networks
library(glmnet)     # Lasso, Ridge and Elastic-Net
library(ranger)     # Random Forest
library(MacroRF)    # Macro Random Forest
library(caret)      # Gradient boosting machine (GBM)
library(ggplot2)    # Graphs
library(reshape2)   # Data manipulation
library(gridExtra)  # To organize results graphs
library(pracma)     # Utilities (for PCA)
library(stringr)    # String manipulation
library(doParallel) # Parallel estimation
library(foreach)    # Parallel estimation
# Load US data retreiver
source(paste(paths$too, 'MakeDataUK_function.R', sep='/'))
# Factor analysis (PCA)
source(paste(paths$too, 'EM_sw.R', sep='/'))
source(paste(paths$too, 'factor.R', sep='/'))
source(paste(paths$too, 'ICp2.R', sep='/'))
# Neural Network function
source(paste(paths$too, 'MLP_function_v6b.R', sep='/'))
# Out-of-sample functions
source(paste(paths$pro, 'OOS_models.R', sep='/'))
# Number of CPU for the estimation (if you don't want to use the parallel setting : ncores <- NA)
ncores <- 4
torch_set_num_threads(1)
## OOS Parameters ----------------------------------------------------------------
OOS_params <- list()
# Target names from FRED DB
# OOS_params$targetName <- c("CPIAUCSL","UNRATE",
#                            "HOUST", "PAYEMS",
#                            "GDPC1")
OOS_params$targetName <- c("IOP_PROD","UNEMP_RATE","CPI_ALL")[1]
# Change the transformation code of the target, "NA" to keep FRED's code
OOS_params$target_tcode <- c(5,2,NA) # not really needed here since we use the balanced UK data set
# Forecasting horizons (in quarter), month here
OOS_params$horizon <- c(3,12)
# Out-of-sample starting date
OOS_params$OOS_starting_date <- "2008-01-01"
# Number of FRED's factors
OOS_params$nFac <- 5
# Number of target lags
OOS_params$lagY <- 24
# Number of regressors lags (factors included)
OOS_params$lagX <- 24
# Create MARX
OOS_params$lagMARX <- NA
# Number of folds for CV
OOS_params$nfolds <- 5
# How many quarters between hyperparameters CV (in quarters)
OOS_params$reEstimate <- 20 # each 5 years
OOS_params$model_list <- c("AR, BIC","AR-RF")
# Folder name in 50_results
OOS_params$save_path = "demo"
# Elastic - Net hyperparameters (CV)
OOS_params$EN_hyps <- list(alpha_range = round(seq(0.01,0.99, length = 100),4))
# Boosting hyperparameters (CV)
OOS_params$Boosting_hyps <- list(man_grid = expand.grid(n.trees = c(seq(25, 700, by = 100)),
interaction.depth = c(3,5),
shrinkage = c(0.01),
n.minobsinnode = c(10)),
fitControl = trainControl(method = "cv",
number = OOS_params$nfolds,
search = "grid"))
# Random Forest hyperparameters
OOS_params$RF_hyps <- list(num.trees = 500,
min.node.size = 3,
mtry = 1/3)
# Macro Random Forest hyperparamaters
OOS_params$MacroRF_hyps <- list(x_pos = c(2,3), ####????
B = 20,
mtry_frac = 0.15,
minsize = 15,
block_size = 8) # block size is 24 in monthly i.e. 2 years
# Neural network hyperparameters
OOS_params$nn_hyps <- list(n_features=NA,
nodes=rep(100,5),      # same number of nodes in every layers
patience=10,           # Return the best model
epochs=100,
lr=0.001,
tol=0.01,
show_train=3,          # 1=show each bootstrap loss, 2=progress bar, 3+=show nothing
num_average=5,
dropout_rate=0.2,
sampling_rate = 0.75,
batch_size = 32,
num_batches = NA)
# Create all possible of targets and horizons
combn <- list(var = c(1:length(OOS_params$targetName)),
hor = OOS_params$horizon)
all_options <- expand.grid(combn)
all_options <- all_options[order(all_options$var,all_options$hor, decreasing = F),]
rownames(all_options) <- c()
# Choice of variable and horizon
var <- 1
hor <- 4
# Get the data
UKdata <- MakeDataUK(path = paste0(path,paths$dat,"/"), targetName = OOS_params$targetName[var], h = hor,
nFac = OOS_params$nFac, lag_y = OOS_params$lagY, lag_f = OOS_params$lagX, lag_marx = OOS_params$lagMARX,
versionName = "current",
download = F, EM_algorithm=T, EM_last_date=NA,
frequency = 1, target_new_tcode=OOS_params$target_tcode[var])
data <- UKdata[[1]]$lagged_data
data <- as.data.frame(data)
View(data)
# Parameters
H_max <- c(max(all_options$hor))
H <- unique(all_options$hor)
nfolds <- OOS_params$nfolds
reEstimate <- OOS_params$reEstimate
OOS_date <- OOS_params$OOS_starting_date
model_list <- OOS_params$model_list
lagY <- OOS_params$lagY
lagX <- OOS_params$lagX
nfac <- OOS_params$nFac
# Out of sample start
OOS <- nrow(data) - which(rownames(data) == OOS_date) + 1
OOS_date
# Out of sample start
OOS <- nrow(data) - which(rownames(data) == OOS_date) + 1
# Elastic - Net hyperparameters (CV)
EN_hyps <<- OOS_params$EN_hyps
# Random Forest hyperparameters
RF_hyps <<- OOS_params$RF_hyps
# GBM hyperparameters (CV)
Boosting_hyps <<- OOS_params$Boosting_hyps
# Neural network hyperparameters
nn_hyps <<- OOS_params$nn_hyps
# Macro Random Forest hyperparameters
MacroRF_hyps <<- OOS_params$MacroRF_hyps
prediction_oos <- array(data = NA, dim = c(OOS,length(unique(all_options$var)),H_max,length(model_list)))
rownames(prediction_oos) <- rownames(data)[c((length(rownames(data))-OOS+1):nrow(data))]
dimnames(prediction_oos)[[2]] <- OOS_params$targetName
dimnames(prediction_oos)[[3]] <- paste0("H",1:H_max)
dimnames(prediction_oos)[[4]] <- model_list
err_oos <-  array(data = NA, dim = c(OOS,length(unique(all_options$var)),H_max,length(model_list)))
rownames(err_oos) <- rownames(data)[c((length(rownames(data))-OOS+1):nrow(data))]
dimnames(err_oos)[[2]] <-  OOS_params$targetName
dimnames(err_oos)[[3]] <- paste0("H",1:H_max)
dimnames(err_oos)[[4]] <- model_list
Forecast_all
# Clear all
rm(list = ls())
set.seed(1234)
# Set paths
path <- 'C:/Users/avalder/OneDrive - WU Wien/Documents/Study/SoSe_24/Statistical Learning/assignments/StatL_5454/project/empirical/'
setwd(path)
paths <- list(pro = "00_prog",
dat = "10_data",
fig = "20_figures",
tab = "30_tables",
too = "40_tools",
rst = "50_results")
# Install needed packages
myPKGs <- c('torch', 'pracma','glmnet','ranger',
'ggplot2','reshape2', 'stringr','MacroRF',
'caret','doParallel','gridExtra','gbm')
InstalledPKGs <- names(installed.packages()[,'Package'])
InstallThesePKGs <- myPKGs[!myPKGs %in% InstalledPKGs]
if (length(InstallThesePKGs) > 0) install.packages(InstallThesePKGs, repos = "http://cran.us.r-project.org")
# Load libraries
library(torch)      # Neural Networks
library(glmnet)     # Lasso, Ridge and Elastic-Net
library(ranger)     # Random Forest
library(MacroRF)    # Macro Random Forest
library(caret)      # Gradient boosting machine (GBM)
library(ggplot2)    # Graphs
library(reshape2)   # Data manipulation
library(gridExtra)  # To organize results graphs
library(pracma)     # Utilities (for PCA)
library(stringr)    # String manipulation
library(doParallel) # Parallel estimation
library(foreach)    # Parallel estimation
# Load US data retreiver
source(paste(paths$too, 'MakeDataUK_function.R', sep='/'))
# Factor analysis (PCA)
source(paste(paths$too, 'EM_sw.R', sep='/'))
source(paste(paths$too, 'factor.R', sep='/'))
source(paste(paths$too, 'ICp2.R', sep='/'))
# Neural Network function
source(paste(paths$too, 'MLP_function_v6b.R', sep='/'))
# Out-of-sample functions
source(paste(paths$pro, 'OOS_models.R', sep='/'))
# Number of CPU for the estimation (if you don't want to use the parallel setting : ncores <- NA)
ncores <- 4
torch_set_num_threads(1)
## OOS Parameters ----------------------------------------------------------------
OOS_params <- list()
# Target names from FRED DB
# OOS_params$targetName <- c("CPIAUCSL","UNRATE",
#                            "HOUST", "PAYEMS",
#                            "GDPC1")
OOS_params$targetName <- c("IOP_PROD","UNEMP_RATE","CPI_ALL")[1]
# Change the transformation code of the target, "NA" to keep FRED's code
OOS_params$target_tcode <- c(5,2,NA) # not really needed here since we use the balanced UK data set
OOS_params$targetName
# Change the transformation code of the target, "NA" to keep FRED's code
OOS_params$target_tcode <- c(5,2,NA) # not really needed here since we use the balanced UK data set
# Forecasting horizons (in quarter), month here
OOS_params$horizon <- c(3,12)
# Out-of-sample starting date
OOS_params$OOS_starting_date <- "2008-01-01"
# Number of FRED's factors
OOS_params$nFac <- 5
# Number of target lags
OOS_params$lagY <- 24
# Number of regressors lags (factors included)
OOS_params$lagX <- 24
# Create MARX
OOS_params$lagMARX <- NA
# Number of folds for CV
OOS_params$nfolds <- 5
# How many quarters between hyperparameters CV (in quarters)
OOS_params$reEstimate <- 20 # each 5 years
OOS_params$model_list <- c("AR, BIC","AR-RF")
# Folder name in 50_results
OOS_params$save_path = "demo"
# Folder name in 50_results
OOS_params$save_path = "demo_v2"
# Elastic - Net hyperparameters (CV)
OOS_params$EN_hyps <- list(alpha_range = round(seq(0.01,0.99, length = 100),4))
# Boosting hyperparameters (CV)
OOS_params$Boosting_hyps <- list(man_grid = expand.grid(n.trees = c(seq(25, 700, by = 100)),
interaction.depth = c(3,5),
shrinkage = c(0.01),
n.minobsinnode = c(10)),
fitControl = trainControl(method = "cv",
number = OOS_params$nfolds,
search = "grid"))
# Random Forest hyperparameters
OOS_params$RF_hyps <- list(num.trees = 500,
min.node.size = 3,
mtry = 1/3)
# Macro Random Forest hyperparamaters
OOS_params$MacroRF_hyps <- list(x_pos = c(2,3),
B = 20,
mtry_frac = 0.15,
minsize = 15,
block_size = 8)
# Neural network hyperparameters
OOS_params$nn_hyps <- list(n_features=NA,
nodes=rep(100,5),      # same number of nodes in every layers
patience=10,           # Return the best model
epochs=100,
lr=0.001,
tol=0.01,
show_train=3,          # 1=show each bootstrap loss, 2=progress bar, 3+=show nothing
num_average=5,
dropout_rate=0.2,
sampling_rate = 0.75,
batch_size = 32,
num_batches = NA)
# Create all possible of targets and horizons
combn <- list(var = c(1:length(OOS_params$targetName)),
hor = OOS_params$horizon)
all_options <- expand.grid(combn)
all_options <- all_options[order(all_options$var,all_options$hor, decreasing = F),]
rownames(all_options) <- c()
# Choice of variable and horizon
var <- 1
hor <- 4
# Get the data
UKdata <- MakeDataUK(path = paste0(path,paths$dat,"/"), targetName = OOS_params$targetName[var], h = hor,
nFac = OOS_params$nFac, lag_y = OOS_params$lagY, lag_f = OOS_params$lagX, lag_marx = OOS_params$lagMARX,
versionName = "current",
download = F, EM_algorithm=T, EM_last_date=NA,
frequency = 1, target_new_tcode=OOS_params$target_tcode[var])
OOS_params$lagX
View(make_reg_matrix)
lag_y
install.packages("dplyr")
install.packages("dplyr")
library(dplyr)
# Assuming your data frame is named 'data'
selected_data <- data %>%
select(2:258, contains("_F_UK"))
# Assuming your data frame is named 'data'
selected_data <- data %>%
select(2:258, contains("_F_UK"))
