source(paste(paths$too, 'factor.R', sep='/'))
source(paste(paths$too, 'ICp2.R', sep='/'))
# Neural Network function
source(paste(paths$too, 'MLP_function_v6b.R', sep='/'))
# Out-of-sample functions
source(paste(paths$pro, 'OOS_models.R', sep='/'))
# Number of CPU for the estimation (if you don't want to use the parallel setting : ncores <- NA)
ncores <- 4
torch_set_num_threads(1)
## OOS Parameters ----------------------------------------------------------------
OOS_params <- list()
# Target names from FRED DB
# OOS_params$targetName <- c("CPIAUCSL","UNRATE",
#                            "HOUST", "PAYEMS",
#                            "GDPC1")
OOS_params$targetName <- c("CPI_ALL")
# Change the transformation code of the target, "NA" to keep FRED's code
OOS_params$target_tcode <- c(NA) # not really needed here since we use the balanced UK data set
# Forecasting horizons (in quarter), month here
OOS_params$horizon <- c(3,12)
# Out-of-sample starting date
OOS_params$OOS_starting_date <- "2008-01-01"
# Number of FRED's factors
OOS_params$nFac <- 5
# Number of target lags
OOS_params$lagY <- 24
# Number of regressors lags (factors included)
OOS_params$lagX <- 24
# Create MARX
OOS_params$lagMARX <- NA
# Number of folds for CV
OOS_params$nfolds <- 5
# How many quarters between hyperparameters CV (in quarters)
OOS_params$reEstimate <- 60 # each 5 years
OOS_params$model_list <- c("AR, BIC","AR-RF","RF-MAF","FA-ARRF") #RF_MAF, RF, FA-ARRF
# Folder name in 50_results
OOS_params$save_path = "demo"
# Random Forest a) hyperparameters
OOS_params$RF_hyps <- list(num.trees = 500,
min.node.size = 3,
mtry = 1/3)
# Random Forest b) hyperparameters
OOS_params$RF_MAF_hyps <- list(num.trees = 500,
min.node.size = 3,
mtry = 1/3)
# Macro Random Forest hyperparamaters
OOS_params$MacroRF_hyps <- list(x_pos = c(2,3,4,5,6,7), #### für den ARRF & month lags i.e. 2 Quarter before now monthly
B = 20,
mtry_frac = 0.15,
minsize = 15,
block_size = 24) # block size is 24 in monthly i.e. 2 years
# Macro Random Forest hyperparamaters
OOS_params$FA_MacroRF_hyps <- list(x_pos = c(2,3,4,5,6,7,26,27), #### für den ARRF & month lags i.e. 2 Quarter before now monthly
B = 20,
mtry_frac = 0.15,
minsize = 15,
block_size = 24) # block size is 24 in monthly i.e. 2 years
# Create all possible of targets and horizons
combn <- list(var = c(1:length(OOS_params$targetName)),
hor = OOS_params$horizon)
all_options <- expand.grid(combn)
all_options <- all_options[order(all_options$var,all_options$hor, decreasing = F),]
rownames(all_options) <- c()
start <- Sys.time()
# Parallel estimation
if(!is.na(ncores)) {
# Start parallel clustering
cl <- makeCluster(ncores)
registerDoParallel(cl) # Shows the number of Parallel Workers to be used
foreach(i=c(1:nrow(all_options))) %dopar% Forecast_all(it_pos = i,
all_options = all_options,
paths = paths,
OOS_params = OOS_params,
seed = 124)
stopImplicitCluster()
# Single core estimation
} else{
for (i in 1:nrow(all_options)) {
Forecast_all(it_pos = i,
all_options = all_options,
paths = paths,
OOS_params = OOS_params,
seed = 124)
}
}
set.seed(seed)
# Load libraries
library(torch)      # Neural Networks
library(glmnet)     # Lasso, Ridge and Elastic-Net
library(ranger)     # Random Forest
library(MacroRF)    # Macro Random Forest
library(caret)      # Gradient boosting machine (GBM)
library(pracma)     # Utilities (for PCA)
library(stringr)    # String manipulation
library(doParallel) # Parallel estimation
library(foreach)    # Parallel estimation
# Load US data retreiver -----------------------------
source(paste(paths$too, 'MakeDataUK_function.R', sep='/'))
# Factor analysis (PCA) ------------------------------
source(paste(paths$too, 'EM_sw.R', sep='/'))
source(paste(paths$too, 'factor.R', sep='/'))
source(paste(paths$too, 'ICp2.R', sep='/'))
# Neural Network function ----------------------------
source(paste(paths$too, 'MLP_function_v6b.R', sep='/'))
torch_set_num_threads(1)
# Variable and Horizon to forecast #############################################################
# ==============================================================================================
#it_pos = 1
var <- all_options$var[it_pos]
hor <- all_options$hor[it_pos]
# Get the data
UKdata <- MakeDataUK(path = paste0(path,paths$dat,"/"), targetName = OOS_params$targetName[var], h = hor,
nFac = OOS_params$nFac, lag_y = OOS_params$lagY, lag_f = OOS_params$lagX, lag_marx = OOS_params$lagMARX,
versionName = "current",
download = F, EM_algorithm=F, EM_last_date=NA,
frequency = 1, target_new_tcode=OOS_params$target_tcode[var])
data <- UKdata[[1]]$lagged_data
# Variable and Horizon to forecast #############################################################
# ==============================================================================================
it_pos = 1
# Variable and Horizon to forecast #############################################################
# ==============================================================================================
#it_pos = 1
var <- all_options$var[it_pos]
hor <- all_options$hor[it_pos]
# Get the data
UKdata <- MakeDataUK(path = paste0(path,paths$dat,"/"), targetName = OOS_params$targetName[var], h = hor,
nFac = OOS_params$nFac, lag_y = OOS_params$lagY, lag_f = OOS_params$lagX, lag_marx = OOS_params$lagMARX,
versionName = "current",
download = F, EM_algorithm=F, EM_last_date=NA,
frequency = 1, target_new_tcode=OOS_params$target_tcode[var])
data <- UKdata[[1]]$lagged_data
# Parameters
H_max <- c(max(all_options$hor))
H <- unique(all_options$hor)
nfolds <- OOS_params$nfolds
reEstimate <- OOS_params$reEstimate
OOS_date <- OOS_params$OOS_starting_date
model_list <- OOS_params$model_list
lagY <- OOS_params$lagY
lagX <- OOS_params$lagX
nfac <- OOS_params$nFac
# Out of sample start
OOS <- nrow(data) - which(rownames(data) == OOS_date) + 1
# Random Forest a) hyperparameters
RF_hyps <<- OOS_params$RF_hyps
# Random Forest b) hyperparameters
RF_MAF_hyps <<- OOS_params$RF_MAF_hyps
# Macro Random Forest hyperparameters
MacroRF_hyps <<- OOS_params$MacroRF_hyps
# Macro Random Forest hyperparameters
FA_MacroRF_hyps <<- OOS_params$FA_MacroRF_hyps
prediction_oos <- array(data = NA, dim = c(OOS,length(unique(all_options$var)),H_max,length(model_list)))
rownames(prediction_oos) <- rownames(data)[c((length(rownames(data))-OOS+1):nrow(data))]
dimnames(prediction_oos)[[2]] <- OOS_params$targetName
dimnames(prediction_oos)[[3]] <- paste0("H",1:H_max)
dimnames(prediction_oos)[[4]] <- model_list
err_oos <-  array(data = NA, dim = c(OOS,length(unique(all_options$var)),H_max,length(model_list)))
rownames(err_oos) <- rownames(data)[c((length(rownames(data))-OOS+1):nrow(data))]
dimnames(err_oos)[[2]] <-  OOS_params$targetName
dimnames(err_oos)[[3]] <- paste0("H",1:H_max)
dimnames(err_oos)[[4]] <- model_list
for (v in var) {
for (h in hor) {
pos <- 1
for (oos in OOS:1) {
start_estim <- Sys.time()
## Data management ------------------------------------------------------------
t <- (nrow(data)-oos+1)
train_data <- data[1:t,]
train_pos <- 1:(nrow(train_data)-h)
oos_pos <- nrow(train_data)
## Models ---------------------------------------------------------------------
### 1) AR, BIC ------
if("AR, BIC" %in% model_list) {
if(((pos-1) %% reEstimate) == 0) {
bic <- Inf
for (P in 1:(lagY+1)) {
AR <- lm(y~., data = as.data.frame(train_data[train_pos,1:(P+1)]))
if(BIC(AR)< bic) {
P_min_ar <- P
bic <- BIC(AR)
}
}
}
AR <- lm(y~., data = as.data.frame(train_data[train_pos,1:(P_min_ar+1)]))
pred <- predict.lm(AR, newdata = as.data.frame(train_data[,1:(P_min_ar+1)]))[oos_pos]
m <- which(model_list == "AR, BIC")
err_oos[pos,v,h,m] <- train_data[oos_pos,1] - pred
prediction_oos[pos,v,h,m] <- pred
}
### 2) ARDI, BIC -----
# if("ARDI, BIC" %in% model_list) {
#
#   if(((pos-1) %% reEstimate) == 0) {
#     bic <- Inf
#     for(K in 1:nfac) {
#       for(P in 0:(lagY-1)) {
#         for (M in 0:(lagX-1)) {
#
#           facName <- unlist(lapply(1:K, function(x) paste0("L",0:M,"_F_UK",x)))
#           posfac <- which(colnames(train_data) %in% facName)
#
#           lmdata <- train_data[,c(1:(P+1),posfac)]
#           rownames(lmdata) <- c()
#
#           ARDI <- lm(y~., data = as.data.frame(lmdata[train_pos,]))
#
#           if(BIC(ARDI)< bic) {
#             P_min <- P
#             M_min <- M
#             K_min <- K
#             bic <- BIC(ARDI)
#           }
#         }
#       }
#     }
#   }
#
#   facName <- unlist(lapply(1:K_min, function(x) paste0("L",0:M_min,"_F_UK",x)))
#   posfac <- which(colnames(train_data) %in% facName)
#   lmdata <- data[,c(1:(P_min+1),posfac)]
#   rownames(lmdata) <- c()
#
#   ARDI <- lm(y~., data = as.data.frame(lmdata[train_pos,]))
#   pred <- predict.lm(ARDI,newdata = as.data.frame(lmdata))[oos_pos]
#
#   m <- which(model_list == "ARDI, BIC")
#   err_oos[pos,v,h,m] <- train_data[oos_pos,1] - pred
#   prediction_oos[pos,v,h,m] <- pred
# }
#
#
### 3) LASSO ------
# if("LASSO" %in% model_list) {
#
#   newtrain <- train_data
#
#   if(((pos-1) %% reEstimate) == 0) {
#     lambda_lasso=cv.glmnet(y = newtrain[train_pos,1], x = newtrain[train_pos,-1], alpha = 1, nfolds = nfolds)$lambda.min
#   }
#   lasso=glmnet(y=newtrain[train_pos,1], x = newtrain[train_pos,-1],
#                lambda = lambda_lasso, alpha = 1)
#   pred=predict(lasso, newx = newtrain[,-1])[oos_pos]
#
#   m <- which(model_list == "LASSO")
#   err_oos[pos,v,h,m] <- newtrain[oos_pos,1] - pred
#   prediction_oos[pos,v,h,m] <- pred
# }
#
# ### 4) RIDGE -------
#
# if("RIDGE" %in% model_list) {
#
#   newtrain <- train_data
#
#   if(((pos-1) %% reEstimate) == 0) {
#     lambda_ridge=cv.glmnet(y = newtrain[train_pos,1], x = newtrain[train_pos,-1],
#                            alpha = 0, nfolds = nfolds)$lambda.min
#   }
#   lasso=glmnet(y=newtrain[train_pos,1], x = newtrain[train_pos,-1], lambda = lambda_ridge, alpha = 0)
#   pred=predict(lasso, newx = newtrain[,-1])[oos_pos]
#
#   m <- which(model_list == "RIDGE")
#   err_oos[pos,v,h,m] <- newtrain[oos_pos,1] - pred
#   prediction_oos[pos,v,h,m] <- pred
# }
#
# ### 5) Elastic-Net -------
#
# if("ELASTIC-NET" %in% model_list) {
#
#   newtrain <- train_data
#
#   alpha_range <- EN_hyps$alpha_range
#   if(((pos-1) %% reEstimate) == 0) {
#     perf_mat <- matrix(NA, 3, length(alpha_range))
#     for (i in 1:length(alpha_range)){
#       fit_cv <- cv.glmnet(y = newtrain[train_pos,1], x = newtrain[train_pos,-1], alpha=alpha_range[i], nfolds=nfolds)
#       min_cv <- min(fit_cv$cvm)
#       lambda_min <- fit_cv$lambda[which.min(fit_cv$cvm)]
#       perf_mat[1,i] <- alpha_range[i]
#       perf_mat[2,i] <- lambda_min
#       perf_mat[3,i] <- min_cv
#     }
#     best_alpha_en <- perf_mat[1,][which.min(perf_mat[3,])]
#     lambda_en <- perf_mat[2,][which.min(perf_mat[3,])]
#   }
#
#   lasso=glmnet(y=newtrain[train_pos,1], x = newtrain[train_pos,-1], lambda = lambda_en, alpha = best_alpha_en)
#   pred=predict(lasso, newx = newtrain[,-1])[oos_pos]
#
#   m <- which(model_list == "ELASTIC-NET")
#   err_oos[pos,v,h,m] <- newtrain[oos_pos,1] - pred
#   prediction_oos[pos,v,h,m] <- pred
# }
### 6a) Plain Random Forest (no linear part and Raw data (this means 8 lags of FRED-QD, after usual transformations for stationarity have been applied)-------
# we need 24 lags
if("RF" %in% model_list) {
newtrain <- data
newtrain <- train_data
col_names <- colnames(newtrain)
cols_to_keep <- !grepl('_F_UK|trend', col_names)
RF=ranger(y~., data = as.data.frame(newtrain[train_pos,cols_to_keep]), mtry = (ncol(newtrain[,-1])*RF_hyps$mtry),
num.trees = RF_hyps$num.trees, min.node.size = RF_hyps$min.node.size)
pred=predict(RF, data = as.data.frame(newtrain[,]))$predictions[oos_pos]
m <- which(model_list == "RF")
err_oos[pos,v,h,m] <- newtrain[oos_pos,1] - pred
prediction_oos[pos,v,h,m] <- pred
}
### 6b) Plain Random Forest but using S_t -------
if("RF-MAF" %in% model_list) {
newtrain <- train_data
# Find indices for columns 2 to 258
cols_2_714 <- 2:714
col_names <- colnames(newtrain)
# Find indices for columns containing '_F_UK'
cols_F_UK <- grep("_F_UK","trend", col_names)
# Combine the indices
selected_cols <- unique(c(cols_2_714, cols_F_UK))
RF=ranger(y~., data = as.data.frame(newtrain[train_pos,selected_cols]), mtry = (ncol(newtrain[,-1])*RF_hyps$mtry),
num.trees = RF_hyps$num.trees, min.node.size = RF_hyps$min.node.size)
pred=predict(RF, data = as.data.frame(newtrain[,]))$predictions[oos_pos]
m <- which(model_list == "RF_MAF")
err_oos[pos,v,h,m] <- newtrain[oos_pos,1] - pred
prediction_oos[pos,v,h,m] <- pred
}
### 7) Boosting -------
# if("GBM" %in% model_list) {
#
#   newtrain <- train_data
#   oosX = newtrain[oos_pos,-1]
#   dim(oosX) <- c(1,length(oosX))
#
#   if(((pos-1) %% reEstimate) == 0) {
#     tuned_gbm <- caret::train(y ~., data = as.matrix(newtrain[train_pos,]),
#                               method = "gbm",
#                               metric = "RMSE",
#                               trControl = Boosting_hyps$fitControl,
#                               tuneGrid = Boosting_hyps$man_grid,
#                               verbose=FALSE
#     )
#     mod <- tuned_gbm$finalModel
#   } else{
#     good_hyps <- Boosting_hyps
#     good_hyps$fitControl$method <- "none"
#     GBM <- caret::train(y ~., data = as.matrix(newtrain[train_pos,]),
#                         method = "gbm",
#                         metric = "RMSE",
#                         trControl = good_hyps$fitControl,
#                         tuneGrid = tuned_gbm$bestTune,
#                         verbose=FALSE
#     )
#     mod <- GBM$finalModel
#   }
#
#   pred <- predict(mod, oosX, n.trees = mod$n.trees)
#
#   m <- which(model_list == "GBM")
#   err_oos[pos,v,h,m] <- newtrain[oos_pos,1] - pred
#   prediction_oos[pos,v,h,m] <- pred
# }
#
### 8) NN -------
# if("NN" %in% model_list) {
#
#   newtrain <- train_data
#   rownames(newtrain) <- c()
#   nn_hyps$n_features <<- ncol(newtrain[,-1])
#
#   if(((pos-1) %% reEstimate) == 0) {
#
#     mlp <- MLP(X = newtrain[-oos_pos,-1], Y = newtrain[-oos_pos,1],
#                Xtest = newtrain[oos_pos,-1], Ytest = newtrain[oos_pos,1],
#                nn_hyps = nn_hyps,
#                standardize=T,seed=1234)
#
#   }
#
#   pred <- mean(predict_nn(mlp, newtrain[oos_pos,-1], nn_hyps))
#
#   m <- which(model_list == "NN")
#   err_oos[pos,v,h,m] <- newtrain[oos_pos,1] - pred
#   prediction_oos[pos,v,h,m] <- pred
# }
#
### 9a) Macro AR-RF -------
if("AR-RF" %in% model_list) {
newtrain <- train_data
newtrain <- as.matrix(newtrain[c(train_pos,oos_pos),])
rownames(newtrain) <- c()
# Find indices for columns 2 to 258
cols_2_714 <- 2:714
col_names <- colnames(newtrain)
# Find indices for columns containing '_F_UK'
cols_F_UK <- grep("_F_UK","trend", col_names)
# Combine the indices
selected_cols <- unique(c(cols_2_714, cols_F_UK))
if(((pos-1) %% reEstimate) == 0) {
mrf <- MRF(data=newtrain[train_pos,],
y.pos=1,
S.pos=selected_cols,
x.pos=MacroRF_hyps$x_pos,
oos.pos=c(),
minsize=MacroRF_hyps$minsize,
mtry.frac=MacroRF_hyps$mtry_frac,
min.leaf.frac.of.x=1.5,
VI=FALSE,
ERT=FALSE,
quantile.rate=0.33,
S.priority.vec=NULL,
random.x = FALSE,
howmany.random.x=1,
howmany.keep.best.VI=20,
cheap.look.at.GTVPs=FALSE,
prior.var=1/c(0.01,0.25,rep(1,length(MacroRF_hyps$x_pos)-1)),
prior.mean=coef(lm(as.matrix(newtrain[train_pos,1])~as.matrix(newtrain[train_pos,MacroRF_hyps$x_pos]))),
subsampling.rate=0.70,
rw.regul=0.95,
keep.forest=TRUE,
block.size=MacroRF_hyps$block_size,
trend.pos=ncol(newtrain),
trend.push=4,
fast.rw=TRUE,
ridge.lambda=0.5,HRW=0,B=MacroRF_hyps$B,resampling.opt=2,printb=FALSE)
}
rf_test <- as.data.frame(newtrain[,-1])
pred <- pred.given.mrf(mrf, newdata = rf_test)
m <- which(model_list == "AR-RF")
err_oos[pos,v,h,m] <- train_data[oos_pos,1] - pred[length(pred)]
prediction_oos[pos,v,h,m] <- pred[length(pred)]
}
### 9b) Macro Factor-Augmented Autoregressive RF (FA-ARRF) -------
if("FA-ARRF" %in% model_list) {
newtrain <- train_data
newtrain <- as.matrix(newtrain[c(train_pos,oos_pos),])
rownames(newtrain) <- c()
#  newtrain_df <- as.data.frame(newtrain)
#  S.pos_df <- newtrain_df %>%
#    select(2:258, contains("_F_UK"))
# #
# Find indices for columns 2 to 258
cols_2_714 <- 2:714
col_names <- colnames(newtrain)
# Find indices for columns containing '_F_UK'
cols_F_UK <- grep("_F_UK","trend", col_names)
# Combine the indices
selected_cols <- unique(c(cols_2_714, cols_F_UK))
if(((pos-1) %% reEstimate) == 0) {
mrf <- MRF(data=newtrain[train_pos,],
y.pos=1,
S.pos=selected_cols,
x.pos=MacroRF_hyps$x_pos,
oos.pos=c(),
minsize=MacroRF_hyps$minsize,
mtry.frac=MacroRF_hyps$mtry_frac,
min.leaf.frac.of.x=1.5,
VI=FALSE,
ERT=FALSE,
quantile.rate=0.33,
S.priority.vec=NULL,
random.x = FALSE,
howmany.random.x=1,
howmany.keep.best.VI=20,
cheap.look.at.GTVPs=FALSE,
prior.var=1/c(0.01,0.25,rep(1,length(MacroRF_hyps$x_pos)-1)),
prior.mean=coef(lm(as.matrix(newtrain[train_pos,1])~as.matrix(newtrain[train_pos,MacroRF_hyps$x_pos]))),
subsampling.rate=0.70,
rw.regul=0.95,
keep.forest=TRUE,
block.size=MacroRF_hyps$block_size,
trend.pos=ncol(newtrain),
trend.push=4,
fast.rw=TRUE,
ridge.lambda=0.5,HRW=0,B=MacroRF_hyps$B,resampling.opt=2,printb=FALSE)
}
rf_test <- as.data.frame(newtrain[,-1])
pred <- pred.given.mrf(mrf, newdata = rf_test)
m <- which(model_list == "FA-ARRF")
err_oos[pos,v,h,m] <- train_data[oos_pos,1] - pred[length(pred)]
prediction_oos[pos,v,h,m] <- pred[length(pred)]
}
end_estim <- Sys.time()
print(end_estim-start_estim)
# -----------------------------------------------------------------------------------------
pos <- pos + 1
gc()
} #oos
} #h
} #v
start <- Sys.time()
# Parallel estimation
if(!is.na(ncores)) {
# Start parallel clustering
cl <- makeCluster(ncores)
registerDoParallel(cl) # Shows the number of Parallel Workers to be used
foreach(i=c(1:nrow(all_options))) %dopar% Forecast_all(it_pos = i,
all_options = all_options,
paths = paths,
OOS_params = OOS_params,
seed = 124)
stopImplicitCluster()
# Single core estimation
} else{
for (i in 1:nrow(all_options)) {
Forecast_all(it_pos = i,
all_options = all_options,
paths = paths,
OOS_params = OOS_params,
seed = 124)
}
}
