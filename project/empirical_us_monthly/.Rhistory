data_rec_2016<-read.csv("~/Dropbox/Data_NowCast_Hugo/Data/Rec_500CNP_2016.csv",sep=";")
data_rec_2011$Profession <-str_trim(data_rec_2011$Profession, side = c("both"))
data_rec_2016$Profession <-str_trim(data_rec_2016$Profession, side = c("both"))
data_rec_2011$NOC_Num<-substr(data_rec_2011$Profession,1,4)
data_rec_2016$NOC_Num<-substr(data_rec_2016$Profession,1,4)
data_rec_2011<-data_rec_2011[,c(1:18,21)]
data_rec_2011_long<-gather(data_rec_2011, Region, VALUE,Quebec:Cote.Nord...Nord.du.Quebec, factor_key=FALSE)
data_rec_2016<-data_rec_2016[,c(1:18,21)]
data_rec_2016_long<-gather(data_rec_2016, Region, VALUE,Quebec:Cote.Nord...Nord.du.Quebec, factor_key=FALSE)
data_rec_2011_long<-subset(data_rec_2011_long,data_rec_2011_long$NOC_Num!="Tota")
data_rec_2016_long<-subset(data_rec_2016_long,data_rec_2016_long$NOC_Num!="Tota")
data_rec_2011_long$Year<-2011
data_rec_2016_long$Year<-2016
data_rec_2011_long$NOC2<-substr(data_rec_2011_long$NOC_Num,1,2)
data_rec_2016_long$NOC2<-substr(data_rec_2016_long$NOC_Num,1,2)
##Donnees recensement- aggrgégées au niveau 2
data_rec_2011_NOC2<-aggregate(VALUE~NOC2+Region+Year,data=data_rec_2011_long,FUN=sum)
data_rec_2016_NOC2<-aggregate(VALUE~NOC2+Region+Year,data=data_rec_2016_long,FUN=sum)
data_rec_NOC2<-rbind(data_rec_2011_NOC2,data_rec_2016_NOC2)
##Donnée monthly QC (NOC4 --> Province)
#########
#Partie 2: Obtient les données mensuelles NOC4 pour la province, puis les
#taux de croissance par NOC4, qui seront utilisés pour combler les années
#non disponible dans la BD du MTESS. Pour cela, on transforme la BD mensuelle
#en BD annuelle.
#
#########
data_monthly<-read.csv("~/Dropbox/Data_NowCast_Hugo/Data/Month_NOC4_MTESS.csv",sep=";")
data_monthly_long<-gather(data_monthly, Date, VALUE, Jan2006:Dec2022, factor_key=FALSE)
data_monthly_long$Year<-substr(data_monthly_long$Date,4,7)
data_monthly_long$month<-substr(data_monthly_long$Date,1,3)
data_monthly_long$month[data_monthly_long$month=="Jan"]<-"01"
data_monthly_long$month[data_monthly_long$month=="Feb"]<-"02"
data_monthly_long$month[data_monthly_long$month=="Mar"]<-"03"
data_monthly_long$month[data_monthly_long$month=="Apr"]<-"04"
data_monthly_long$month[data_monthly_long$month=="May"]<-"05"
data_monthly_long$month[data_monthly_long$month=="Jun"]<-"06"
data_monthly_long$month[data_monthly_long$month=="Jul"]<-"07"
data_monthly_long$month[data_monthly_long$month=="Aug"]<-"08"
data_monthly_long$month[data_monthly_long$month=="Sep"]<-"09"
data_monthly_long$month[data_monthly_long$month=="Oct"]<-"10"
data_monthly_long$month[data_monthly_long$month=="Nov"]<-"11"
data_monthly_long$month[data_monthly_long$month=="Dec"]<-"12"
data_monthly_long$Year<-as.numeric(data_monthly_long$Year)
data_monthly_long$VALUE<-as.numeric(data_monthly_long$VALUE)
data_monthly_long$OCCUPATION <-str_trim(data_monthly_long$OCCUPATION, side = c("both"))
data_monthly_long<-data_monthly_long[-c(which(data_monthly_long$NOC_lvl==33)),]
data_monthly_long<-data_monthly_long[-c(which(data_monthly_long$NOC_lvl==99)),]
data_monthly_long$NocNUM<-gsub( " .*$", "", data_monthly_long$OCCUPATION)
data_monthly_long<-data_monthly_long[-c(which(data_monthly_long$NocNUM=="Total")),]
data_monthly_long<-data_monthly_long[data_monthly_long$REGIONS=="Québec",]
data_monthly_long$VALUE[data_monthly_long$VALUE==0|is.na(data_monthly_long$VALUE)]<-1.4 # POURQUOI 1.4
data_monthly_long$Trim<-1
data_monthly_long$Trim[data_monthly_long$month=="04"|data_monthly_long$month=="05"|data_monthly_long$month=="06"]<-2
data_monthly_long$Trim[data_monthly_long$month=="07"|data_monthly_long$month=="08"|data_monthly_long$month=="09"]<-3
data_monthly_long$Trim[data_monthly_long$month=="10"|data_monthly_long$month=="11"|data_monthly_long$month=="12"]<-4
#Here, we are building a yearly database from the monthly EPA data. We are searching the total EPA estimate
#and the growth rate by NOC_4
data_monthly_year<-aggregate(VALUE ~ REGIONS + NOC_lvl + Year + NocNUM,data=data_monthly_long,FUN=mean)
data_monthly_year<-data_monthly_year[data_monthly_year$NOC_lvl==4,]
data_monthly_year<-data_monthly_year[
with(data_monthly_year, order(NocNUM, Year)),
]
data_monthly_year$growth<-NA
#Growth in EPA data
data_monthly_year$growth[2:length(data_monthly_year$growth)]<-(data_monthly_year$VALUE[2:length(data_monthly_year$growth)]-
data_monthly_year$VALUE[1:(length(data_monthly_year$growth)-1)])/data_monthly_year$VALUE[1:(length(data_monthly_year$growth)-1)]
##############################
#Partie 3: Base de données du MTESS (disponible seulement sur )
#une sous-selection d'années). 2011 est ajouté avec les données du recensement.
#Avec les taux de croissance obtenue dans la section précédente, on remplie les années
# manquantes dans la BD du MTESS entre 2011 et 2022.
#La Partie 3a (voir plus bas) créé une conversion annuelle entre NOC2 et NOC4.
# Commence le calcule pour les résultats par région
#############################
#Données MTESS (Target) -- j'ajoute aussi le recensement de 2011 au données.
data_pred_MTESS<-read.csv("~/Dropbox/Data_NowCast/Data/NOC4_Region_MTESS_Pred.csv",sep=";",
colClasses=c("NOC_4"="character"))
data_pred_MTESS$Region = NULL
names(data_pred_MTESS)[which(names(data_pred_MTESS)=="Region2")]<-"Region"
data_pred_MTESS$Region[data_pred_MTESS$Region=="ENSEMBLE DU QUÉBEC"]<-"Quebec"
data_pred_MTESS$NOC_lvl<-nchar(data_pred_MTESS$NOC_4)
for (i in 1:length(unique(sort(data_pred_MTESS$Region)))){
data_pred_MTESS$Region[data_pred_MTESS$Region==unique(sort(data_pred_MTESS$Region))[i]]<-unique(sort(data_rec_2011_long$Region))[i]
}
#Two missing data in 2018
data_pred_MTESS$X2018[is.na(data_pred_MTESS$X2018)]<-(data_pred_MTESS$X2016[is.na(data_pred_MTESS$X2018)]+data_pred_MTESS$X2020[is.na(data_pred_MTESS$X2018)])/2
# vectorise
data_pred_MTESS_long<-gather(data_pred_MTESS,Year,VALUE,X2014:X2021,factor_key=FALSE)
data_pred_MTESS_long$Year<-substr(data_pred_MTESS_long$Year,2,5)
data_pred_MTESS_long$Occupation<-NULL
data_pred_MTESS_long$Comp<-NULL
temp<-data_rec_2011_long
temp$Profession<-NULL
names(temp)[names(temp)=="NOC_Num"]<-"NOC_4"
temp$NOC2<-NULL
temp$NOC_lvl<-4
temp<-temp[,names(data_pred_MTESS_long)]
data_pred_MTESS_long<-rbind(data_pred_MTESS_long,temp) #ajouter le recensement 2011 aux données annuelles du MTESS
data_monthly_year$REGIONS<-NULL
data_monthly_year$Region<-NA
temp1<-data_monthly_year
temp1$Region<-unique(data_pred_MTESS_long$Region)[1]
# Duplique data_monthly_year pour en avoir un pour chaque région
for (i in 2:length(unique(data_pred_MTESS_long$Region))){
temp<-data_monthly_year
temp$Region<-unique(data_pred_MTESS_long$Region)[i]
temp1<-rbind(temp1,temp)
}
names(temp1)[names(temp1)=="NocNUM"]<-"NOC_4"
names(temp1)[names(temp1)=="VALUE"]<-"VALUE_Month"
data_temp_yearly<-merge(temp1,data_pred_MTESS_long,by=c("NOC_4","NOC_lvl","Year","Region"),all.x=T)
data_temp_yearly<-data_temp_yearly[
with(data_temp_yearly, order(Region,NOC_4,Year)),
]
data_temp_yearly<-data_temp_yearly[data_temp_yearly$Year>=2011,]
#Calcul les valeurs régionales vides avec la croissance annuelle que le Québec a eu dans le NOC_4
for (i in 1:nrow(data_temp_yearly)){
data_temp_yearly$estimate_1[i]<-ifelse(is.na(data_temp_yearly$VALUE[i]),
data_temp_yearly$estimate_1[i-1]*(1+data_temp_yearly$growth[i]),
data_temp_yearly$VALUE[i])
}
for (i in nrow(data_temp_yearly):1){
data_temp_yearly$estimate_2[i]<-ifelse(is.na(data_temp_yearly$VALUE[i]),
data_temp_yearly$estimate_2[i+1]/(1+data_temp_yearly$growth[i+1]),
data_temp_yearly$VALUE[i] )
}
data_temp_yearly$estimate_2[is.na(data_temp_yearly$estimate_2)]<-data_temp_yearly$estimate_1[is.na(data_temp_yearly$estimate_2)]
data_temp_yearly$estimate<- (data_temp_yearly$estimate_2+data_temp_yearly$estimate_1)/2
data_temp_yearly$estimate_1<-NULL
data_temp_yearly$estimate_2<-NULL
##############################################################3
#Partie 3a:
##Nous créons ici un facteur de conversion annuel NOC_2 --> NOC_4 pour transformer la BD trimestriel
############################################################################33
data_temp_yearly$NOC_2<-substr(data_temp_yearly$NOC_4,1,2)
data_MTESS_NOC2_temp<-aggregate(estimate~NOC_2+Region+Year,data=data_temp_yearly,sum)
names(data_MTESS_NOC2_temp)[4]<-"VALUE_NOC2"
data_temp_yearly<-merge(data_temp_yearly,data_MTESS_NOC2_temp,by=c("NOC_2","Region","Year"),all.x=T)
data_temp_yearly$conversion<-data_temp_yearly$estimate/data_temp_yearly$VALUE_NOC2
sum(data_temp_yearly[1:6,"conversion"])
data_temp_yearly<-data_temp_yearly[
with(data_temp_yearly, order(Region,NOC_4,Year)),
]
data_temp_yearly$conversion_int<-na_interpolation(data_temp_yearly$conversion, option = "linear", maxgap = Inf)
data_temp_yearly[is.na(data_temp_yearly$conversion),]
names(data_temp_yearly)[names(data_temp_yearly)=="VALUE"]<-"VALUE_MTESS"
names(data_temp_yearly)[names(data_temp_yearly)=="estimate"]<-"estimate_MTESS"
data_temp_yearly$VALUE_Month<-NULL
############################
Data_Quarterly_NocRegions<-read.csv("~/Dropbox/Data_NowCast/Data/Quarterly_NocPart_regions.csv",sep=";")
Data_Quarterly_NocRegions_long<-gather(Data_Quarterly_NocRegions, Date, VALUE, X1Q2006:X4Q2022, factor_key=FALSE)
Data_Quarterly_NocRegions_long$Trim<-paste("Q",substr(Data_Quarterly_NocRegions_long$Date,2,2),sep="")
Data_Quarterly_NocRegions_long$Year<-substr(Data_Quarterly_NocRegions_long$Date,4,7)
Data_Quarterly_NocRegions_long$OCCUPATION <-str_trim(Data_Quarterly_NocRegions_long$OCCUPATION, side = c("both"))
Data_Quarterly_NocRegions_long<-Data_Quarterly_NocRegions_long[-c(which(Data_Quarterly_NocRegions_long$Noc_Lvl==33)),]
Data_Quarterly_NocRegions_long<-Data_Quarterly_NocRegions_long[-c(which(Data_Quarterly_NocRegions_long$Noc_Lvl==99)),]
Data_Quarterly_NocRegions_long$VALUE<-as.numeric(Data_Quarterly_NocRegions_long$VALUE)
Data_Quarterly_NocRegions_long$NocNUM<-gsub( " .*$", "", Data_Quarterly_NocRegions_long$OCCUPATION)
Data_Quarterly_NocRegions_long$NocNUM[Data_Quarterly_NocRegions_long$NocNUM=="Total"]<-"Total"
Data_Quarterly_NOC2<-Data_Quarterly_NocRegions_long[Data_Quarterly_NocRegions_long$Noc_Lvl==2,]
Data_Quarterly_NOC2$Trim<-substr(Data_Quarterly_NOC2$Trim,2,2)
Data_Quarterly_NOC2$Trim<-as.numeric(Data_Quarterly_NOC2$Trim)
Data_Quarterly_NOC2$Year<-as.numeric(Data_Quarterly_NOC2$Year)
Data_Quarterly_NOC2$OCCUPATION=NULL
Data_Quarterly_NOC2$Noc_Lvl=NULL
Data_Quarterly_NOC2$Date=NULL
Data_Quarterly_NOC2<-Data_Quarterly_NOC2[substr(Data_Quarterly_NOC2$NocNUM,1,1)!=0,]
View(Data_Quarterly_NOC2)
Data_Quarterly_NOC2<-Data_Quarterly_NOC2[Data_Quarterly_NOC2$Régions!="Canada"&Data_Quarterly_NOC2$Régions!="Ottawa-Gatineau"&Data_Quarterly_NOC2$Régions!="Province de l'Ontario"&
Data_Quarterly_NOC2$Régions!="RMR de Montréal"&Data_Quarterly_NOC2$Régions!="RMR de Québec"&
Data_Quarterly_NOC2$Régions!="Saguenay"&Data_Quarterly_NOC2$Régions!="Sherbrooke"&
Data_Quarterly_NOC2$Régions!="Trois-Rivières",]
Data_Quarterly_NOC2$Régions[Data_Quarterly_NOC2$Régions=="Québec"]<-"Capitale.Nationale"
Data_Quarterly_NOC2$Régions[Data_Quarterly_NOC2$Régions=="Province de Québec"]<-"Québec"
unique(sort(data_rec_NOC2$Region))
unique(sort(data_rec_NOC2$Region))
for (i in 1:length(unique(sort(data_rec_NOC2$Region)))){
Data_Quarterly_NOC2$Régions[Data_Quarterly_NOC2$Régions==unique(sort(Data_Quarterly_NOC2$Régions))[i]]<-unique(sort(data_rec_NOC2$Region))[i]
}
names(Data_Quarterly_NOC2)[names(Data_Quarterly_NOC2)=="Régions"]<-"Region"
Data_Quarterly_NOC2<-Data_Quarterly_NOC2[
with(Data_Quarterly_NOC2, order(Region, NocNUM, Year, Trim)),
]
names(Data_Quarterly_NOC2)[names(Data_Quarterly_NOC2)=="NocNUM"]<-"NOC_2"
Data_Quarterly_NOC2$VALUE<-Data_Quarterly_NOC2$VALUE*1000
temp<-Data_Quarterly_NOC2[Data_Quarterly_NOC2$Year>=2011,]
temp$zero<-ifelse(temp$VALUE==0,1,0)
temp2<-aggregate(zero~Region+NOC_2,data=temp,FUN=sum)
View(temp2)
Data_Quarterly_NOC2<-merge(Data_Quarterly_NOC2,temp2,by=c("Region","NOC_2"),all.x=T)
Data_Quarterly_NOC2$zero<-Data_Quarterly_NOC2$zero/48
length(Data_Quarterly_NOC2$zero[Data_Quarterly_NOC2$zero>0.4])
hist(Data_Quarterly_NOC2$zero)
Data_Quarterly_NOC2$zero
# ===========================================================================================================
# 0. INITIALIZATION
# ===========================================================================================================
# Clear all
rm(list = ls())
set.seed(1234)
# Set paths
path <- '~/Dropbox/GCS_SIDEsummerschool_codes/GouletCoulombe/Couture_OOS/'
setwd(path)
paths <- list(pro = "00_prog",
dat = "10_data",
fig = "20_figures",
tab = "30_tables",
too = "40_tools",
rst = "50_results")
# Install needed packages
myPKGs <- c('torch', 'pracma','glmnet','ranger',
'ggplot2','reshape2', 'stringr','MacroRF',
'caret','doParallel','gridExtra','gbm')
InstalledPKGs <- names(installed.packages()[,'Package'])
InstallThesePKGs <- myPKGs[!myPKGs %in% InstalledPKGs]
if (length(InstallThesePKGs) > 0) install.packages(InstallThesePKGs, repos = "http://cran.us.r-project.org")
# ===========================================================================================================
# 0. INITIALIZATION
# ===========================================================================================================
# Clear all
rm(list = ls())
set.seed(1234)
# Set paths
path <- '~/Dropbox/GCS_SIDEsummerschool_codes/GouletCoulombe/Couture_OOS/'
setwd(path)
paths <- list(pro = "00_prog",
dat = "10_data",
fig = "20_figures",
tab = "30_tables",
too = "40_tools",
rst = "50_results")
# Install needed packages
myPKGs <- c('torch', 'pracma','glmnet','ranger',
'ggplot2','reshape2', 'stringr','MacroRF',
'caret','doParallel','gridExtra','gbm')
InstalledPKGs <- names(installed.packages()[,'Package'])
InstallThesePKGs <- myPKGs[!myPKGs %in% InstalledPKGs]
if (length(InstallThesePKGs) > 0) install.packages(InstallThesePKGs, repos = "http://cran.us.r-project.org")
# Load libraries
library(torch)      # Neural Networks
library(glmnet)     # Lasso, Ridge and Elastic-Net
library(ranger)     # Random Forest
library(MacroRF)    # Macro Random Forest
library(caret)      # Gradient boosting machine (GBM)
library(ggplot2)    # Graphs
library(reshape2)   # Data manipulation
library(gridExtra)  # To organize results graphs
library(pracma)     # Utilities (for PCA)
library(stringr)    # String manipulation
library(doParallel) # Parallel estimation
library(foreach)    # Parallel estimation
# If you are installing "torch" for the first time, you must use the following functions :
# install_torch()
# Load US data retreiver
source(paste(paths$too, 'MakeDataUS_function.R', sep='/'))
# Factor analysis (PCA)
source(paste(paths$too, 'EM_sw.R', sep='/'))
source(paste(paths$too, 'factor.R', sep='/'))
source(paste(paths$too, 'ICp2.R', sep='/'))
# Neural Network function
source(paste(paths$too, 'MLP_function_v6b.R', sep='/'))
# Out-of-sample functions
source(paste(paths$pro, 'OOS_models.R', sep='/'))
# Number of CPU for the estimation (if you don't want to use the parallel setting : ncores <- NA)
ncores <- NA
torch_set_num_threads(1)
# ===========================================================================================================
# 1. OUT-OF-SAMPLE AND MODEL'S PARAMETERS
# ===========================================================================================================
## OOS Parameters ----------------------------------------------------------------
OOS_params <- list()
# Target names from FRED DB
OOS_params$targetName <- c("CPIAUCSL","UNRATE",
"HOUST", "PAYEMS",
"GDPC1")
# Change the transformation code of the target, "NA" to keep FRED's code
OOS_params$target_tcode <- c(5,2,5,NA,NA)
# Forecasting horizons (in quarter)
OOS_params$horizon <- c(1,4)
# Out-of-sample starting date
OOS_params$OOS_starting_date <- "3/1/2015"
# Number of FRED's factors
OOS_params$nFac <- 5
# Number of target lags
OOS_params$lagY <- 2
# Number of regressors lags (factors included)
OOS_params$lagX <- 1
# Create MARX
OOS_params$lagMARX <- NA
# Number of folds for CV
OOS_params$nfolds <- 5
# How many quarters between hyperparameters CV (in quarters)
OOS_params$reEstimate <- 20 # each 5 years
# Which models to used ? Possible choice c("AR, BIC", "ARDI, BIC","LASSO","RIDGE","ELASTIC-NET","RF","GBM","NN,"AR-RF")
OOS_params$model_list <- c("AR, BIC", "ARDI, BIC","LASSO","RIDGE","RF","GBM","NN","AR-RF")
# Folder name in 50_results
OOS_params$save_path = "demo"
## Hyperparamters ----------------------------------------------------------------
# Elastic - Net hyperparameters (CV)
OOS_params$EN_hyps <- list(alpha_range = round(seq(0.01,0.99, length = 100),4))
# Boosting hyperparameters (CV)
OOS_params$Boosting_hyps <- list(man_grid = expand.grid(n.trees = c(seq(25, 700, by = 100)),
interaction.depth = c(3,5),
shrinkage = c(0.01),
n.minobsinnode = c(10)),
fitControl = trainControl(method = "cv",
number = OOS_params$nfolds,
search = "grid"))
# Random Forest hyperparameters
OOS_params$RF_hyps <- list(num.trees = 500,
min.node.size = 3,
mtry = 1/3)
# Macro Random Forest hyperparamaters
OOS_params$MacroRF_hyps <- list(x_pos = c(2,3),
B = 20,
mtry_frac = 0.15,
minsize = 15,
block_size = 8)
# Neural network hyperparameters
OOS_params$nn_hyps <- list(n_features=NA,
nodes=rep(100,5),      # same number of nodes in every layers
patience=10,           # Return the best model
epochs=100,
lr=0.001,
tol=0.01,
show_train=2,          # 1=show each bootstrap loss, 2=progress bar, 3+=show nothing
num_average=5,
dropout_rate=0.2,
sampling_rate = 0.75,
batch_size = 32,
num_batches = NA)
# ===========================================================================================================
# 2. PARALLEL ESTIMATION
# ===========================================================================================================
# Create all possible of targets and horizons
combn <- list(var = c(1:length(OOS_params$targetName)),
hor = OOS_params$horizon)
all_options <- expand.grid(combn)
all_options <- all_options[order(all_options$var,all_options$hor, decreasing = F),]
rownames(all_options) <- c()
# Start parallel clustering
start <- Sys.time()
if(!is.na(ncores)) {
cl <- makeCluster(ncores)
registerDoParallel(cl) # Shows the number of Parallel Workers to be used
foreach(i=c(1:nrow(all_options))) %dopar% Forecast_all(it_pos = i,
all_options = all_options,
paths = paths,
OOS_params = OOS_params,
seed = 124)
stopImplicitCluster()
} else{
for (i in 1:length(all_options)) {
Forecast_all(it_pos = i,
all_options = all_options,
paths = paths,
OOS_params = OOS_params,
seed = 124)
}
}
end <- Sys.time()
end-start
# ===========================================================================================================
# 0. INITIALIZATION
# ===========================================================================================================
# Clear all
rm(list = ls())
set.seed(1234)
# Set paths
path <- '~/Dropbox/GCS_SIDEsummerschool_codes/GouletCoulombe/Couture_OOS/'
setwd(path)
paths <- list(pro = "00_prog",
dat = "10_data",
fig = "20_figures",
tab = "30_tables",
too = "40_tools",
rst = "50_results")
# Install needed packages
myPKGs <- c('torch', 'pracma','glmnet','ranger',
'ggplot2','reshape2', 'stringr','MacroRF',
'caret','doParallel','gridExtra','gbm')
InstalledPKGs <- names(installed.packages()[,'Package'])
InstallThesePKGs <- myPKGs[!myPKGs %in% InstalledPKGs]
if (length(InstallThesePKGs) > 0) install.packages(InstallThesePKGs, repos = "http://cran.us.r-project.org")
# Load libraries
library(torch)      # Neural Networks
library(glmnet)     # Lasso, Ridge and Elastic-Net
library(ranger)     # Random Forest
library(MacroRF)    # Macro Random Forest
library(caret)      # Gradient boosting machine (GBM)
library(ggplot2)    # Graphs
library(reshape2)   # Data manipulation
library(gridExtra)  # To organize results graphs
library(pracma)     # Utilities (for PCA)
library(stringr)    # String manipulation
library(doParallel) # Parallel estimation
library(foreach)    # Parallel estimation
# If you are installing "torch" for the first time, you must use the following functions :
# install_torch()
# Load US data retreiver
source(paste(paths$too, 'MakeDataUS_function.R', sep='/'))
# Factor analysis (PCA)
source(paste(paths$too, 'EM_sw.R', sep='/'))
source(paste(paths$too, 'factor.R', sep='/'))
source(paste(paths$too, 'ICp2.R', sep='/'))
# Neural Network function
source(paste(paths$too, 'MLP_function_v6b.R', sep='/'))
# Out-of-sample functions
source(paste(paths$pro, 'OOS_models.R', sep='/'))
# Number of CPU for the estimation (if you don't want to use the parallel setting : ncores <- NA)
ncores <- NA
torch_set_num_threads(1)
# ===========================================================================================================
# 1. OUT-OF-SAMPLE AND MODEL'S PARAMETERS
# ===========================================================================================================
## OOS Parameters ----------------------------------------------------------------
OOS_params <- list()
# Target names from FRED DB
OOS_params$targetName <- c("CPIAUCSL","UNRATE",
"HOUST", "PAYEMS",
"GDPC1")
# Change the transformation code of the target, "NA" to keep FRED's code
OOS_params$target_tcode <- c(5,2,5,NA,NA)
# Forecasting horizons (in quarter)
OOS_params$horizon <- c(1,4)
# Out-of-sample starting date
OOS_params$OOS_starting_date <- "3/1/2015"
# Number of FRED's factors
OOS_params$nFac <- 5
# Number of target lags
OOS_params$lagY <- 2
# Number of regressors lags (factors included)
OOS_params$lagX <- 1
# Create MARX
OOS_params$lagMARX <- NA
# Number of folds for CV
OOS_params$nfolds <- 5
# How many quarters between hyperparameters CV (in quarters)
OOS_params$reEstimate <- 20 # each 5 years
# Which models to used ? Possible choice c("AR, BIC", "ARDI, BIC","LASSO","RIDGE","ELASTIC-NET","RF","GBM","NN,"AR-RF")
OOS_params$model_list <- c("AR, BIC", "ARDI, BIC","LASSO","RIDGE","RF","GBM","NN","AR-RF")
# Folder name in 50_results
OOS_params$save_path = "demo"
## Hyperparamters ----------------------------------------------------------------
# Elastic - Net hyperparameters (CV)
OOS_params$EN_hyps <- list(alpha_range = round(seq(0.01,0.99, length = 100),4))
# Boosting hyperparameters (CV)
OOS_params$Boosting_hyps <- list(man_grid = expand.grid(n.trees = c(seq(25, 700, by = 100)),
interaction.depth = c(3,5),
shrinkage = c(0.01),
n.minobsinnode = c(10)),
fitControl = trainControl(method = "cv",
number = OOS_params$nfolds,
search = "grid"))
# Random Forest hyperparameters
OOS_params$RF_hyps <- list(num.trees = 500,
min.node.size = 3,
mtry = 1/3)
# Macro Random Forest hyperparamaters
OOS_params$MacroRF_hyps <- list(x_pos = c(2,3),
B = 20,
mtry_frac = 0.15,
minsize = 15,
block_size = 8)
# Neural network hyperparameters
OOS_params$nn_hyps <- list(n_features=NA,
nodes=rep(100,5),      # same number of nodes in every layers
patience=10,           # Return the best model
epochs=100,
lr=0.001,
tol=0.01,
show_train=3,          # 1=show each bootstrap loss, 2=progress bar, 3+=show nothing
num_average=5,
dropout_rate=0.2,
sampling_rate = 0.75,
batch_size = 32,
num_batches = NA)
# ===========================================================================================================
# 2. PARALLEL ESTIMATION
# ===========================================================================================================
# Create all possible of targets and horizons
combn <- list(var = c(1:length(OOS_params$targetName)),
hor = OOS_params$horizon)
all_options <- expand.grid(combn)
all_options <- all_options[order(all_options$var,all_options$hor, decreasing = F),]
rownames(all_options) <- c()
start <- Sys.time()
# Parallel estimation
if(!is.na(ncores)) {
# Start parallel clustering
cl <- makeCluster(ncores)
registerDoParallel(cl) # Shows the number of Parallel Workers to be used
foreach(i=c(1:nrow(all_options))) %dopar% Forecast_all(it_pos = i,
all_options = all_options,
paths = paths,
OOS_params = OOS_params,
seed = 124)
stopImplicitCluster()
# Single core estimation
} else{
for (i in 1:length(all_options)) {
Forecast_all(it_pos = i,
all_options = all_options,
paths = paths,
OOS_params = OOS_params,
seed = 124)
}
}
end <- Sys.time()
end-start
length(all_options)
