lm_wage <- lm(wage ~ ., data = Wage[,!(names(Wage) %in% c('age','wage'))])
lm_wage <- lm(wage ~ ., data = Wage[,!(names(Wage) %in% c('age','year'))])
View(Wage)
summary(as.factor(Wage))
summary(as.factor(Wage$education))
data("Wage", package = "ISLR2")
summary(Wage)
# exclude log wage
Wage <- Wage[ , !(names(Wage) %in% c('logwage'))]
summary(as.factor(Wage$education))
Wage <- Wage %>%
mutate("age_sq" = age^2,
"education" = factor(Wage$education, ordered = TRUE))
contrasts(Wage$education) <- contr.poly(levels(Wage$education))
lm_wage <- lm(Wage ~ age + age2 + education, data = Wage)
lm_wage <- lm(Wage ~ age + age_sq + education, data = Wage)
lm_wage <- lm(wage ~ age + age_sq + education, data = Wage)
summary(lm_wage)
View(Wage)
lm_wage <- lm(wage ~ ., data = Wage[,!(names(Wage) %in% c('age','year'))])
data$education <- factor(data$education, levels = c("High School", "Bachelor", "Master", "PhD"), ordered = TRUE)
contrasts(Wage$education) <- contr.poly(levels(Wage$education)) # polynominal coding bc.we have ordinal variables, where the levels have a natural order
data("Wage", package = "ISLR2")
summary(Wage)
# exclude log wage
Wage <- Wage[ , !(names(Wage) %in% c('logwage'))]
data$education <- factor(data$education, levels = c("High School", "Bachelor", "Master", "PhD"), ordered = TRUE)
Wage$education <- factor(data$education, levels = c("High School", "Bachelor", "Master", "PhD"), ordered = TRUE)
summary(as.factor(Wage$education))
Wage <- Wage %>%
mutate("age_sq" = age^2,
"education" = factor(Wage$education, ordered = TRUE))
lm_wage <- lm(wage ~ age + age_sq + education, data = Wage)
summary(lm_wage)
contrasts(Wage$education) <- contr.poly(levels(Wage$education)) # polynominal coding bc.we have ordinal variables, where the levels have a natural order
lm_wage <- lm(wage ~ age + age_sq + education, data = Wage)
summary(lm_wage)
# or all X's variables?
#lm_wage <- lm(wage ~ ., data = Wage[,!(names(Wage) %in% c('age','year'))])
kable(summary(lm_wage))
lm_wage_sum <- summary(lm_wage)
kable(lm_wage_sum, caption = "Regression Summary", digits = 3)
library(broom)
library(kableExtra)
model_summary <- tidy(lm_wage)
kable(lm_wage_sum, "latex", booktabs = T, digits = 3) %>%
kable_styling(latex_options = c("striped", "scale_down")) %>%
add_header_above(c(" " = 2, "Regression Summary" = 3))
View(model_summary)
model_summary <- broom::tidy(lm_wage)
kable(lm_wage_sum, "latex", booktabs = T, digits = 3) %>%
kable_styling(latex_options = c("striped", "scale_down")) %>%
add_header_above(c(" " = 2, "Regression Summary" = 3))
kable(model_summary, "latex", booktabs = T, digits = 3) %>%
kable_styling(latex_options = c("striped", "scale_down")) %>%
add_header_above(c(" " = 2, "Regression Summary" = 3))
lm_wage_sum <- broom::tidy(lm_wage_sum)
View(lm_wage_sum)
kable(lm_wage_sum, "latex", booktabs = T, digits = 3) %>%
kable_styling(latex_options = c("striped", "scale_down")) %>%
add_header_above(c(" " = 2, "Regression Summary" = 3))
stargazer(lm_wage, type = "latex", title = "Regression Summary", header = FALSE)
stargazer(lm_wage_sum, type = "latex", title = "Regression Summary", header = FALSE)
#Use best subset selection to determine a suitable model.
library(stargazer)
stargazer(lm_wage_sum, type = "latex", title = "Regression Summary", header = FALSE)
lm_wage_all <- lm(wage ~ . + age_sq, data = Wage)
lm_wage_sub <-  leaps::regsubsets(wage ~ age + age2 + education, data = Wage,
nvmax = 9, really.big = TRUE)
lm_wage_sub <-  leaps::regsubsets(wage ~ age + age_sq + education, data = Wage,
nvmax = 9, really.big = TRUE)
lm_wage_sub_sum <- summary(lm_wage_sub)##
lm_wage_sub_sum
plot(summary(lm_wage_sub)$rss, xlab = "Subset size", ylab = "RSS", type = "b")
# Find the model with the lowest AIC
data.frame(
Adj.R2 = which.max(lm_wage_sub_sum$adjr2),
BIC = which.min(lm_wage_sub_sum$cp),
AIC = which.min(lm_wage_sub_sum$bic)
)
#choose based on AIC
lm_wage_sub_aic <- lm(select_model(4,lm_subset,"Y"), train)
#choose based on AIC
lm_wage_sub_aic <- lm(select_model(4,lm_wage_sub,"wage"), train)
#help function
select_model <- function(id, object, outcome){
models <- summary(object)$which[id,-1]
formula <- as.formula(object$call[[2]])
outcome <- all.vars(formula)[1]
predictors <- names(which(models == TRUE))
predictors <- paste(predictors, collapse = "+")
as.formula(paste0(outcome, "~", predictors))
}
#choose based on AIC
lm_wage_sub_aic <- lm(select_model(4,lm_wage_sub,"wage"), train)
#choose based on AIC
lm_wage_sub_aic <- lm(select_model(4,lm_wage_sub,"wage"), Wage)
#Use best subset selection to determine a suitable model.
# best subset selection model
lm_wage_sub <-  leaps::regsubsets(wage ~ age + age_sq + education, data = Wage,
nvmax = 9, really.big = TRUE)
lm_wage_sub_sum <- summary(lm_wage_sub)##
lm_wage_sub_sum
plot(summary(lm_wage_sub)$rss, xlab = "Subset size", ylab = "RSS", type = "b")
# Find the model with the lowest AIC
data.frame(
Adj.R2 = which.max(lm_wage_sub_sum$adjr2),
BIC = which.min(lm_wage_sub_sum$cp),
AIC = which.min(lm_wage_sub_sum$bic)
)
#choose based on AIC
lm_wage_sub_aic <- lm(select_model(4,lm_wage_sub,"wage"), Wage)
#choose based on AIC
lm_wage_sub_aic <- lm(select_model(4,lm_wage_sub,"wage"), Wage)
# Direct polynomial terms
lm_wage_sub_direct <- lm(Wage ~ age + I(age_sq) + education, data = Wage)
# Direct polynomial terms
lm_wage_sub_direct <- lm(wage ~ age + I(age_sq) + education, data = Wage)
summary(model_direct_poly)
summary(lm_wage_sub_direct)
summary(lm_wage_sub_ortho)
# Orthogonal polynomials
lm_wage_sub_ortho <- lm(Wage ~ poly(age, 2) + education, data = Wage)
# Orthogonal polynomials
lm_wage_sub_ortho <- lm(wage ~ poly(age, 2) + education, data = Wage)
summary(lm_wage_sub_ortho)
# Direct polynomial terms
lm_wage_sub_direct <- lm(wage ~ age + I(age_sq) + education, data = Wage)
summary(lm_wage_sub_direct)
# Orthogonal polynomials
lm_wage_sub_ortho <- lm(wage ~ poly(age, 2) + education, data = Wage)
summary(lm_wage_sub_ortho)
# comparison
# AIC and BIC for the direct polynomial model
aic_direct_poly <- AIC(model_direct_poly)
# comparison
# AIC and BIC for the direct polynomial model
aic_direct_poly <- AIC(lm_wage_sub_direct)
bic_direct_poly <- BIC(lm_wage_sub_direct)
# Calculate AIC and BIC for the orthogonal polynomials model
aic_ortho_poly <- AIC(lm_wage_sub_ortho)
bic_ortho_poly <- BIC(lm_wage_sub_ortho)
aic_direct_poly
bic_direct_poly
aic_ortho_poly
bic_ortho_poly
data("Wage", package = "ISLR2")
summary(Wage)
# exclude log wage
Wage <- Wage[ , !(names(Wage) %in% c('logwage'))]
# For the 'education' variable, we'll need to set up contrasts.
summary(as.factor(Wage$education))
Wage <- Wage %>%
mutate("age_sq" = age^2, #specify non-linear effects for the variable age: if more than squares use splines..
"education" = factor(Wage$education, ordered = TRUE))
contrasts(Wage$education) <- contr.poly(levels(Wage$education)) # polynominal coding bc.we have ordinal variables, where the levels have a natural order
lm_wage <- lm(wage ~ age + age_sq + education, data = Wage)
lm_wage_sum <- summary(lm_wage)
# make output nice
library(broom)
library(kableExtra)
lm_wage_sum <- broom::tidy(lm_wage_sum)
kable(lm_wage_sum, "latex", booktabs = T, digits = 3) %>%
kable_styling(latex_options = c("striped", "scale_down")) %>%
add_header_above(c(" " = 2, "Regression Summary" = 3))
#Use best subset selection to determine a suitable model.
# best subset selection model
lm_wage_sub <-  leaps::regsubsets(wage ~ age + age_sq + education, data = Wage,
nvmax = 9, really.big = TRUE)
lm_wage_sub_sum <- summary(lm_wage_sub)##
lm_wage_sub_sum
plot(summary(lm_wage_sub)$rss, xlab = "Subset size", ylab = "RSS", type = "b")
# Find the model with the lowest AIC
data.frame(
Adj.R2 = which.max(lm_wage_sub_sum$adjr2),
BIC = which.min(lm_wage_sub_sum$cp),
AIC = which.min(lm_wage_sub_sum$bic) # best is model 4
)
#choose based on AIC
#Error in eval(predvars, data, env) : object 'education.L' not found
lm_wage_sub_aic <- lm(select_model(4,lm_wage_sub,"wage"), Wage)
install.packages("tlm")
# ADJUST SETUP
knitr::opts_chunk$set(echo = TRUE)
# modify!
#knitr::opts_knit$set(root.dir = 'C:/Users/avalder/OneDrive - WU Wien/Documents/Study/SoSe_24/Statistical Learning/assignments/StatL_5454/A1')
# Clear memory
rm(list=ls())
# Import libraries
library(tidyverse)
library(readr)
library(dplyr)
library(knitr)
library(leaps)
library(reshape2)
library(plyr)
library(Hmisc)
library(broom)
#library(kableExtra) if included PDF does not compile..
# load data
data("diabetes", package = "lars")
diabetes <- as.data.frame(cbind(diabetes$y,diabetes$x))
colnames(diabetes)[1] <- "y"
#describe(diabetes)
summary(diabetes)
# kable(summary(diabetes),
#      col.names = colnames(diabetes),
#      caption="main",
#      digits = 2)
set.seed(123)
# generate & separate train and test data
train_index<- sample(seq_len(nrow(diabetes)), size = 400)
train <- data.frame(diabetes[train_index, ])
test <- data.frame(diabetes[-train_index, ])
cormatrix <- cor(diabetes, use = "complete.obs")
cormatrix  <- round(cormatrix,digits = 2)
#print(cormatrix)
kable(cormatrix, caption = "Correlation Matrix")
pairs(diabetes, pch= 19)
#corrplot::corrplot(diabetes)
# Fit linear regression model using all explanatory variables
lm_full <- lm(y ~ ., data = train)
summary(lm_full)
printCoefmat(round(coef(summary(lm_full)), digits = 2))
# in sample fit
lm_full_mse_ins <- mean(lm_full$residuals^2)
print(lm_full_mse_ins)
# out of sample fit
pred_test <- predict(lm_full, test)
lm_full_mse_oos <- mean((pred_test-test[,1])^2)
print(lm_full_mse_oos)
print(lm_full_mse_ins)
print(lm_full_mse_oos)
print(lm_full_mse_ins)
print(lm_full_mse_oos)
set.seed(1)
# generate & separate train and test data
train_index<- sample(seq_len(nrow(diabetes)), size = 400)
train <- data.frame(diabetes[train_index, ])
test <- data.frame(diabetes[-train_index, ])
set.seed(1)
# generate & separate train and test data
train_index<- sample(seq_len(nrow(diabetes)), size = 400)
train <- data.frame(diabetes[train_index, ])
test <- data.frame(diabetes[-train_index, ])
```
Next, we analyze the pairwise correlation structure between the covariates as well as the covariates and the dependent variable y. These correlations impact model selection as we can get a first impression of whether or not a linear model would be a good assumption through the correlation matrix and the correlation scatter plot. We can see that sex is a categorical and tch seems to be discrete. We observe a clear linear relationship between tc and ldl with a correlation 0.90. Therefore we might ask ourselves if these two variables are really independent predictors. Adding only one to the regression instead of both comes with a slight omitted variable bias, but can make sense for dependent variables in terms of variance reduction. Also the correlation between tc and hdl lies above 0.70. In general, however, a linear relationship is not clearly observable.
#print(cormatrix)
kable(cormatrix, caption = "Correlation Matrix")
# load data
data("diabetes", package = "lars")
diabetes <- as.data.frame(cbind(diabetes$y,diabetes$x))
colnames(diabetes)[1] <- "y"
#describe(diabetes)
summary(diabetes)
# kable(summary(diabetes),
#      col.names = colnames(diabetes),
#      caption="main",
#      digits = 2)
set.seed(1)
# generate & separate train and test data
train_index<- sample(seq_len(nrow(diabetes)), size = 400)
train <- data.frame(diabetes[train_index, ])
test <- data.frame(diabetes[-train_index, ])
cormatrix <- cor(diabetes, use = "complete.obs")
cormatrix  <- round(cormatrix,digits = 2)
#print(cormatrix)
kable(cormatrix, caption = "Correlation Matrix")
pairs(diabetes, pch= 19)
#corrplot::corrplot(diabetes)
# Fit linear regression model using all explanatory variables
lm_full <- lm(y ~ ., data = train)
summary(lm_full)
printCoefmat(round(coef(summary(lm_full)), digits = 2))
# in sample fit
lm_full_mse_ins <- mean(lm_full$residuals^2)
print(lm_full_mse_ins)
# out of sample fit
pred_test <- predict(lm_full, test)
lm_full_mse_oos <- mean((pred_test-test[,1])^2)
print(lm_full_mse_oos)
lm_small <- lm(y ~ sex + bmi + map + tc + ltg , data = train)
summary(lm_small)
printCoefmat(round(coef(summary(lm_small)), digits = 2))
# in sample fit
lm_small_mse_ins <- mean(lm_small$residuals^2)
print(lm_small_mse_ins)
# out of sample fit
pred_test_small <- predict(lm_small, test)
lm_small_mse_oos <- mean((pred_test_small-test[,1])^2)
print(lm_small_mse_oos)
# Comparison of the two models using an F-test
F_small <- anova(lm_full,lm_small)
print(F_small)
print(F_small)
lm_step <- step(lm_full, criteria = "AIC")
lm_step <- step(lm_full, criteria = "AIC")
summary(lm_step)
summary(lm_step)
printCoefmat(round(coef(summary(lm_step)), digits = 2))
# in sample fit
lm_step_mse_ins <- mean(lm_step$residuals^2)
print(lm_step_mse_ins)
# out of sample fit
pred_test_step <- predict(lm_step, test)
lm_step_mse_oos <- mean((pred_test_step-test[,1])^2)
print(lm_step_mse_oos)
# out of sample fit
pred_test_step <- predict(lm_step, test)
lm_step_mse_oos <- mean((pred_test_step-test[,1])^2)
print(lm_step_mse_oos)
# out of sample fit
pred_test_step <- predict(lm_step, test)
lm_step_mse_oos <- mean((pred_test_step-test[,1])^2)
print(lm_step_mse_oos)
# in sample fit
lm_step_mse_ins <- mean(lm_step$residuals^2)
print(lm_step_mse_ins)
# Comparison of the two models using an F-test
F_step <- anova(lm_full,lm_step)
print(F_step)
lm_subset <-  leaps::regsubsets(y ~ ., data = train,
nvmax = 9, really.big = TRUE)
lm_subset_sum <- summary(lm_subset)##
lm_subset_sum
plot(summary(lm_subset)$rss, xlab = "Subset size", ylab = "RSS", type = "b")
# Find the model with the lowest AIC
data.frame(
Adj.R2 = which.max(lm_subset_sum$adjr2),
BIC = which.min(lm_subset_sum$cp),
AIC = which.min(lm_subset_sum$bic)
)
#help function
select_model <- function(id, object, outcome){
models <- summary(object)$which[id,-1]
formula <- as.formula(object$call[[2]])
outcome <- all.vars(formula)[1]
predictors <- names(which(models == TRUE))
predictors <- paste(predictors, collapse = "+")
as.formula(paste0(outcome, "~", predictors))
}
#choose based on AIC
lm_subset_aic <- lm(select_model(5,lm_subset,"Y"), train)
lm_subset_aic_sum <- summary(lm_subset_aic)
lm_subset_aic_sum
# in sample fit
lm_sub_mse_ins <- mean(lm_subset_aic$residuals^2)
print(lm_sub_mse_ins)
# out of sample fit
pred_test_sub <- predict(lm_subset_aic, test)
lm_sub_mse_oos <- mean((pred_test_sub-test[,1])^2)
print(lm_sub_mse_oos)
lm_subset <-  leaps::regsubsets(y ~ ., data = train,
nvmax = 9, really.big = TRUE)
lm_subset_sum <- summary(lm_subset)##
lm_subset_sum
plot(summary(lm_subset)$rss, xlab = "Subset size", ylab = "RSS", type = "b")
# Find the model with the lowest AIC
data.frame(
Adj.R2 = which.max(lm_subset_sum$adjr2),
BIC = which.min(lm_subset_sum$cp),
AIC = which.min(lm_subset_sum$bic)
)
#help function
select_model <- function(id, object, outcome){
models <- summary(object)$which[id,-1]
formula <- as.formula(object$call[[2]])
outcome <- all.vars(formula)[1]
predictors <- names(which(models == TRUE))
predictors <- paste(predictors, collapse = "+")
as.formula(paste0(outcome, "~", predictors))
}
#choose based on AIC
lm_subset_aic <- lm(select_model(5,lm_subset,"Y"), train)
lm_subset_aic_sum <- summary(lm_subset_aic)
lm_subset_aic_sum
# in sample fit
lm_sub_mse_ins <- mean(lm_subset_aic$residuals^2)
print(lm_sub_mse_ins)
# out of sample fit
pred_test_sub <- predict(lm_subset_aic, test)
lm_sub_mse_oos <- mean((pred_test_sub-test[,1])^2)
print(lm_sub_mse_oos)
# Comparison of the two models using an F-test
F_sub <- anova(lm_full,lm_subset_aic)
print(F_sub)
summary(lm_small)
# Comparison of the two models using an F-test
F_small <- anova(lm_small,lm_full)
print(F_small)
results <- join_all(list(melt(data.frame(as.list(lm_full$coefficients))),
melt(data.frame(as.list(lm_small$coefficients))),
melt(data.frame(as.list(lm_step$coefficients))),
melt(data.frame(as.list(lm_subset_aic$coefficients)))),
by="variable", type = "left")
colnames(results) <- c("coef","full","small","stepwise","subset")
results <- data.frame(lapply(results, as.character), stringsAsFactors = F)
ins_MSE <- c("MSE in sample",lm_full_mse_ins, lm_small_mse_ins, lm_step_mse_ins, lm_sub_mse_ins)
oos_MSE <- c("MSE out of sample",lm_full_mse_oos, lm_small_mse_oos, lm_step_mse_oos, lm_sub_mse_oos)
results <- (rbind(results,ins_MSE,oos_MSE))
results <- cbind(results[,1],data.frame(lapply(results[,-1],
function(x) round(as.numeric(x),digits = 2))))
rownames(results) <- results[,1]
results <- results[,-1]
kable(results, caption = "Results all models")
data("Wage", package = "ISLR2")
summary(Wage)
# exclude log wage
Wage <- Wage[ , !(names(Wage) %in% c('logwage'))]
# For the 'education' variable, we'll need to set up contrasts.
summary(as.factor(Wage$education))
Wage <- Wage %>%
mutate("age_sq" = age^2, #specify non-linear effects for the variable age: if more than squares use splines..
"education" = factor(Wage$education, ordered = TRUE))
contrasts(Wage$education) <- contr.poly(levels(Wage$education)) # polynominal coding bc.we have ordinal variables, where the levels have a natural order
lm_wage <- lm(wage ~ age + age_sq + education, data = Wage)
lm_wage_sum <- summary(lm_wage)
lm_wage_sum
lm_wage_sub_sum
#Use best subset selection to determine a suitable model.
# best subset selection model
lm_wage_sub <-  leaps::regsubsets(wage ~ age + age_sq + education, data = Wage,
nvmax = 9, really.big = TRUE)
lm_wage_sub_sum <- summary(lm_wage_sub)##
lm_wage_sub_sum
# Direct polynomial terms
lm_wage_sub_direct <- lm(wage ~ age + I(age_sq) + education, data = Wage)
summary(lm_wage_sub_direct)
summary(lm_wage_sub_ortho)
# comparison
# AIC and BIC for the direct polynomial model
aic_direct_poly <- AIC(lm_wage_sub_direct)
bic_direct_poly <- BIC(lm_wage_sub_direct)
# Calculate AIC and BIC for the orthogonal polynomials model
aic_ortho_poly <- AIC(lm_wage_sub_ortho)
bic_ortho_poly <- BIC(lm_wage_sub_ortho)
#Use best subset selection to determine a suitable model.
# best subset selection model
lm_wage_sub <-  leaps::regsubsets(wage ~ age + age_sq + education, data = Wage,
nvmax = 9, really.big = TRUE)
lm_wage_sub_sum <- summary(lm_wage_sub)##
lm_wage_sub_sum
plot(summary(lm_wage_sub)$rss, xlab = "Subset size", ylab = "RSS", type = "b")
# Find the model with the lowest AIC
data.frame(
Adj.R2 = which.max(lm_wage_sub_sum$adjr2),
BIC = which.min(lm_wage_sub_sum$cp),
AIC = which.min(lm_wage_sub_sum$bic) # best is model 4
)
#choose based on AIC
#Error in eval(predvars, data, env) : object 'education.L' not found
#lm_wage_sub_aic <- lm(select_model(4,lm_wage_sub,"wage"), Wage)
#lm_wage_sub_aic_sum <- summary(lm_subset_aic)
#lm_wage_sub_aic_sum
# Direct polynomial terms
lm_wage_sub_direct <- lm(wage ~ age + I(age_sq) + education, data = Wage)
summary(lm_wage_sub_direct)
# Orthogonal polynomials
lm_wage_sub_ortho <- lm(wage ~ poly(age, 2) + education, data = Wage)
summary(lm_wage_sub_ortho)
# comparison
# AIC and BIC for the direct polynomial model
aic_direct_poly <- AIC(lm_wage_sub_direct)
bic_direct_poly <- BIC(lm_wage_sub_direct)
# Calculate AIC and BIC for the orthogonal polynomials model
aic_ortho_poly <- AIC(lm_wage_sub_ortho)
bic_ortho_poly <- BIC(lm_wage_sub_ortho)
# Direct polynomial terms
lm_wage_sub_direct <- lm(wage ~ age + I(age_sq) + education, data = Wage)
summary(lm_wage_sub_direct)
# Orthogonal polynomials
lm_wage_sub_ortho <- lm(wage ~ poly(age, 2) + education, data = Wage)
summary(lm_wage_sub_ortho)
# comparison
# AIC and BIC for the direct polynomial model
aic_direct_poly <- AIC(lm_wage_sub_direct)
bic_direct_poly <- BIC(lm_wage_sub_direct)
# Calculate AIC and BIC for the orthogonal polynomials model
aic_ortho_poly <- AIC(lm_wage_sub_ortho)
bic_ortho_poly <- BIC(lm_wage_sub_ortho)
aic_direct_poly
aic_ortho_poly
bic_direct_poly
bic_ortho_poly
# Direct polynomial terms
lm_wage_sub_direct <- lm(wage ~ age + age_sq + education, data = Wage)
summary(lm_wage_sub_direct)
# Orthogonal polynomials
lm_wage_sub_ortho <- lm(wage ~ poly(age, 2) + education, data = Wage)
summary(lm_wage_sub_ortho)
# comparison
# AIC and BIC for the direct polynomial model
aic_direct_poly <- AIC(lm_wage_sub_direct)
bic_direct_poly <- BIC(lm_wage_sub_direct)
# Calculate AIC and BIC for the orthogonal polynomials model
aic_ortho_poly <- AIC(lm_wage_sub_ortho)
bic_ortho_poly <- BIC(lm_wage_sub_ortho)
aic_direct_poly
aic_ortho_poly
set.seed(1)
#draw sample for epsilon
epsilon <- rnorm(40,0,1)
#draw sample for x
x <- rnorm(40,0,1)
X <- matrix(cbind(rep(1,length(x)),x,x^2))
View(X)
set.seed(123)
#draw sample for epsilon
epsilon <- rnorm(40,0,1)
#draw sample for x
x <- rnorm(40,0,1)
X <- matrix(cbind(rep(1,length(x)),x,x^2))
