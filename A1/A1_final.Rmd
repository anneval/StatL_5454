---
title: "Statistical Learning (5454) - Assignment 1" # change! 
author: "Matthias Hochholzer, Lukas Pirnbacher, Anne Valder"
date: "Due: 2024-03-25"
output: pdf_document
---

```{r setup, include=FALSE}
# ADJUST SETUP 
knitr::opts_chunk$set(echo = TRUE)
# modify! 
#knitr::opts_knit$set(root.dir = 'C:/Users/avalder/OneDrive - WU Wien/Documents/Study/SoSe_24/Statistical Learning/assignments/StatL_5454/A1')
```

<!-- This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>. -->

<!-- When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: -->

<!-- Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot. -->

```{r,include=FALSE}
# Clear memory
rm(list=ls())
```

```{r, echo=FALSE, include=FALSE, warning=FALSE}
# Import libraries
library(tidyverse)
library(readr)
library(dplyr)
library(knitr)
library(leaps)
library(reshape2)
library(plyr)
library(Hmisc)
```

# Exercise 1
```{r Exercise 1 prep, echo = FALSE, warning=F}
# load data 
data("diabetes", package = "lars")
 
diabetes <- as.data.frame(cbind(diabetes$y,diabetes$x))
colnames(diabetes)[1] <- "y"
 #describe(diabetes)
summary(diabetes)
 
 # kable(summary(diabetes), 
 #      col.names = colnames(diabetes), 
 #      caption="main",
 #      digits = 2) 

```


```{r Exercise 1.1, echo = FALSE}
set.seed(1)
# generate & separate train and test data 
train_index<- sample(seq_len(nrow(diabetes)), size = 400)
train <- data.frame(diabetes[train_index, ])
test <- data.frame(diabetes[-train_index, ])
```
- Explanation: Random selection


```{r Exercise 1.2, echo = FALSE}
cormatrix <- cor(diabetes, use = "complete.obs")
cormatrix  <- round(cormatrix,digits = 2)

#print(cormatrix)
kable(cormatrix, caption = "Correlation Matrix")
pairs(diabetes, pch= 19)

#corrplot::corrplot(diabetes)
```
- Explanation standardized 
- Interpret correlation 


```{r Exercise 1.3, echo = FALSE}
# Fit linear regression model using all explanatory variables
lm_full <- lm(y ~ ., data = train)
summary(lm_full)
printCoefmat(round(coef(summary(lm_full)), digits = 2))

# in sample fit 
lm_full_mse_ins <- mean(lm_full$residuals^2)
print(lm_full_mse_ins)

# out of sample fit 
pred_test <- predict(lm_full, test)
lm_full_mse_oos <- mean((pred_test-test[,1])^2)
print(lm_full_mse_oos)
```
```{r Exercise 1.4, echo = FALSE}
# Fit linear regression model using the covariates which according to a t-test are significant at the 5% significance level conditional on all other variables being included (see summary(lm_full) for significance)

lm_small <- lm(y ~ sex + bmi + map + tc + ltg , data = train)
summary(lm_small)
printCoefmat(round(coef(summary(lm_small)), digits = 2))

# in sample fit 
lm_small_mse_ins <- mean(lm_small$residuals^2)
print(lm_small_mse_ins)

# out of sample fit 
pred_test_small <- predict(lm_small, test)
lm_small_mse_oos <- mean((pred_test_small-test[,1])^2)
print(lm_small_mse_oos)

# Comparison of the two models using an F-test
F_small <- anova(lm_full,lm_small)
print(F_small)
```
```{r Exercise 1.5, echo = FALSE}
# stepwise model

lm_step <- step(lm_full, criteria = "AIC")
summary(lm_step)
printCoefmat(round(coef(summary(lm_step)), digits = 2))


# in sample fit 
lm_step_mse_ins <- mean(lm_step$residuals^2)
print(lm_step_mse_ins)

# out of sample fit 
pred_test_step <- predict(lm_step, test)
lm_step_mse_oos <- mean((pred_test_step-test[,1])^2)
print(lm_step_mse_oos)


# Comparison of the two models using an F-test
F_step <- anova(lm_full,lm_step)
print(F_step)
```


```{r Exercise 1.6, echo = FALSE}
# best subset selection model

lm_subset <-  leaps::regsubsets(y ~ ., data = train, 
     nvmax = 9, really.big = TRUE)

lm_subset_sum <- summary(lm_subset)##
lm_subset_sum

plot(summary(lm_subset)$rss, xlab = "Subset size", ylab = "RSS", type = "b")

# Find the model with the lowest AIC
data.frame(
Adj.R2 = which.max(lm_subset_sum$adjr2),
BIC = which.min(lm_subset_sum$cp),
AIC = which.min(lm_subset_sum$bic)
)


#help function
select_model <- function(id, object, outcome){
models <- summary(object)$which[id,-1]
formula <- as.formula(object$call[[2]])
outcome <- all.vars(formula)[1]
predictors <- names(which(models == TRUE))
predictors <- paste(predictors, collapse = "+")
as.formula(paste0(outcome, "~", predictors))
}
#choose based on BIC
lm_subset_aic <- lm(select_model(5,lm_subset,"Y"), train)
lm_subset_aic_sum <- summary(lm_subset_aic)
lm_subset_aic_sum

# in sample fit 
lm_sub_mse_ins <- mean(lm_subset_aic$residuals^2)
print(lm_sub_mse_ins)

# out of sample fit 
pred_test_sub <- predict(lm_subset_aic, test)
lm_sub_mse_oos <- mean((pred_test_sub-test[,1])^2)
print(lm_sub_mse_oos)


# Comparison of the two models using an F-test
F_sub <- anova(lm_full,lm_subset_aic)
print(F_sub)
```

```{r Exercise 1 summary, echo = FALSE}
# Extract coefficients into data frames

results <- join_all(list(melt(data.frame(as.list(lm_full$coefficients))),
  melt(data.frame(as.list(lm_small$coefficients))),
  melt(data.frame(as.list(lm_step$coefficients))),
  melt(data.frame(as.list(lm_subset_aic$coefficients)))),
  by="variable", type = "left")
s
colnames(results) <- c("coef","full","small","stepwise","subset")
results <- data.frame(lapply(results, as.character), stringsAsFactors = F)

ins_MSE <- c("MSE in sample",lm_full_mse_ins, lm_small_mse_ins, lm_step_mse_ins, lm_sub_mse_ins)
oos_MSE <- c("MSE out of sample",lm_full_mse_oos, lm_small_mse_oos, lm_step_mse_oos, lm_sub_mse_oos)

results <- (rbind(results,ins_MSE,oos_MSE))

results <- cbind(results[,1],data.frame(lapply(results[,-1],
function(x) round(as.numeric(x),digits = 2))))

rownames(results) <- results[,1]
results <- results[,-1]

kable(results, caption = "Results all models")
```


# Exercise 2
```{r Exercise 2 prep, include=FALSE}
# Clear memory
#rm(list=ls())

data("Wage", package = "ISLR2")
summary(Wage)
#describe(Wage)

# exclude log wage
Wage <- Wage[ , !(names(Wage) %in% c('logwage'))] 

#Fit a linear regression model to predict Wage. Omit the variable logwage before analysis, specify non-linear effects for the variable age and use suitable contrasts for the variable education


# For the 'education' variable, we'll need to set up contrasts.
# Assuming 'education' is a factor with ordered levels, we could use polynomial contrasts

summary(as.factor(Wage$education))
Wage <- Wage %>% 
  mutate("age_sq" = age^2,
          "education" = factor(Wage$education, ordered = TRUE))

contrasts(Wage$education) <- contr.poly(levels(Wage$education)) # polynominal coding bc.we have ordinal variables, where the levels have a natural order

lm_wage <- lm(wage ~ age + age_sq + education, data = Wage)
summary(lm_wage)

# or all X's variables? 
#lm_wage <- lm(wage ~ ., data = Wage[,!(names(Wage) %in% c('age','year'))])
```


```{r Exercise 2, include=FALSE}
#Use best subset selection to determine a suitable model.


```


# Exercise 3
```{r Exercise 3, include=FALSE}

```

# Exercise 4
```{r Exercise 4, include=FALSE}

```

# Exercise 5
```{r Exercise 5, include=FALSE}

```




