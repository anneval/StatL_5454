---
title: "Statistical Learning (5454) - Assignment 1"  
author: "Matthias Hochholzer, Lukas Pirnbacher, Anne Valder"
date: "Due: 2024-03-25"
output: pdf_document
---

```{r setup, include=FALSE}
# ADJUST SETUP 
#knitr::opts_chunk$set(echo = TRUE)
# modify! 
#knitr::opts_knit$set(root.dir = 'C:/Users/avalder/OneDrive - WU Wien/Documents/Study/SoSe_24/Statistical Learning/assignments/StatL_5454/A1')
```

<!-- This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>. -->

<!-- When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: -->

<!-- Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot. -->

```{r,include=FALSE}
# Clear memory
rm(list=ls())
```

```{r, echo=FALSE, include=FALSE, warning=FALSE}
# Import libraries
library(tidyverse)
library(readr)
library(dplyr)
library(knitr)
library(leaps)
library(reshape2)
library(plyr)
library(Hmisc)
library(broom)
library(kableExtra)
```

# Exercise 1
```{r Exercise 1 prep, echo = FALSE, warning=F}
# load data 
data("diabetes", package = "lars")
 
diabetes <- as.data.frame(cbind(diabetes$y,diabetes$x))
colnames(diabetes)[1] <- "y"
 #describe(diabetes)
summary(diabetes)
 
 # kable(summary(diabetes), 
 #      col.names = colnames(diabetes), 
 #      caption="main",
 #      digits = 2) 

```


```{r Exercise 1.1, echo = FALSE}
set.seed(1)
# generate & separate train and test data 
train_index<- sample(seq_len(nrow(diabetes)), size = 400)
train <- data.frame(diabetes[train_index, ])
test <- data.frame(diabetes[-train_index, ])
```
- Explanation: Random selection


```{r Exercise 1.2, echo = FALSE}
cormatrix <- cor(diabetes, use = "complete.obs")
cormatrix  <- round(cormatrix,digits = 2)

#print(cormatrix)
kable(cormatrix, caption = "Correlation Matrix")
pairs(diabetes, pch= 19)

#corrplot::corrplot(diabetes)
```
- Explanation standardized 
- Interpret correlation 


```{r Exercise 1.3, echo = FALSE}
# Fit linear regression model using all explanatory variables
lm_full <- lm(y ~ ., data = train)
summary(lm_full)
printCoefmat(round(coef(summary(lm_full)), digits = 2))

# in sample fit 
lm_full_mse_ins <- mean(lm_full$residuals^2)
print(lm_full_mse_ins)

# out of sample fit 
pred_test <- predict(lm_full, test)
lm_full_mse_oos <- mean((pred_test-test[,1])^2)
print(lm_full_mse_oos)
```
```{r Exercise 1.4, echo = FALSE}
# Fit linear regression model using the covariates which according to a t-test are significant at the 5% significance level conditional on all other variables being included (see summary(lm_full) for significance)

lm_small <- lm(y ~ sex + bmi + map + tc + ltg , data = train)
summary(lm_small)
printCoefmat(round(coef(summary(lm_small)), digits = 2))

# in sample fit 
lm_small_mse_ins <- mean(lm_small$residuals^2)
print(lm_small_mse_ins)

# out of sample fit 
pred_test_small <- predict(lm_small, test)
lm_small_mse_oos <- mean((pred_test_small-test[,1])^2)
print(lm_small_mse_oos)

# Comparison of the two models using an F-test
F_small <- anova(lm_full,lm_small)
print(F_small)
```
```{r Exercise 1.5, echo = FALSE}
# stepwise model

lm_step <- step(lm_full, criteria = "AIC")
summary(lm_step)
printCoefmat(round(coef(summary(lm_step)), digits = 2))


# in sample fit 
lm_step_mse_ins <- mean(lm_step$residuals^2)
print(lm_step_mse_ins)

# out of sample fit 
pred_test_step <- predict(lm_step, test)
lm_step_mse_oos <- mean((pred_test_step-test[,1])^2)
print(lm_step_mse_oos)


# Comparison of the two models using an F-test
F_step <- anova(lm_full,lm_step)
print(F_step)
```


```{r Exercise 1.6, echo = FALSE}
# best subset selection model

lm_subset <-  leaps::regsubsets(y ~ ., data = train, 
     nvmax = 9, really.big = TRUE)

lm_subset_sum <- summary(lm_subset)##
lm_subset_sum

plot(summary(lm_subset)$rss, xlab = "Subset size", ylab = "RSS", type = "b")

# Find the model with the lowest AIC
data.frame(
Adj.R2 = which.max(lm_subset_sum$adjr2),
BIC = which.min(lm_subset_sum$cp),
AIC = which.min(lm_subset_sum$bic)
)


#help function
select_model <- function(id, object, outcome){
models <- summary(object)$which[id,-1]
formula <- as.formula(object$call[[2]])
outcome <- all.vars(formula)[1]
predictors <- names(which(models == TRUE))
predictors <- paste(predictors, collapse = "+")
as.formula(paste0(outcome, "~", predictors))
}
#choose based on AIC
lm_subset_aic <- lm(select_model(5,lm_subset,"Y"), train)
lm_subset_aic_sum <- summary(lm_subset_aic)
lm_subset_aic_sum

# in sample fit 
lm_sub_mse_ins <- mean(lm_subset_aic$residuals^2)
print(lm_sub_mse_ins)

# out of sample fit 
pred_test_sub <- predict(lm_subset_aic, test)
lm_sub_mse_oos <- mean((pred_test_sub-test[,1])^2)
print(lm_sub_mse_oos)


# Comparison of the two models using an F-test
F_sub <- anova(lm_full,lm_subset_aic)
print(F_sub)
```

```{r Exercise 1 summary, echo = FALSE}
# Extract coefficients into data frames

results <- join_all(list(melt(data.frame(as.list(lm_full$coefficients))),
  melt(data.frame(as.list(lm_small$coefficients))),
  melt(data.frame(as.list(lm_step$coefficients))),
  melt(data.frame(as.list(lm_subset_aic$coefficients)))),
  by="variable", type = "left")

colnames(results) <- c("coef","full","small","stepwise","subset")
results <- data.frame(lapply(results, as.character), stringsAsFactors = F)

ins_MSE <- c("MSE in sample",lm_full_mse_ins, lm_small_mse_ins, lm_step_mse_ins, lm_sub_mse_ins)
oos_MSE <- c("MSE out of sample",lm_full_mse_oos, lm_small_mse_oos, lm_step_mse_oos, lm_sub_mse_oos)

results <- (rbind(results,ins_MSE,oos_MSE))

results <- cbind(results[,1],data.frame(lapply(results[,-1],
function(x) round(as.numeric(x),digits = 2))))

rownames(results) <- results[,1]
results <- results[,-1]

kable(results, caption = "Results all models")
```


# Exercise 2
```{r Exercise 2 prep, include=FALSE}
# # Clear memory
# #rm(list=ls())
# 
# data("Wage", package = "ISLR2")
# summary(Wage)
# #describe(Wage)
# 
# # exclude log wage
# Wage <- Wage[ , !(names(Wage) %in% c('logwage'))] 
# 
# #Fit a linear regression model to predict Wage. Omit the variable logwage before analysis, specify non-linear effects for the variable age and use suitable contrasts for the variable education
# 
# # For the 'education' variable, we'll need to set up contrasts.
# summary(as.factor(Wage$education))
# Wage <- Wage %>% 
#   mutate("age_sq" = age^2, #specify non-linear effects for the variable age: if more than squares use splines.. 
#           "education" = factor(Wage$education, ordered = TRUE))
# 
# contrasts(Wage$education) <- contr.poly(levels(Wage$education)) # polynominal coding bc.we have ordinal variables, where the levels have a natural order
# 
# lm_wage <- lm(wage ~ age + age_sq + education, data = Wage)
# # or all X's variables? Need contrasts for all other factors.. so I assume no 
# #lm_wage_all <- lm(wage ~ . + age_sq, data = Wage)
# 
# lm_wage_sum <- summary(lm_wage)
# 
# # make output nice
# #lm_wage_sum <- broom::tidy(lm_wage_sum)
# #kable(lm_wage_sum, "latex", booktabs = T, digits = 3) %>%
#  # kable_styling(latex_options = c("striped", "scale_down")) %>%
#  # add_header_above(c(" " = 2, "Regression Summary" = 3))

```


```{r Exercise 2.1, include=FALSE}
# #Use best subset selection to determine a suitable model.
# # best subset selection model
# lm_wage_sub <-  leaps::regsubsets(wage ~ age + age_sq + education, data = Wage, 
#      nvmax = 9, really.big = TRUE)
# 
# lm_wage_sub_sum <- summary(lm_wage_sub)##
# lm_wage_sub_sum
# 
# plot(summary(lm_wage_sub)$rss, xlab = "Subset size", ylab = "RSS", type = "b")
# 
# # Find the model with the lowest AIC
# data.frame(
# Adj.R2 = which.max(lm_wage_sub_sum$adjr2),
# BIC = which.min(lm_wage_sub_sum$cp),
# AIC = which.min(lm_wage_sub_sum$bic) # best is model 4
# )
# 
# #choose based on AIC
# #Error in eval(predvars, data, env) : object 'education.L' not found
# #lm_wage_sub_aic <- lm(select_model(4,lm_wage_sub,"wage"), Wage)
# #lm_wage_sub_aic_sum <- summary(lm_subset_aic)
# #lm_wage_sub_aic_sum

```


```{r Exercise 2.2, include=FALSE}
# # Direct polynomial terms
# lm_wage_sub_direct <- lm(wage ~ age + I(age_sq) + education, data = Wage)
# summary(lm_wage_sub_direct)
# 
# # Orthogonal polynomials
# lm_wage_sub_ortho <- lm(wage ~ poly(age, 2) + education, data = Wage)
# summary(lm_wage_sub_ortho)
# 
# # comparison
# # AIC and BIC for the direct polynomial model
# aic_direct_poly <- AIC(lm_wage_sub_direct)
# bic_direct_poly <- BIC(lm_wage_sub_direct)
# 
# # Calculate AIC and BIC for the orthogonal polynomials model
# aic_ortho_poly <- AIC(lm_wage_sub_ortho)
# bic_ortho_poly <- BIC(lm_wage_sub_ortho)
# 
# Direct polynomials are straightforward (linear and squared terms of age), making them somewhat easier to interpret in terms of the direct effect of aging. However, they can be collinear, especially with higher-degree polynomials. Orthogonal polynomials deal with the potential issue of multicollinearity between the polynomial terms, leading to more stable coefficient estimates. However, the coefficients of orthogonal polynomials do not directly translate to the simple linear and quadratic terms, making them a bit more challenging to interpret. For predictive accuracy, orthogonal polynomials can sometimes offer an advantage, especially in complex models. For interpretation, direct polynomials might be preferred if the primary interest is in understanding the specific nature of the relationship between age and wage.
# 
# Here: AIC and BIC values are the same (or virtually the same) for both models using direct polynomial terms and orthogonal polynomials for age. This suggests that both models are equally good from the standpoint of information criteria, balancing model fit and complexity in a similar manner. In such a case, the decision on which model to choose may depends on other considerations (see above).
```


# Exercise 3
```{r Exercise 3, include=FALSE}

```

# Exercise 4
```{r Exercise 4, include=FALSE}

```

# Exercise 5
```{r Exercise 5, include=FALSE}

```




