---
title: "Statistical Learning (5454) - Assignment 1"  
author: "Matthias Hochholzer, Lukas Pirnbacher, Anne Valder"
date: "Due: 2024-03-25"
output: pdf_document
---

```{r setup, include=FALSE}
# ADJUST SETUP 
knitr::opts_chunk$set(echo = TRUE)
# modify! 
#knitr::opts_knit$set(root.dir = 'C:/Users/avalder/OneDrive - WU Wien/Documents/Study/SoSe_24/Statistical Learning/assignments/StatL_5454/A1')
```

<!-- This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>. -->

<!-- When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: -->

<!-- Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot. -->

```{r,include=FALSE}
# Clear memory
rm(list=ls())
```

```{r, echo=FALSE, include=FALSE, warning=FALSE}
# Import libraries
library(tidyverse)
library(readr)
library(dplyr)
library(knitr)
library(leaps)
library(reshape2)
library(plyr)
library(Hmisc)
library(broom)
library(glmnet)
library(MASS)
#library(kableExtra) if included PDF does not compile..
```

# Exercise 1

We use the diabetes data set from "lars" to fit several linear models. First we load and prepare the data and look at the summary statistics.

```{r Exercise 1 prep, echo = FALSE, warning=F}
# load data 
data("diabetes", package = "lars")
 
diabetes <- as.data.frame(cbind(diabetes$y,diabetes$x))
colnames(diabetes)[1] <- "y"
 #describe(diabetes)
summary(diabetes)
 
 # kable(summary(diabetes), 
 #      col.names = colnames(diabetes), 
 #      caption="main",
 #      digits = 2) 

```

Next, we set a random seed and split the data into train and test data set such that 400 observations (approx. 95%) are used for training and the remaining ones for testing. Selecting the observations for the training set randomly has several reasons. First, we prevent sample bias since the data may be ordered or have patterns based on how the data was collected. If the data has a temporal, spatial, or any systematic order, the first 400 observations might not represent the overall variability in the data set. First we mitigate overfitting and improve model robustness. Overall this leads to increased generalizability of our results.

```{r Exercise 1.1, echo = FALSE}
set.seed(1)
# generate & separate train and test data 
train_index <- sample(seq_len(nrow(diabetes)), size = 400)
train <- data.frame(diabetes[train_index, ])
test <- data.frame(diabetes[-train_index, ])
```

The covariates are only available in standardized form. This can have several implications, both positive and negative.One the one hand standardization allows us to compare the relative importance of coefficients directly in a regression model, as they are on the same scale. This can be particularly useful in identifying which variables have the most significant effects on the outcome variable. This can in some cases also improve the numerical stability of the estimation process, especially when the variables were measured on vastly different scales. This can lead to more reliable and faster convergence in some algorithms.On the other hand while standardized coefficients facilitate comparison, they can complicate the interpretation of the model. The coefficients of standardized variables represent the change in the outcome variable for a one-standard-deviation change in the predictor variable, which may not be as intuitive as the original units. Moreover, when transforming back to the usual units the question is whether effects are captured correctly. \\

Next, we analyze the pairwise correlation structure between the covariates as well as the covariates and the dependent variable y. These correlations impact model selection as we can get a first impression of whether or not a linear model would be a good assumption through the correlation matrix and the correlation scatter plot. We can see that sex is a categorical and tch seems to be discrete. We observe a clear linear relationship between tc and ldl with a correlation 0.90. Therefore we might ask ourselves if these two variables are really independent predictors. Adding only one to the regression instead of both comes with a slight omitted variable bias, but can make sense for dependent variables in terms of variance reduction. Also the correlation between tch and hdl lies above 0.70. In general, however, a linear relationship is not clearly observable.

```{r Exercise 1.2, echo = FALSE}
cormatrix <- cor(diabetes, use = "complete.obs")
cormatrix  <- round(cormatrix,digits = 2)

#print(cormatrix)
kable(cormatrix, caption = "Correlation Matrix")
pairs(diabetes, pch= 19)

#corrplot::corrplot(diabetes)
```

Now, we fit linear regression model containing all explanatory variables and evaluate its performance using the in-sample mean squared error (MSE) and the out of sample (oos) MSE. As expected the in-sample MSE (2854.869) is lower than the oos MSE on the test data (2945.384).

```{r Exercise 1.3, echo = FALSE}
# Fit linear regression model using all explanatory variables
lm_full <- lm(y ~ ., data = train)
summary(lm_full)
printCoefmat(round(coef(summary(lm_full)), digits = 2))

# in sample fit 
lm_full_mse_ins <- mean(lm_full$residuals^2)
print(lm_full_mse_ins)

# out of sample fit 
pred_test <- predict(lm_full, test)
lm_full_mse_oos <- mean((pred_test-test[,1])^2)
print(lm_full_mse_oos)
```

In the next part, we fit a smaller model where only the covariates are contained which according to a t-test are significant at the 5% significance level conditional on all other variables being included (see model summary for the full model). This leaves us with the following covariates: "sex, bmi, map, tc, ltg". Again we evaluate the performance in-sample as well as on the test data. The in-sample MSE is now 2963.644 and the oos MSE is 3022.301. I.e. again we observe a higher out of sample MSE. When comparing this model to the full model using an F-test we see that the full model, which includes more predictors, provides a significantly better fit to the data compared to the small model, as evidenced by the p-value (0.01221) being less than 0.05.

```{r Exercise 1.4, echo = FALSE}
# Fit linear regression model using the covariates which according to a t-test are significant at the 5% significance level conditional on all other variables being included (see summary(lm_full) for significance)

lm_small <- lm(y ~ sex + bmi + map + tc + ltg , data = train)
summary(lm_small)
printCoefmat(round(coef(summary(lm_small)), digits = 2))

# in sample fit 
lm_small_mse_ins <- mean(lm_small$residuals^2)
print(lm_small_mse_ins)

# out of sample fit 
pred_test_small <- predict(lm_small, test)
lm_small_mse_oos <- mean((pred_test_small-test[,1])^2)
print(lm_small_mse_oos)

# Comparison of the two models using an F-test
F_small <- anova(lm_small,lm_full)
print(F_small)
```

In the following we use step wise regression based on the AIC to select a suitable model. We use the step() function which checks whether the AIC decreases when dropping variables in a step wise procedure and stops as soon as it does not decrease any further. In a similar matter to before we evaluate the performance in-sample as well as oos on the test data and compare this model to the full model using an F-test. The in-sample MSE is now 2870.25 and the oos MSE is 2966.798.The F-test suggests that the p-value (0.7182) is much greater than the typical alpha level of 0.05, suggesting there's no significant evidence to favor the full model over the step model regarding how well they explain the variability in y. In other words, the additional predictors in the full model (age, hdl, tch, and glu) do not significantly improve the model's explanatory power compared to the step model.

```{r Exercise 1.5, echo = FALSE}
# stepwise model

lm_step <- step(lm_full, criteria = "AIC")
summary(lm_step)
printCoefmat(round(coef(summary(lm_step)), digits = 2))


# in sample fit 
lm_step_mse_ins <- mean(lm_step$residuals^2)
print(lm_step_mse_ins)

# out of sample fit 
pred_test_step <- predict(lm_step, test)
lm_step_mse_oos <- mean((pred_test_step-test[,1])^2)
print(lm_step_mse_oos)


# Comparison of the two models using an F-test
F_step <- anova(lm_full,lm_step)
print(F_step)
```

Here we use best subset selection to select a suitable model based on the AIC and using the leaps() function. We evaluate the performance in-sample as well as on the test data and compare this model to the full model using an F-test. The in-sample MSE is now 2911.967 and the oos MSE is 2956.157. The F-test suggests that the additional predictors included in the full model (age, tc, ldl, tch, glu) do not significantly improve the model's ability to explain the variability in the dependent variable, since the p-value (0.1716) is greater than the 0.05 significance level. This means there isn't enough statistical evidence to justify the added complexity of the full model over the sub set model for this data set.

```{r Exercise 1.6, echo = FALSE}
# best subset selection model

lm_subset <-  leaps::regsubsets(y ~ ., data = train, 
     nvmax = 9, really.big = TRUE)

lm_subset_sum <- summary(lm_subset)##
lm_subset_sum

plot(summary(lm_subset)$rss, xlab = "Subset size", ylab = "RSS", type = "b")

# Find the model with the lowest AIC
data.frame(
Adj.R2 = which.max(lm_subset_sum$adjr2),
BIC = which.min(lm_subset_sum$cp),
AIC = which.min(lm_subset_sum$bic)
)


#help function
select_model <- function(id, object, outcome){
models <- summary(object)$which[id,-1]
formula <- as.formula(object$call[[2]])
outcome <- all.vars(formula)[1]
predictors <- names(which(models == TRUE))
predictors <- paste(predictors, collapse = "+")
as.formula(paste0(outcome, "~", predictors))
}
#choose based on AIC
lm_subset_aic <- lm(select_model(5,lm_subset,"Y"), train)
lm_subset_aic_sum <- summary(lm_subset_aic)
lm_subset_aic_sum

# in sample fit 
lm_sub_mse_ins <- mean(lm_subset_aic$residuals^2)
print(lm_sub_mse_ins)

# out of sample fit 
pred_test_sub <- predict(lm_subset_aic, test)
lm_sub_mse_oos <- mean((pred_test_sub-test[,1])^2)
print(lm_sub_mse_oos)


# Comparison of the two models using an F-test
F_sub <- anova(lm_full,lm_subset_aic)
print(F_sub)
```

Last, we summarize our results in the following table, containing the regression coefficients of the different models as well as the in-sample and the test data performance:

```{r Exercise 1 summary, echo = FALSE}
# Extract coefficients into data frames

results <- join_all(list(melt(data.frame(as.list(lm_full$coefficients))),
  melt(data.frame(as.list(lm_small$coefficients))),
  melt(data.frame(as.list(lm_step$coefficients))),
  melt(data.frame(as.list(lm_subset_aic$coefficients)))),
  by="variable", type = "left")

colnames(results) <- c("coef","full","small","stepwise","subset")
results <- data.frame(lapply(results, as.character), stringsAsFactors = F)

ins_MSE <- c("MSE in sample",lm_full_mse_ins, lm_small_mse_ins, lm_step_mse_ins, lm_sub_mse_ins)
oos_MSE <- c("MSE out of sample",lm_full_mse_oos, lm_small_mse_oos, lm_step_mse_oos, lm_sub_mse_oos)

results <- (rbind(results,ins_MSE,oos_MSE))

results <- cbind(results[,1],data.frame(lapply(results[,-1],
function(x) round(as.numeric(x),digits = 2))))

rownames(results) <- results[,1]
results <- results[,-1]

kable(results, caption = "Results all models")
```

# Exercise 2

We use the wage data set to fit different linear models. The data set is available in the R package ISLR2. First we load and prepare the data. We look at the summary statistics and omit the logwage variable. Next, we specify non-linear effects for the variable age by adding the variable age squared to our data set. Moreover, we chose suitable contrasts for the variable education to compare the different levels of education in a meaningful way. In R, contrasts define how categorical variables are encoded into numerical values for analysis. The default encoding is treatment coding (also known as dummy coding), where one level is chosen as a baseline and the other levels are compared to this baseline. For an ordinal variable like education, where the levels have a natural order, polynomial contrasts might be more appropriate as they can model the linear and non-linear relationships between the levels of education and the outcome variable. Next, we fit a linear regression model to predict wage using age, age sqaured and education as predictors. The summary results are depicted below.

```{r Exercise 2 prep, echo = FALSE}
# Clear memory
#rm(list=ls())

data("Wage", package = "ISLR2")
summary(Wage)
#describe(Wage)

# exclude log wage
Wage <- Wage[ , !(names(Wage) %in% c('logwage'))]

# For the 'education' variable, we'll need to set up contrasts.
summary(as.factor(Wage$education))
Wage <- Wage %>%
  mutate("age_sq" = age^2, #specify non-linear effects for the variable age: if more than squares use splines..
          "education" = factor(Wage$education, ordered = TRUE))

contrasts(Wage$education) <- contr.poly(levels(Wage$education)) # polynominal coding bc.we have ordinal variables, where the levels have a natural order

lm_wage <- lm(wage ~ age + age_sq + education, data = Wage)
# or all X's variables? Need contrasts for all other factors.. so I assume no
#lm_wage_all <- lm(wage ~ . + age_sq, data = Wage)

lm_wage_sum <- summary(lm_wage)
lm_wage_sum
# make output nice
#lm_wage_sum <- broom::tidy(lm_wage_sum)
#kable(lm_wage_sum, "latex", booktabs = T, digits = 3) %>%
 # kable_styling(latex_options = c("striped", "scale_down"))
```

In the following we perform best subset selection (of wage \~ age + age_sq + education) to determine a suitable model. To do this we use again the leaps package in R. Below the summary and the plot comparing RSS and the subset size k. According to the AIC the best sub model is model 4.

```{r Exercise 2.1, echo = FALSE}
#Use best subset selection to determine a suitable model.
# best subset selection model
lm_wage_sub <-  leaps::regsubsets(wage ~ age + age_sq + education, data = Wage,
     nvmax = 9, really.big = TRUE)

lm_wage_sub_sum <- summary(lm_wage_sub)##
lm_wage_sub_sum

plot(summary(lm_wage_sub)$rss, xlab = "Subset size", ylab = "RSS", type = "b")

# Find the model with the lowest AIC
data.frame(
Adj.R2 = which.max(lm_wage_sub_sum$adjr2),
BIC = which.min(lm_wage_sub_sum$cp),
AIC = which.min(lm_wage_sub_sum$bic) # best is model 4
)
#choose based on AIC
#Error in eval(predvars, data, env) : object 'education.L' not found
#lm_wage_sub_aic <- lm(select_model(4,lm_wage_sub,"wage"), Wage)
#lm_wage_sub_aic_sum <- summary(lm_subset_aic)
#lm_wage_sub_aic_sum
```

Last we assess if it makes a difference if we include the polynom of the original variable age or orthogonal polynoms constructed using poly(age, k). Direct polynomials are straightforward (linear and squared terms of age), making them somewhat easier to interpret in terms of the direct effect of aging. However, they can be collinear, especially with higher-degree polynomials. Orthogonal polynomials deal with the potential issue of multicollinearity between the polynomial terms, leading to more stable coefficient estimates. However, the coefficients of orthogonal polynomials do not directly translate to the simple linear and quadratic terms, making them a bit more challenging to interpret. For predictive accuracy, orthogonal polynomials can sometimes offer an advantage, especially in complex models. For interpretation, direct polynomials might be preferred if the primary interest is in understanding the specific nature of the relationship between age and wage. In the context of our models AIC and BIC values are the same for both models using direct polynomial terms and orthogonal polynomials for age. This suggests that both models are equally good from the standpoint of information criteria, balancing model fit and complexity in a similar manner. In such a case, the decision on which model to choose may depends on other considerations, like interpreation etc.

```{r Exercise 2.2, echo = FALSE}
# Direct polynomial terms
lm_wage_sub_direct <- lm(wage ~ age + age_sq + education, data = Wage)
summary(lm_wage_sub_direct)

# Orthogonal polynomials
lm_wage_sub_ortho <- lm(wage ~ poly(age, 2) + education, data = Wage)
summary(lm_wage_sub_ortho)

# comparison
# AIC and BIC for the direct polynomial model
aic_direct_poly <- AIC(lm_wage_sub_direct)
bic_direct_poly <- BIC(lm_wage_sub_direct)

# Calculate AIC and BIC for the orthogonal polynomials model
aic_ortho_poly <- AIC(lm_wage_sub_ortho)
bic_ortho_poly <- BIC(lm_wage_sub_ortho)
```

# Exercise 3

We assume the following data generating process:
$$y = f(x) + \epsilon =  x + x^2 + \epsilon,$$
with $\epsilon \sim N(0,\sigma^2_{\epsilon})$, $x \sim N(0,\sigma^2_x)$ and $x$ and $\epsilon$ being independent. First we analytically determine the test error using the squared error loss for given parameter estimates $\hat \beta = \left(\hat{\beta}_1, \hat{\beta}_2\right)'$.

Given a training set $\mathcal{T}$, the test error (also called generalization error) of the model $\hat f$ is given by 
$$Err_{\mathcal{T}}=\mathbb{E}_{x,y} [L(y,\hat{f}(x)) \, | \, \mathcal{T}],$$ 
where $\hat f(x) = \hat \beta_1 x + \hat \beta_2 x^2$ and $L(y,\hat{f}(x)) = (y - \hat{f}(x))^2$ denotes the squared error loss function. It follows that

$$
\begin{aligned}
Err_{\mathcal{T}} &= \mathbb{E}_{x,y} \left[ 
\bigl(x+x^2+\epsilon- \hat \beta_1 x - \hat \beta_2 x^2 \bigr)^2 \, \Big| \, \mathcal{T} \right] \\
&= \mathbb{E}_{x,y} \Bigl[ \bigl( \underbrace{(1-\hat\beta_1)x+(1-\hat\beta_2)x^2}_{\text{red}(x)}+\epsilon\bigr)^2 \, \Big| \, \mathcal{T} \Bigr] \\
&=\mathbb{E}_{x,y} \left[ \text{red}(x)^2 + \epsilon^2 + 2 \ \text{red}(x) \epsilon \, | \, \mathcal{T} \right] \\
&= \mathbb{E}_{x,y} \left[(1-\hat\beta_1)^2x^2+(1-\hat\beta_2)^2x^4 +2(1-\hat\beta_1)(1-\hat\beta_2)x^3 \, | \, \mathcal{T}\right] + \mathbb{E}_{x,y} \left[\epsilon^2 \, | \, \mathcal{T}\right] + \mathbb{E}_{x,y} \left[\text{red}(x) \epsilon \, | \, \mathcal{T}\right]\\
&= (1-\hat\beta_1)^2 \mathbb{E}_{x,y} \left[x^2 \, | \, \mathcal{T} \right] + (1-\hat\beta_2)^2 \mathbb{E}_{x,y} \left[x^4 \, | \, \mathcal{T}\right] + 2(1-\hat\beta_1)(1-\hat\beta_2) \mathbb{E}_{x,y} \left[x^3 \, | \, \mathcal{T}\right] +\sigma_{\epsilon}^2\\
&= (1-\hat\beta_1)^2 \sigma_x^2 + (1-\hat\beta_2)^2 3 \sigma_x^4+\sigma_{\epsilon}^2,
\end{aligned}
$$

where $\text{red}(x)$ is the reducible error. In the last step we used the $2^{nd}$, $3^{rd}$ and $4^{th}$ moment of the Normal distribution. In the step before, we used the fact, that $x$ an $\epsilon$ are independent.

Next, we draw a sample of size $N = 40$ as training data (assuming $\sigma^2_{\epsilon} = \sigma^2_x = 1$) and estimate the regression coefficients using OLS. We then determine the test error using the analytical formula as well as simulations. For the simulations we generate a test sample of size $N_{test} = 10,000$, in order for the mean of the squared prediction errors to be a reasonable approximation of the test error. 

Ultimately, we find that simulated and analytical test errors are quite similar and given by

```{r Exercise 3.1, echo=FALSE}

# helper function to generate data
GenerateData <- function(N = 40, var_epsilon = 1, var_x = 1){
  
  # draw sample for epsilon
  epsilon <- rnorm(N,0,sd = sqrt(var_epsilon))
  # draw sample for x
  x <- rnorm(N,0, sd = sqrt(var_x))
  # compute y
  y <- x + x^2 + epsilon
  
  # create data set
  data <- as.data.frame(cbind(y,x,x^2))
  colnames(data) <- c("y", "x", "x_squared")
  
  return(data)
}

# function to compute test error (analytically and based on simulations)
TestError <- function(N_train = 40, N_test = 10000,
                       var_epsilon = 1, var_x = 1){
  
  # generate training data
  data_train <- GenerateData(N = N_train, var_epsilon = var_epsilon, var_x = var_x)
  y_train <- as.vector(data_train$y)
  X_train <- as.matrix(data_train[c("x","x_squared")])
  
  # estimate beta_hat
  beta_hat <- solve(crossprod(X_train))%*%crossprod(X_train,y_train)
  
  # compute test error using our analytical formula
  error_analytical <- (1-beta_hat[1])^2*var_x + 3*(1-beta_hat[2])^2*var_x^2 + var_epsilon
  
  # compute test error using simulation
  data_test <- GenerateData(N = N_test, var_epsilon = var_epsilon, 
                            var_x = var_x) # generate test data
  y_test <- as.vector(data_test$y)
  X_test <- as.matrix(data_test[c("x", "x_squared")])
  
  y_predict <- X_test%*%beta_hat # predicted values
  prediction_errors <- y_predict - y_test
  
  error_simulated <- mean(prediction_errors^2) # MSE
  
  return(c("test_error_analytical" = error_analytical,
           "test_error_simulated" = error_simulated))
}

# set seed and apply our function
set.seed(123)
(test_errors <- TestError())
```

Finally, we want to determine the expected test error, which is defined as
$$
Err = \mathbb{E}_{x,y}\left[L(y,\hat{f}(x)) \right] = \mathbb{E}_{\mathcal{T}} \left[Err_{\mathcal{T}}\right].
$$


```{r Exercise 3.2, include=FALSE}
N_tau <- 1000
test_errors_vec <- numeric(N_tau)

for(i in 1:N_tau){
  test_errors_vec[i] <- TestError()[1]
}

expected_test_error <- mean(test_errors_vec)

```

We estimate the expected test error as the mean test error across $N_{\mathcal{T}} = 1000$ different training samples of size $N = 40$ and find that $Err = `r expected_test_error`$. Consequently, the expected test error is similar to the test error we obtained above. However, a closer look at the summary statistics of the test errors computed for different sets of training data shows that there is in fact some variation in $Err_{\mathcal{T}}$, depending on the particular characteristics of the respective trainings data $\mathcal{T}$. 

```{r Exercise 3.2 summary, echo=FALSE}
summary(test_errors_vec)
```


# Exercise 4

```{r Exercise 4, include=FALSE}

```

# Exercise 5
We use artificial data to perform LASSO and ridge regression. First, we set a random seed before the analysis. Then, we draw 100 observations from a 100-dimensional standard multivariate normal distribution. This is the matrix of covariates X of dimension 100 $\times$ 100. Next, we draw 100 observations for the dependent variable given by 
$$y=\sum_{i=1}^{10}x_i+\epsilon, \quad \text{with} \ \epsilon \sim N(0,0.1)$$

```{r Exercise 5.1, echo=FALSE}
#library(MASS) # for mvrnorm
# set a random seed
set.seed(123)

# Draw 100 observations from a 100-dimensional standard multivariate normal distribution.
# Mean is 0. Cov. matrix is the identity matrix.

X <- mvrnorm(n=100, mu=rep(0,100), Sigma = diag(100))

# Draw 100 observations for the dependent variable.

y <- rowSums(X[,1:10]) + rnorm(100, mean=0,sd=0.1)
```
Now, we fit LASSO and ridge models with different values of $\lambda$ using function \textit{glmnet} from package \textbf{glmnet}.
```{r Exercise 5.2, echo=FALSE}

# Note : default is intercept = TRUE
lasso.fit <- glmnet(X, y, alpha=1)
ridge.fit <- glmnet(X, y, alpha=0)

```
We plot the default plots for the returned objects.
```{r Exercise 5.3, echo=FALSE}
#plot(lasso.fit)
#mtext("numb. of non-zero coeff.", side = 1, line = -16.2, col = 1)
#mtext("numb. of non-zero coeff.", side = 3, line = 3, col = 1)
#title("LASSO", line = 3)
#plot(ridge.fit, xlab="L2 norm")
#mtext("numb. of non-zero coeff.", side = 3, line = 3, col = 1)
#mtext("numb. of non-zero coeff.", side = 1, line = -16.2, col = 1)
#title("Ridge", line = 3)
```

```{r Exercise 5.3 new, echo=FALSE, fig.width=8, fig.height=5, out.width='\\textwidth'}
par(mar=c(5, 4, 4, 2) + 0.1)
plot(lasso.fit)
mtext("numb. of non-zero coeff.", side = 3, line = 2, col = 1)
title("LASSO", line = 3)
plot(ridge.fit, xlab="L2 norm")
mtext("numb. of non-zero coeff.", side = 3, line = 2, col = 1)
title("Ridge", line = 3)
```

Each line represents one variable. L1 norm is the regularization term of LASSO, and L2 the regularization term of Ridge. A small L1 or L2 norm represent a lot of regularization. On the other hand, a high L1 or L2 norm represent low regularization.
For LASSO, an L1 norm of zero gives an empty model. Variables enter the model with increasing L1 norm, as their coefficients take non-zero values. On the top axis, we see the number of non-zero coefficients. 
For Ridge, an L2 norm of zero also gives an empty model. But compared to Ridge, all variables enter right away. Also note, that some are negative.

Additionally, we plot the plots where the argument \textit{xvar} is set to \textit{"lambda"}.
```{r Exercise 5.4 neu, echo=FALSE, fig.width=8, fig.height=5, out.width='\\textwidth'}
par(mar=c(5, 4, 4, 2) + 0.1)
plot(lasso.fit, xvar="lambda")
mtext("numb. of non-zero coeff.", side = 3, line = 2, col = 1)
title("LASSO", line = 3)
plot(ridge.fit, xvar="lambda")
mtext("numb. of non-zero coeff.", side = 3, line = 2, col = 1)
title("Ridge", line = 3)
```
We basically see the same as before, just on another scale. This time the x-axis is log Lambda, the logarithm of the weight given to the regularization (the complexity parameter). For Lambda = 0, the solution is the OLS solution.

Next, we determine the number of non-zero coefficients in dependence of $\lambda$ for LASSO and ridge.

```{r Exercise 5.5 , echo=FALSE, fig.width=8, fig.height=5, out.width='\\textwidth'}
# lasso.fit$lambda
# ridge.fit$lambda

lasso.nonzero <- predict(lasso.fit, type = "nonzero")
ridge.nonzero <- predict(ridge.fit, type = "nonzero")

plot(lasso.fit$lambda, lengths(lasso.nonzero), xlab = "lambda",
ylab = "numb. of non-zero coeff.", ylim = c(0,22), main = "LASSO", type="l")

plot(ridge.fit$lambda, lengths(ridge.nonzero), xlab = "lambda",
ylab = "numb. of non-zero coeff.", ylim = c(0,110), main = "Ridge", type="l")
```
We can see, that for LASSO, the number of non-zero coefficients decreases with lambda. We actually stay at 10 non-zero coefficents for Lambda values between $\sim 0.05-0.55$. For Ridge all variables are in the model for all values of lambda.
Finally, we find the model fit as measured by the deviance() (= RSS) in dependence of $\lambda$ for LASSO and ridge.
```{r Exercise 5.6 , echo=FALSE, fig.width=8, fig.height=5, out.width='\\textwidth'}
lasso.rss <- deviance(lasso.fit)
ridge.rss <- deviance(ridge.fit)

plot(lasso.fit$lambda, lasso.rss, xlab = "lambda", ylab = "RSS", xlim = c(0,1.25),
ylim = c(0,1000), main = "LASSO", type="l")

plot(ridge.fit$lambda, ridge.rss, xlab = "lambda", ylab = "RSS", xlim = c(0,1250),
ylim = c(0,1000), main = "Ridge", type="l")


```
In general, a low Lambda gives a better fit (lower RSS). Compared to LASS0, for Ridge the RSS increases faster with increasing Lambda. This could be a result of all variables entering already with low levels of Lambda.