---
title: "Statistical Learning (5454) - Assignment 1"  
author: "Matthias Hochholzer, Lukas Pirnbacher, Anne Valder"
date: "Due: 2024-03-25"
output: pdf_document

header-includes:
  - \usepackage{bm}
  - \usepackage{amsmath}
  - \newcommand*{\defeq}{\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}=}
  - \newcommand*{\eqdef}{=\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}}
---

```{r setup, include=FALSE}
# ADJUST SETUP 
knitr::opts_chunk$set(echo = TRUE)
# modify! 
#knitr::opts_knit$set(root.dir = 'C:/Users/avalder/OneDrive - WU Wien/Documents/Study/SoSe_24/Statistical Learning/assignments/StatL_5454/A1')
```

<!-- This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>. -->

<!-- When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: -->

<!-- Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot. -->

```{r,include=FALSE}
# Clear memory
rm(list=ls())
```

```{r, echo=FALSE, include=FALSE, warning=FALSE}
# Import libraries
library(tidyverse)
library(readr)
library(dplyr)
library(knitr)
library(leaps)
library(reshape2)
library(plyr)
library(Hmisc)
library(broom)
library(glmnet)
library(MASS)
#library(kableExtra) if included PDF does not compile..
```






<!-- EXERCISE 1 -->

# Exercise 1

We use the diabetes data set from "lars" to fit several linear models. First we load and prepare the data and look at the summary statistics.

```{r Exercise 1 prep, echo = FALSE, warning=F}
# load data 
data("diabetes", package = "lars")
 
diabetes <- as.data.frame(cbind(diabetes$y,diabetes$x))
colnames(diabetes)[1] <- "y"
 #describe(diabetes)
summary(diabetes)
 
 # kable(summary(diabetes), 
 #      col.names = colnames(diabetes), 
 #      caption="main",
 #      digits = 2) 

```

Next, we set a random seed and split the data into train and test data set such that 400 observations (approx. 95%) are used for training and the remaining ones for testing. Selecting the observations for the training set randomly has several reasons. First, we prevent sample bias since the data may be ordered or have patterns based on how the data was collected. If the data has a temporal, spatial, or any systematic order, the first 400 observations might not represent the overall variability in the data set. Second, we mitigate overfitting and improve model robustness. Overall this leads to increased generalizability of our results.

```{r Exercise 1.1, echo = FALSE}
set.seed(1)
# generate & separate train and test data 
train_index <- sample(seq_len(nrow(diabetes)), size = 400)
train <- data.frame(diabetes[train_index, ])
test <- data.frame(diabetes[-train_index, ])
```

The covariates are only available in standardized form. This can have several implications, both positive and negative. On the one hand, standardization allows us to compare the relative importance of coefficients directly in a regression model, as they are on the same scale. This can be particularly useful in identifying which variables have the most significant effects on the outcome variable. This can in some cases also improve the numerical stability of the estimation process, especially when the variables were measured on vastly different scales. This can lead to more reliable and faster convergence in some algorithms. On the other hand, while standardized coefficients facilitate comparison, they can complicate the interpretation of the model. The coefficients of standardized variables represent the change in the outcome variable for a one-standard-deviation change in the predictor variable, which may not be as intuitive as the original units. Moreover, when transforming back to the usual units, the question is whether effects are captured correctly. 

Next, we analyze the pairwise correlation structure between the covariates as well as the covariates and the dependent variable $y$. These correlations impact model selection as we can get a first impression of whether or not a linear model would be a good assumption through the correlation matrix and the correlation scatter plot. We can see that $sex$ is a categorical variable and $tch$ seems to be discrete. We observe a clear linear relationship between $tc$ and $ldl$ with a correlation coefficient of $0.9$. Therefore we might ask ourselves if these two variables are really independent predictors. Adding only one to the regression instead of both comes with a slight omitted variable bias, but can make sense for dependent variables in terms of variance reduction. Also the correlation between $tch$ and $hdl$ lies above 0.7. In general, however, a (strong) linear relationship among the covariates is not clearly observable.

```{r Exercise 1.2, echo = FALSE}
cormatrix <- cor(diabetes, use = "complete.obs")
cormatrix  <- round(cormatrix,digits = 2)

#print(cormatrix)
kable(cormatrix, caption = "Correlation Matrix")
pairs(diabetes, pch= 19)

#corrplot::corrplot(diabetes)
```

Now, we fit a linear regression model containing all explanatory variables and evaluate its performance using the in-sample mean squared error (MSE) and the out of sample (oos) MSE. 

```{r Exercise 1.3, echo = FALSE}
# Fit linear regression model using all explanatory variables
lm_full <- lm(y ~ ., data = train)
summary(lm_full)
#printCoefmat(round(coef(summary(lm_full)), digits = 2))

# in sample fit 
lm_full_mse_ins <- mean(lm_full$residuals^2)

# out of sample fit 
pred_test <- predict(lm_full, test)
lm_full_mse_oos <- mean((pred_test-test[,1])^2)

print(c("in_sample_MSE" = lm_full_mse_ins,
        "out_of_sample_MSE" = lm_full_mse_oos))
```

As expected, the in-sample MSE (2854.869) is lower than the oos MSE on the test data (2945.384).

In the next part, we fit a smaller model where only the covariates are contained, which according to a $t$-test are significant at the 5% significance level conditional on all other variables being included (see model summary for the full model). This leaves us with the following covariates: $sex$, $bmi$, $map$, $tc$ and $ltg$. Again, we evaluate the performance in-sample as well as on the test data.   

```{r Exercise 1.4, echo = FALSE}
# Fit linear regression model using the covariates which according to a t-test are significant at the 5% significance level conditional on all other variables being included (see summary(lm_full) for significance)

lm_small <- lm(y ~ sex + bmi + map + tc + ltg , data = train)
summary(lm_small)
#printCoefmat(round(coef(summary(lm_small)), digits = 2))

# in sample fit 
lm_small_mse_ins <- mean(lm_small$residuals^2)
#print(lm_small_mse_ins)

# out of sample fit 
pred_test_small <- predict(lm_small, test)
lm_small_mse_oos <- mean((pred_test_small-test[,1])^2)
#print(lm_small_mse_oos)

# print MSE (in-sample and out-of-sample)
print(c("in_sample_MSE" = lm_small_mse_ins,
        "out_of_sample_MSE" = lm_small_mse_oos))

# Comparison of the two models using an F-test
F_small <- anova(lm_small,lm_full)
print(F_small)
```

The in-sample MSE is now 2963.644 and the oos MSE is 3022.301. Hence, we once again observe a higher out-of-sample MSE. When comparing this smaller model to the full model using an $F$-test, we see that the full model, which includes more predictors, provides a significantly better fit to the data compared to the small model, as evidenced by the p-value (0.01221) being less than 0.05.


In the following, we use stepwise regression based on the AIC to select a suitable model. We use the \textit{step()} function, which checks whether the AIC decreases when dropping variables in a stepwise procedure and stops as soon as it does not decrease any further. In a similar manner to before, we evaluate the performance in-sample as well as oos on the test data and compare this model to the full model using an $F$-test. 

```{r Exercise 1.5, echo = FALSE}
# stepwise model
lm_step <- step(lm_full, criteria = "AIC")


```

Consequently, stepwise regression based on the AIC yields the following model:


```{r Exercise 1.5 continued, echo = FALSE}

#summary(lm_step)
printCoefmat(round(coef(summary(lm_step)), digits = 2))

# in sample fit 
lm_step_mse_ins <- mean(lm_step$residuals^2)
#print(lm_step_mse_ins)

# out of sample fit 
pred_test_step <- predict(lm_step, test)
lm_step_mse_oos <- mean((pred_test_step-test[,1])^2)
#print(lm_step_mse_oos)


# print MSE (in-sample and out-of-sample)
print(c("in_sample_MSE" = lm_step_mse_ins,
        "out_of_sample_MSE" = lm_step_mse_oos))

# Comparison of the two models using an F-test
F_step <- anova(lm_full,lm_step)
print(F_step)
```



The in-sample MSE is now 2870.25 and the oos MSE is 2966.798. The p-value of the $F$-test (0.7182) is much greater than the typical $\alpha$-level of 0.05, suggesting there's no significant evidence to favor the full model over the step model, regarding how well they explain the variability in $y$. In other words, the additional predictors in the full model ($age$, $hdl$, $tch$, and $glu$) do not significantly improve the model's explanatory power compared to the model suggested by stepwise regression based on the AIC.


Next, we use best subset selection to select a suitable model. In order to do so, we make use of the \textit{regsubsets()} function from the \textbf{leaps} package.

```{r Exercise 1.6, echo = FALSE}
# best subset selection model
lm_subset <-  leaps::regsubsets(y ~ ., data = train, 
     nvmax = 12, really.big = TRUE)

lm_subset_sum <- summary(lm_subset)
lm_subset_sum

plot(summary(lm_subset)$rss, xlab = "Subset size", ylab = "RSS", type = "b")


```
So far, the algorithm has helped us to determine the best-fitting model for different levels of model complexity, i.e. different numbers of covariates. As can be seen in the plot above, the residual sum of squares (RSS) decreases with increasing model complexity. However, the incremental reduction in RSS from including additional explanatory variables diminishes rather quickly. Hence, we now want to find the optimal model complexity based on the AIC.


```{r Exercise 1.6 continued, echo=FALSE}
# Find the model with the lowest AIC
best_model <- data.frame(
  Adj.R2 = which.max(lm_subset_sum$adjr2),
  BIC = which.min(lm_subset_sum$bic),
  AIC = which.min(lm_subset_sum$cp) # Mallows' cp = AIC in case of linear regression
)
rownames(best_model) <- "best_model"
best_model

```
The AIC suggests that model 6 (with covariates `r names(which(lm_subset_sum$which[6,-1]))`) is the best model, while Adjusted $R^2$ and BIC recommend a slightly larger and smaller model, respectively.

We now evaluate the performance of the chosen model in-sample as well as on the test data and compare it to the full model using an $F$-test. 

```{r Exercise 1.6 continued further, echo=FALSE}
# help function
select_model <- function(id, object){
  models <- summary(object)$which[id,-1] # get booleans of 'active' and 'inactive' covariates
  formula <- as.formula(object$call[[2]]) # get model formula
  outcome <- all.vars(formula)[1] # get dependent variable
  predictors <- names(which(models)) # get names of active variables 
  predictors <- paste(predictors, collapse = "+") # create formula for covariates
  as.formula(paste0(outcome, "~", predictors)) # create formula for selected model
}

# choose model based on AIC
lm_subset_aic <- lm(select_model(best_model$AIC,lm_subset), train)
lm_subset_aic_sum <- summary(lm_subset_aic)
lm_subset_aic_sum

# in sample fit 
lm_sub_mse_ins <- mean(lm_subset_aic$residuals^2)
#print(lm_sub_mse_ins)

# out of sample fit 
pred_test_sub <- predict(lm_subset_aic, test)
lm_sub_mse_oos <- mean((pred_test_sub-test[,1])^2)
#print(lm_sub_mse_oos)

# print MSE (in-sample and out-of-sample)
print(c("in_sample_MSE" = lm_sub_mse_ins,
        "out_of_sample_MSE" = lm_sub_mse_oos))



# Comparison of the two models using an F-test
F_sub <- anova(lm_full,lm_subset_aic)
print(F_sub)
```


Since best subset selection has led us to the same model choice as the stepwise regression, in-sample, out-of-sample performance as well as the $F$-test against the full model yield the same conclusions as above.

\newpage
Last, we summarize our results in the following table, containing the regression coefficients of the different models as well as the in-sample and the test data performance:

```{r Exercise 1 summary, echo=FALSE, message=FALSE, warning=FALSE}
# Extract coefficients into data frames

results <- join_all(list(melt(data.frame(as.list(lm_full$coefficients))),
  melt(data.frame(as.list(lm_small$coefficients))),
  melt(data.frame(as.list(lm_step$coefficients))),
  melt(data.frame(as.list(lm_subset_aic$coefficients)))),
  by="variable", type = "left")

colnames(results) <- c("coef","full","small","stepwise","subset")
results <- data.frame(lapply(results, as.character), stringsAsFactors = F)

ins_MSE <- c("MSE in-sample",lm_full_mse_ins, lm_small_mse_ins, lm_step_mse_ins, lm_sub_mse_ins)
oos_MSE <- c("MSE out-of-sample",lm_full_mse_oos, lm_small_mse_oos, lm_step_mse_oos, lm_sub_mse_oos)

results <- (rbind(results,ins_MSE,oos_MSE))

results <- cbind(results[,1],data.frame(lapply(results[,-1],
function(x) round(as.numeric(x),digits = 2))))

rownames(results) <- results[,1]
results <- results[,-1]

kable(results, caption = "Results for all models")
```

While it is clear that the full model has the best in-sample performance in terms of MSE, it is somewhat surprising that the full model outperforms our alternative candidates on the test data as well.

















<!-- EXERCISE 2 -->
# Exercise 2

We use the wage data set to fit different linear models. The data set is available in the R package \textbf{ISLR2}. First, we load and prepare the data and have a look at the summary statistics. 

Next, we fit a linear regression model to predict wage using age and education as predictors. We specify non-linear effects for the variable age by including a polynomial of degree 10. Moreover, we choose suitable contrasts for the variable education to compare the different levels of education in a meaningful way. Contrasts define how categorical variables are encoded into numerical values for the analysis. The default encoding in R is treatment coding (also known as dummy coding), where one level is chosen as a baseline and the other levels are compared to this baseline. For an ordinal variable like education, where the levels have a natural order, polynomial contrasts might be more appropriate as they can model the linear and non-linear relationships between the levels of education and the outcome variable. 


```{r Exercise 2 prep, echo = FALSE}
# Clear memory
#rm(list=ls())

data("Wage", package = "ISLR2")
summary(Wage)
#describe(Wage)

# exclude log wage
Wage <- Wage[ , !(names(Wage) %in% c('logwage'))]

# create polynomial of 'age'
age_raw_poly <- poly(Wage$age, degree = 10, raw = TRUE)
colnames(age_raw_poly) <- paste0("age", 1:10)

Wage <- cbind(Wage, age_raw_poly[,-1]) # add to data

# For the 'education' variable, we'll need to set up contrasts.
summary(as.factor(Wage$education))
Wage <- Wage %>%
  mutate("education" = factor(Wage$education, ordered = TRUE))

contrasts(Wage$education) <- contr.poly(levels(Wage$education)) # polynominal coding bc.we have ordinal variables, where the levels have a natural order

formula_full <- as.formula(paste0("wage ~ age + ", 
                       paste(colnames(age_raw_poly)[-1], collapse = " + "),
                       " + education")) 
lm_wage <- lm(formula_full, data = Wage)

lm_wage_sum <- summary(lm_wage)
lm_wage_sum
# make output nice
#lm_wage_sum <- broom::tidy(lm_wage_sum)
#kable(lm_wage_sum, "latex", booktabs = T, digits = 3) %>%
 # kable_styling(latex_options = c("striped", "scale_down"))
```
We can see from the summaries of our full model that education has significant explanatory power with respect to wages and that wages ceteris paribus increase with higher levels of education. At the same time, none of the coefficients corresponding to the polynomial of $age$ are significantly different from zero conditional on all other variables being in the model. This is not necessarily a sign that $age$ cannot predict wages, but rather a sign of overfitting and some collinearity issues related to including a polynomial of such a high degree. Consequently, we now perform best subset selection to determine a suitable model. To do this, we once again use the \textbf{leaps} package in R. 

```{r Exercise 2.1, echo = FALSE}
#Use best subset selection to determine a suitable model.
# best subset selection model
lm_wage_sub <-  leaps::regsubsets(formula_full, data = Wage,
     nvmax = 20, really.big = TRUE)

lm_wage_sub_sum <- summary(lm_wage_sub)##
lm_wage_sub_sum

plot(summary(lm_wage_sub)$rss, xlab = "Subset size", ylab = "RSS", type = "b")


```
Just as in Exercise 1, we obtain the best-fitting model for different levels of complexity. Next, we want to select the model that minimizes the AIC.

```{r exe_2.1_continued, echo = FALSE}
# Find the model with the lowest AIC
best_model <- data.frame(
  Adj.R2 = which.max(lm_wage_sub_sum$adjr2),
  BIC = which.min(lm_wage_sub_sum$bic),
  AIC = which.min(lm_wage_sub_sum$cp) # best is model 8
)
rownames(best_model) <- "best_model"
best_model

```

Based on the AIC, we conclude that model 8 (with covariates `r names(which(lm_wage_sub_sum$which[8,-1]))`) is the best model. In contrast, the BIC would suggest model 4 (with covariates `r names(which(lm_wage_sub_sum$which[4,-1]))`). Let us now estimate and have a closer look at the chosen model.

```{r echo = FALSE}
active_covariates <- names(which(lm_wage_sub_sum$which[best_model$AIC, -1]))
design_matrix_full <- as.data.frame(model.matrix(lm_wage))
design_matrix_aic <- design_matrix_full[active_covariates]

temp_data <- cbind("wage" = Wage$wage, design_matrix_aic)

lm_wage_sub_aic <- lm(wage  ~ ., data = temp_data)
summary(lm_wage_sub_aic)
```
Since treatment contrasts are generally easier to interpret, especially in the context of model selection, we repeat the procedure above with treatment instead of polynomial contrasts. Let us first estimate the full model.

```{r echo = FALSE}
# Clear memory
#rm(list=ls())

# change to treatment contrasts
contrasts(Wage$education) <- contr.treatment(levels(Wage$education)) # polynominal coding bc.we have ordinal variables, where the levels have a natural order
lm_wage_dummy <- lm(formula_full, data = Wage)

lm_wage_dummy_sum <- summary(lm_wage_dummy)
lm_wage_dummy_sum



```
While residuals and fitted values of the full model are independent of our contrast choice, the interpretation of the estimated coefficients is different now. In particular, the estimated $education$ coefficients now capture the mean difference in earnings of the respective education level to the baseline of \textit{< HS Grad}.

We continue with the best subset selection.
```{r, echo = FALSE}
#Use best subset selection to determine a suitable model.
# best subset selection model
lm_wage_sub_dummy <-  leaps::regsubsets(formula_full, data = Wage,
     nvmax = 20, really.big = TRUE)

lm_wage_sub_dummy_sum <- summary(lm_wage_sub_dummy)##
lm_wage_sub_dummy_sum

plot(summary(lm_wage_sub_dummy)$rss, xlab = "Subset size", ylab = "RSS", type = "b")

# Find the model with the lowest AIC
best_model_dummy <- data.frame(
  Adj.R2 = which.max(lm_wage_sub_dummy_sum$adjr2),
  BIC = which.min(lm_wage_sub_dummy_sum$bic),
  AIC = which.min(lm_wage_sub_dummy_sum$cp) 
)
rownames(best_model_dummy) <- "best_model"
best_model_dummy


```
In the model selection context we start to see some differences between polynomial and treatment contrasts. With dummy coding, the best model according to the AIC is model 9, which includes all dummy variables created from $education$ and the higher-order terms of $age$, i.e. $age^5, age^6, age^7, age^8$ and $age^9$. Let's have a look at that model:

```{r echo = FALSE}
active_covariates_dummy <- names(which(lm_wage_sub_dummy_sum$which[best_model_dummy$AIC, -1]))
design_matrix_full_dummy <- as.data.frame(model.matrix(lm_wage_dummy))
design_matrix_aic_dummy <- design_matrix_full_dummy[active_covariates_dummy]

temp_data <- cbind("wage" = Wage$wage, design_matrix_aic_dummy)

lm_wage_sub_aic_dummy <- lm(wage  ~ ., data = temp_data)
summary(lm_wage_sub_aic_dummy)

```
If we now compare the AIC of the two selected models for polynomial and treatment contrasts, we find that the former is slightly superior.
```{r, echo = FALSE}
AIC_dummy <- AIC(lm_wage_sub_aic_dummy)
AIC_polynomial <- AIC(lm_wage_sub_aic)

c("AIC_poly_contr" = AIC_polynomial, 
  "AIC_treat_contr" = AIC_dummy)
```


Finally, we assess if it makes a difference if we include the polynomial of the original variable $age$ or orthogonal polynomials constructed using poly(age, k). Direct polynomials are straightforward, making them somewhat easier to interpret in terms of the direct effect of aging. However, they can be collinear, especially with higher-degree polynomials. Orthogonal polynomials deal with the potential issue of multicollinearity between the polynomial terms, leading to more stable coefficient estimates. However, the coefficients of orthogonal polynomials are a bit more challenging to interpret. For predictive accuracy, orthogonal polynomials can sometimes offer an advantage, especially in complex models. For interpretation, direct polynomials might be preferred if the primary interest is in understanding the specific nature of the relationship between age and wage. 

In the full model, both the orthogonal polynomial and the direct polynomial yield the same model fit, since each design matrix spans the same space. In particular,  AIC and BIC values are the same for both models using direct polynomial terms and orthogonal polynomials for age. 
```{r, echo = FALSE}

# go back to polynomial contrasts
contrasts(Wage$education) <- contr.poly(levels(Wage$education)) # polynominal coding bc.we have ordinal variables, where the levels have a natural order

# create orthogonal polynomial of 'age'
age_poly <- poly(Wage$age, degree = 10, raw = FALSE)
colnames(age_poly) <- paste0("age", 1:10)

# add to data
Wage_ortho <- Wage
Wage_ortho['age'] <- age_poly[,1]
Wage_ortho[paste0('age', 2:10)] <- age_poly[,-1]


formula_full_ortho <- as.formula(paste0("wage ~ age + ", 
                       paste(colnames(age_poly)[-1], collapse = " + "),
                       " + education")) 

# Orthogonal polynomials
lm_wage_ortho <- lm(formula_full_ortho, data = Wage_ortho)
#summary(lm_wage_sub_ortho)

# comparison
# AIC and BIC for the direct polynomial model
aic_direct_poly <- AIC(lm_wage)
bic_direct_poly <- BIC(lm_wage)

# Calculate AIC and BIC for the orthogonal polynomials model
aic_ortho_poly <- AIC(lm_wage_ortho)
bic_ortho_poly <- BIC(lm_wage_ortho)

# Table for comparison of AIC and BIS for direct and orthogonal polynomials
variable_data <- data.frame(
  direct = c(aic_direct_poly, bic_direct_poly),
  orthogonal = c(aic_ortho_poly, bic_ortho_poly),
  row.names = c("AIC", "BIC")
)

kable(variable_data)
```
Let us now investigate whether there are any differences between orthogonal and direct polynomials when it comes to model selection.

```{r, echo=FALSE}

#Use best subset selection to determine a suitable model.
# best subset selection model
lm_wage_sub_ortho <-  leaps::regsubsets(formula_full_ortho, data = Wage_ortho,
     nvmax = 20, really.big = TRUE)

lm_wage_sub_sum_ortho <- summary(lm_wage_sub_ortho)##
lm_wage_sub_sum_ortho

# Find the model with the lowest AIC
best_model_ortho <- data.frame(
  Adj.R2 = which.max(lm_wage_sub_sum_ortho$adjr2),
  BIC = which.min(lm_wage_sub_sum_ortho$bic),
  AIC = which.min(lm_wage_sub_sum_ortho$cp) 
)
rownames(best_model_ortho) <- "best_model"
best_model_ortho

active_covariates_ortho <- names(which(lm_wage_sub_sum_ortho$which[best_model_ortho$AIC, -1]))
design_matrix_full_ortho <- as.data.frame(model.matrix(lm_wage_ortho))
design_matrix_aic_ortho <- design_matrix_full_ortho[active_covariates_ortho]

temp_data_ortho <- cbind("wage" = Wage_ortho$wage, design_matrix_aic_ortho)

lm_wage_sub_aic_ortho <- lm(wage  ~ ., data = temp_data_ortho)
summary(lm_wage_sub_aic_ortho)


```
Consequently, we find that the use of orthogonal polynomials leads to different conclusions when it comes to model selection. In particular, the model selected based on the AIC is now more complex and also includes the lower-order terms of $age$. The AIC of the best model with orthogonal polynomials is slightly larger than that of the best model with the direct polynomial.
```{r, echo = FALSE}
c("AIC_direct" = AIC(lm_wage_sub_aic),
  "AIC_orthogonal" = AIC(lm_wage_sub_aic_ortho))
```






# Exercise 3

We assume the following data generating process:
$$y = f(x) + \epsilon =  x + x^2 + \epsilon,$$
where $\epsilon \sim N(0,\sigma^2_{\epsilon})$, $x \sim N(0,\sigma^2_x)$ and $x$ and $\epsilon$ are independent. First, we analytically determine the test error using the squared error loss for given parameter estimates $\hat \beta = \left(\hat{\beta}_1, \hat{\beta}_2\right)'$.

Given a training set $\mathcal{T}$, the test error (also called generalization error) of the model $\hat f$ is given by 
$$Err_{\mathcal{T}}=\mathbb{E}_{x,y} [L(y,\hat{f}(x)) \, | \, \mathcal{T}],$$ 
where $\hat f(x) = \hat \beta_1 x + \hat \beta_2 x^2$ and $L(y,\hat{f}(x)) = (y - \hat{f}(x))^2$ denotes the squared error loss function. It follows that

$$
\begin{aligned}
Err_{\mathcal{T}} &= \mathbb{E}_{x,y} \left[ 
\bigl(x+x^2+\epsilon- \hat \beta_1 x - \hat \beta_2 x^2 \bigr)^2 \, \Big| \, \mathcal{T} \right] \\
&= \mathbb{E}_{x,y} \Bigl[ \bigl( \underbrace{(1-\hat\beta_1)x+(1-\hat\beta_2)x^2}_{\eqdef \text{red}(x)}+\epsilon\bigr)^2 \, \Big| \, \mathcal{T} \Bigr] \\
&=\mathbb{E}_{x,y} \left[ \text{red}(x)^2 + \epsilon^2 + 2  \text{red}(x) \epsilon \, | \, \mathcal{T} \right] \\
&= \mathbb{E}_{x,y} \left[(1-\hat\beta_1)^2x^2+(1-\hat\beta_2)^2x^4 +2(1-\hat\beta_1)(1-\hat\beta_2)x^3 \, | \, \mathcal{T}\right] + \mathbb{E}_{x,y} \left[\epsilon^2 \, | \, \mathcal{T}\right] + 2\mathbb{E}_{x,y} \left[\text{red}(x) \epsilon \, | \, \mathcal{T}\right]\\
&= (1-\hat\beta_1)^2 \mathbb{E}_{x,y} \left[x^2 \, | \, \mathcal{T} \right] + (1-\hat\beta_2)^2 \mathbb{E}_{x,y} \left[x^4 \, | \, \mathcal{T}\right] + 2(1-\hat\beta_1)(1-\hat\beta_2) \mathbb{E}_{x,y} \left[x^3 \, | \, \mathcal{T}\right] +\sigma_{\epsilon}^2\\
&= (1-\hat\beta_1)^2 \sigma_x^2 + (1-\hat\beta_2)^2 3 \sigma_x^4+\sigma_{\epsilon}^2,
\end{aligned}
$$

where $\text{red}(x)$ is the reducible error. In the last step we used the $2^{nd}$, $3^{rd}$ and $4^{th}$ moment of the Normal distribution. In the step before, we used the fact, that $x$ and $\epsilon$ are independent and that $\mathbb{E}[ \epsilon] = 0$.

Next, we draw a sample of size $N = 40$ as training data (assuming $\sigma^2_{\epsilon} = \sigma^2_x = 1$) and estimate the regression coefficients using OLS. We then determine the test error using the analytical formula as well as simulations. For the simulations we generate a test sample of size $N_{test} = 10,000$, in order for the mean of the squared prediction errors to be a reasonable approximation of the test error. 

Ultimately, we find that simulated and analytical test errors are quite similar and given by

```{r Exercise 3.1, echo=FALSE}

# helper function to generate data
GenerateData <- function(N = 40, var_epsilon = 1, var_x = 1){
  
  # draw sample for epsilon
  epsilon <- rnorm(N,0,sd = sqrt(var_epsilon))
  # draw sample for x
  x <- rnorm(N,0, sd = sqrt(var_x))
  # compute y
  y <- x + x^2 + epsilon
  
  # create data set
  data <- as.data.frame(cbind(y,x,x^2))
  colnames(data) <- c("y", "x", "x_squared")
  
  return(data)
}

# function to compute test error (analytically and based on simulations)
TestError <- function(N_train = 40, N_test = 10000,
                       var_epsilon = 1, var_x = 1){
  
  # generate training data
  data_train <- GenerateData(N = N_train, var_epsilon = var_epsilon, var_x = var_x)
  y_train <- as.vector(data_train$y)
  X_train <- as.matrix(data_train[c("x","x_squared")])
  
  # estimate beta_hat
  beta_hat <- solve(crossprod(X_train))%*%crossprod(X_train,y_train)
  
  # compute test error using our analytical formula
  error_analytical <- (1-beta_hat[1])^2*var_x + 3*(1-beta_hat[2])^2*var_x^2 + var_epsilon
  
  # compute test error using simulation
  data_test <- GenerateData(N = N_test, var_epsilon = var_epsilon, 
                            var_x = var_x) # generate test data
  y_test <- as.vector(data_test$y)
  X_test <- as.matrix(data_test[c("x", "x_squared")])
  
  y_predict <- X_test%*%beta_hat # predicted values
  prediction_errors <- y_predict - y_test
  
  error_simulated <- mean(prediction_errors^2) # MSE
  
  return(c("test_error_analytical" = error_analytical,
           "test_error_simulated" = error_simulated))
}

# set seed and apply our function
set.seed(123)
(test_errors <- TestError())
```

Finally, we want to determine the expected test error, which is defined as
$$
Err = \mathbb{E}_{x,y}\left[L(y,\hat{f}(x)) \right] = \mathbb{E}_{\mathcal{T}} \left[Err_{\mathcal{T}}\right].
$$


```{r Exercise 3.2, include=FALSE}
N_tau <- 1000
test_errors_vec <- numeric(N_tau)

for(i in 1:N_tau){
  test_errors_vec[i] <- TestError()[1]
}

expected_test_error <- mean(test_errors_vec)

```

We estimate the expected test error as the mean test error across $N_{\mathcal{T}} = 1000$ different training samples of size $N = 40$ and find that $Err = `r expected_test_error`$. Consequently, the expected test error is similar to the test error we obtained above. However, a closer look at the summary statistics of the test errors computed for different sets of training data shows that there is in fact some variation in $Err_{\mathcal{T}}$, depending on the particular characteristics of the respective training data $\mathcal{T}$. 

```{r Exercise 3.2 summary, echo=FALSE}
summary(test_errors_vec)
```


# Exercise 4

We consider the data generating process
$$
\bm{y} = \bm{X\beta} + \bm{\epsilon},
$$
where $\bm{\epsilon} \sim N(\bm{0}, \sigma_{\epsilon}^2 \bm{I})$ and $\bm{X} \in \mathbb{R}^{N \times p}$ is a fixed covariate matrix. First, we want to derive the in-sample error for given parameter estimates $\hat{\bm{\beta}}$ using the squared error loss.

Let $\bm{x}_i$ denote the $i$-th row of $\bm{X}$, i.e. the covariates of observation $i = 1, \ldots, N$. Similarly, let $y_i$ and $\epsilon_i$ denote the $i$-th entry of $\bm{y}$ and $\bm{\epsilon}$, respectively. The in-sample error for given training data $\mathcal{T}$ can then be defined as
$$
Err_{in} = \frac{1}{N} \sum_{i = 1}^N \mathbb{E}_{y_i^0} \left[\left(y_i^0 - \hat{f}(x_i)\right)^2 \, \Bigl| \, \mathcal{T} \right],
$$
where $y_i^0 = f(x_i) + \epsilon_i^0 = \bm{x}_i\bm{\beta} + \epsilon_i^0$ is a new response for observation $i = 1, \ldots, N$ and $\hat{f}(x_i) = \bm{x}_i\hat{\bm{\beta}}$. For arbitrary $i = 1,\ldots, N$ it now follows that
$$
\begin{aligned}
 \mathbb{E}_{y_i^0} \left[\left(y_i^0 - \hat{f}(x_i)\right)^2 \, \Bigl| \, \mathcal{T} \right] &=  \mathbb{E}_{y_i^0} \left[\left(\bm{x}_i\bm{\beta} + \epsilon_i^0 - {x}_i\hat{\bm{\beta}})\right)^2 \, \Bigl| \, \mathcal{T} \right] \\
 &=  \mathbb{E}_{y_i^0} \left[\left(\bm{x}_i(\bm{\beta}-\bm{\hat{\beta}})\right)^2 + 2\epsilon_i^0 \bm{x}_i(\bm{\beta}-\bm{\hat{\beta}}) + \left(\epsilon_i^0\right)^2 \, \Bigl| \, \mathcal{T} \right] \\
 &= \left(\bm{x}_i (\bm{\beta}- \bm{\hat{\beta}})\right)^2 + 2 \bm{x}_i(\bm{\beta}-\bm{\hat{\beta}}) \underbrace{\mathbb{E}\left[\epsilon_i^0 \right]}_{ = 0} +  \underbrace{\mathbb{E}\left[ (\epsilon_i^0)^2\right]}_{=\sigma_\epsilon^2} \\
 &= \sigma_{\epsilon}^2 + \left(\bm{x}_i (\bm{\beta}- \bm{\hat{\beta}})\right)^2 .
\end{aligned}
$$
The in-sample error can therefore be written as
$$
\begin{aligned}
Err_{in} &= \frac{1}{N} \sum_{i=1}^N \sigma_{\epsilon}^2 + \left(\bm{x}_i (\bm{\beta}- \bm{\hat{\beta}})\right)^2 \\
&= \sigma_{\epsilon}^2 + \frac{1}{N} \left(\bm{X}(\bm{\beta}- \bm{\hat{\beta}})\right)'\left(\bm{X}(\bm{\beta}- \bm{\hat{\beta}})\right) \\
&= \sigma_{\epsilon}^2 + \frac{1}{N}(\bm{\beta}- \bm{\hat{\beta}})' \bm{X}'\bm{X} (\bm{\beta}- \bm{\hat{\beta}}).
\end{aligned}
$$
Next, we want to determine the expected in-sample error for OLS estimates of the regression coefficients. The expected in-sample error can be obtained by averaging the in-sample error over the distribution of training data $\mathcal{T}$. Hence, we are interested in $\mathbb{E}_{\mathcal{T}}\left[Err_{in}\right]$. Since the design matrix $\bm{X}$ is deterministic in this example, the only source of randomness in our training data are the error terms $\bm{\epsilon}^{\mathcal{T}} = \left(\epsilon_1^{\mathcal{T}}, \ldots, \epsilon_N^{\mathcal{T}}\right)'$. 

Let us first consider the in-sample error for OLS estimates and fixed training data $\mathcal{T}$. The OLS estimates can be expressed as
$$
\begin{aligned}
\bm{\hat{\beta}} &= \left(\bm{X}'\bm{X}\right)^{-1}\bm{X}'\bm{y}^{\mathcal{T}} \\
&= \left(\bm{X}'\bm{X}\right)^{-1}\bm{X}'\left(\bm{X}\bm{\beta} + \bm{\epsilon}^{\mathcal{T}}\right) \\
&= \left(\bm{X}'\bm{X}\right)^{-1}\bm{X}'\bm{X}\bm{\beta} + \underbrace{\left(\bm{X}'\bm{X}\right)^{-1}\bm{X}'}_{\eqdef\bm{X}^{\dagger}} \bm{\epsilon}^{\mathcal{T}} \\
&= \bm{\beta} + \bm{X}^{\dagger}\bm{\epsilon}^{\mathcal{T}}
\end{aligned}
$$
Hence, the in-sample error is given by
$$
\begin{aligned}
Err_{in} &= \sigma_{\epsilon}^2 + \frac{1}{N}\left(-\bm{X}^{\dagger}\bm{\epsilon}^{\mathcal{T}} \right)' \bm{X}'\bm{X}\left(-\bm{X}^{\dagger} \bm{\epsilon}^{\mathcal{T}}\right) \\
&= \sigma_{\epsilon}^2 + \frac{1}{N} \left(\bm{\epsilon}^{\mathcal{T}} \right)'\left(\bm{X}^{\dagger}\right)' \bm{X}'\bm{X}\bm{X}^{\dagger} \bm{\epsilon}^{\mathcal{T}} \\
&= \sigma_{\epsilon}^2 + \frac{1}{N} \left(\bm{\epsilon}^{\mathcal{T}} \right)' \bm{X} (\bm{X}'\bm{X})^{-1}\bm{X}'\bm{X}(\bm{X}'\bm{X})^{-1}\bm{X}' \bm{\epsilon}^{\mathcal{T}} \\
&=  \sigma_{\epsilon}^2 + \frac{1}{N} \left(\bm{\epsilon}^{\mathcal{T}} \right)' \underbrace{\bm{X} (\bm{X}'\bm{X})^{-1}\bm{X}'}_{\eqdef P_X} \bm{\epsilon}^{\mathcal{T}},
\end{aligned}
$$
where the projection matrix $P_X$ is the orthogonal projection onto the column space of $\bm{X}$. Assuming that $\bm{X}$ has full column rank, which is a necessary and sufficient condition for $\bm{X}'\bm{X}$ to be invertible, it follows that $\text{rank}(P_X) = p$.

In order to derive the expected in-sample error, we make use of the following result: If $P$ is a projection matrix with rank $r$ and $z \sim N(0,\bm{I})$, then the quadratic form $z'Pz$ is distributed as $\chi^2(r)$. In particular,
$$
\left(\sigma_{\epsilon}^{-1}\bm{\epsilon}^{\mathcal{T}}\right)'P_X \left(\sigma_{\epsilon}^{-1}\bm{\epsilon}^{\mathcal{T}}\right) \sim \chi^2(p).
$$
Consequently, we find that the expected in-sample error is given by
$$
\begin{aligned}
\mathbb{E}_{\mathcal{T}}\left[Err_{in} \right] &= \mathbb{E}_{\mathcal{T}}\left[\sigma_{\epsilon}^2 + \frac{\sigma_{\epsilon}^2}{N} \left(\sigma_{\epsilon}^{-1}\bm{\epsilon}^{\mathcal{T}}\right)'P_X \left(\sigma_{\epsilon}^{-1}\bm{\epsilon}^{\mathcal{T}}\right) \right] \\
&= \sigma_{\epsilon}^2 + \frac{\sigma_{\epsilon}^2}{N} \underbrace{\mathbb{E}_{\mathcal{T}}\left[\left(\sigma_{\epsilon}^{-1}\bm{\epsilon}^{\mathcal{T}}\right)'P_X \left(\sigma_{\epsilon}^{-1}\bm{\epsilon}^{\mathcal{T}}\right)  \right]}_{=p} \\
&= \sigma_{\epsilon}^2\left(1 + \frac{p}{N}\right).
\end{aligned}
$$


# Exercise 5
We use artificial data to perform LASSO and ridge regression. First, we set a random seed before the analysis. Then, we draw 100 observations from a 100-dimensional standard multivariate normal distribution. This is the matrix of covariates $X$ of dimension 100 $\times$ 100. Next, we draw 100 observations for the dependent variable given by 
$$y=\sum_{i=1}^{10}x_i+\epsilon, \quad \text{with} \ \epsilon \sim N(0,0.1)$$

```{r Exercise 5.1, echo=FALSE}
#library(MASS) # for mvrnorm
# set a random seed
set.seed(123)

# Draw 100 observations from a 100-dimensional standard multivariate normal distribution.
# Mean is 0. Cov. matrix is the identity matrix.

X <- mvrnorm(n=100, mu=rep(0,100), Sigma = diag(100))

# Draw 100 observations for the dependent variable.

y <- rowSums(X[,1:10]) + rnorm(100, mean=0,sd=0.1)
```
Now, we fit LASSO and Ridge models with different values of $\lambda$ using function \textit{glmnet} from package \textbf{glmnet}.
```{r Exercise 5.2, echo=FALSE}

# Note : default is intercept = TRUE
lasso.fit <- glmnet(X, y, alpha=1)
ridge.fit <- glmnet(X, y, alpha=0)

```
We plot the default plots for the returned objects.

```{r Exercise 5.3 new, echo=FALSE, fig.width=8, fig.height=5, out.width='\\textwidth'}
par(mar=c(5, 4, 4, 2) + 0.1)
plot(lasso.fit)
mtext("numb. of non-zero coeff.", side = 3, line = 2, col = 1)
title("LASSO", line = 3)
plot(ridge.fit, xlab="L2 norm")
mtext("numb. of non-zero coeff.", side = 3, line = 2, col = 1)
title("Ridge", line = 3)
```

Each line represents one variable. L1 norm is the regularization term of LASSO, and L2 the regularization term of Ridge. A small L1 or L2 norm represent a lot of regularization. On the other hand, a high L1 or L2 norm represent low regularization.
For LASSO, an L1 norm of zero gives the null model. Variables enter the model with increasing L1 norm, as their coefficients take non-zero values. On the top axis, we see the number of non-zero coefficients. 
For Ridge, an L2 norm of zero also gives the null model. But compared to Ridge, all variables enter right away. Also note, that some are negative.

Additionally, we plot the plots where the argument \textit{xvar} is set to \textit{"lambda"}.
```{r Exercise 5.4 neu, echo=FALSE, fig.width=8, fig.height=5, out.width='\\textwidth'}
par(mar=c(5, 4, 4, 2) + 0.1)
plot(lasso.fit, xvar="lambda", xlab = expression(paste("log ", lambda)))
mtext("numb. of non-zero coeff.", side = 3, line = 2, col = 1)
title("LASSO", line = 3)
plot(ridge.fit, xvar="lambda", xlab = expression(paste("log ", lambda)))
mtext("numb. of non-zero coeff.", side = 3, line = 2, col = 1)
title("Ridge", line = 3)
```
We basically see the same as before, just on another scale. This time the x-axis is log $\lambda$, the logarithm of the weight given to the regularization. $\lambda$ is therefore the complexity parameter. For $\lambda$ = 0, the solution is the OLS solution.

Next, we determine the number of non-zero coefficients in dependence of $\lambda$ for LASSO and ridge.

```{r Exercise 5.5 , echo=FALSE, fig.width=8, fig.height=5, out.width='\\textwidth'}
# lasso.fit$lambda
# ridge.fit$lambda

lasso.nonzero <- predict(lasso.fit, type = "nonzero")
ridge.nonzero <- predict(ridge.fit, type = "nonzero")

plot(lasso.fit$lambda, lengths(lasso.nonzero), xlab = expression(lambda),
ylab = "numb. of non-zero coeff.", ylim = c(0,22), main = "LASSO", type="l")

plot(ridge.fit$lambda, lengths(ridge.nonzero), xlab = expression(lambda),
ylab = "numb. of non-zero coeff.", ylim = c(0,110), main = "Ridge", type="l")
```
We can see, that for LASSO, the number of non-zero coefficients decreases with $\lambda$. We actually stay at 10 non-zero coefficents for $\lambda$ values between $\sim 0.05-0.55$. For Ridge all variables are in the model for all values of $\lambda$.
Finally, we find the model fit as measured by the deviance() (= RSS) in dependence of $\lambda$ for LASSO and Ridge.
```{r Exercise 5.6 , echo=FALSE, fig.width=8, fig.height=5, out.width='\\textwidth'}
lasso.rss <- deviance(lasso.fit)
ridge.rss <- deviance(ridge.fit)

plot(lasso.fit$lambda, lasso.rss, xlab = expression(lambda), ylab = "RSS", xlim = c(0,1.25),
ylim = c(0,1000), main = "LASSO", type="l")

plot(ridge.fit$lambda, ridge.rss, xlab = expression(lambda), ylab = "RSS", xlim = c(0,1250),
ylim = c(0,1000), main = "Ridge", type="l")


```
In general, a low $\lambda$ gives a better fit (lower RSS). Compared to LASS0, for Ridge the RSS increases faster with increasing $\lambda$. This could be a result of all variables entering already with low levels of $\lambda$.