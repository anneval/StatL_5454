---
title: "Statistical Learning (5454) - Assignment 2"  
author: "Matthias Hochholzer, Lukas Pirnbacher, Anne Valder"
date: "Due: 2024-04-22"
output: 
  pdf_document: 
    keep_tex: yes
header-includes:
   - \usepackage{titlesec}
   - \titleformat*{\section}{\normalfont\Large\bfseries\flushleft}
   - \titleformat*{\subsection}{\normalfont\large\bfseries\flushleft}
   - \titleformat*{\subsubsection}{\normalfont\normalsize\bfseries\flushleft}
   - \usepackage{amsmath}
   - \newcommand*{\defeq}{\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}=}
   - \newcommand*{\eqdef}{=\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}}
                     
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)
```


```{r,include=FALSE}
# Clear memory
rm(list=ls())
```

```{r, echo=FALSE, include=FALSE, warning=FALSE}
# Import libraries
library(tidyverse)
library(readr)
library(dplyr)
library(knitr)
library(Ecdat)
library(glmnet)
# change path! ElemStatLearn not directly availabe from CRAN
if (!require("ElemStatLearn")) install.packages("C:/Users/avalder/Downloads/ElemStatLearn_2015.6.26 (1).tar.gz", repos = NULL, type = "source")
library(ElemStatLearn)
library(boot)
library(caret)
```



<!-- EXERCISE 1 -->
# Exercise 1

After generating the simulated data, we fit four models using least squares. We then calculate the AIC values and determine the in-sample error estimates by drawing suitable test data from the data generating process using twice the negative log-likelihood as loss function. 


```{r, echo = FALSE}
#(a)
set.seed(1)
x <- rnorm(100)
y <- x - 2*x^2 + rnorm(100)
plot(x,y)

#(b) model specifications:
lm1 <- lm(y ~ x) # alternative: lm1 <- glm(y ~ x, data, family = gaussian()) 
lm2 <- lm(y ~ x +I(x^2))
lm3 <- lm(y ~ x + I(x^2) + I(x^3))
lm4 <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4))

# Calculate AIC values
aic1 <- AIC(lm1)
aic2 <- AIC(lm2)
aic3 <- AIC(lm3)
aic4 <- AIC(lm4)

# Print AIC values
aic_values <- c("linear" = aic1, "quadratic" = aic2, 
                "cubed" = aic3, "fourth_power" = aic4)
print("AIC:")
print(aic_values)


```


Plotting the simulated data, we see a clearly non-linear relationship between $x$ and $y$. In line with this, we observe that the linear specification (model 1) performs worst according to the AIC. The most preferable model in terms of AIC value is the second model, closely followed by models 3 and 4. 

```{r eval=FALSE, include=FALSE}


# #(c)
# # Generate test data from the data generating process
# x_test <- rnorm(100)
# y_test <- x_test - 2*x_test^2 + rnorm(100)
# 
# # Predict and calculate twice the negative log-likelihood as loss function
# lm1_test <- lm(y_test ~ x_test)
# lm2_test <- lm(y_test ~ x_test +I(x_test^2))
# lm3_test <- lm(y_test ~ x_test + I(x_test) + I(x_test^3))
# lm4_test <- lm(y_test ~ x_test + I(x_test^2) + I(x_test^3) + I(x_test^4))
# 
# lm_test <- list(lm1_test,lm2_test,lm3_test,lm4_test)
# 
# error_est <- lapply(lm_test, function(x) -2*logLik(x))
# print(error_est)
```

We now want to compute estimates of the in-sample error
$$
\text{Err}_{\text{in}} = \frac{1}{N} \sum_{i=1}^N \mathbb{E}\left[ L\left(y,\hat{f}(x_i)\right) \Big| \mathcal{T}\right],
$$
where the expectation is over the distribution of $y$ given $x_i$ and twice the negative log-likelihood is used as loss function. We approximate the expectations by computing means. To do so, we sample $N_{\text{error}}=1000$ error terms for each data point and then compute the corresponding new responses $y_{i,j} = \hat{f}(x_i) + \epsilon_j, \ j = 1,\ldots, 1000$.

In order to evaluate the likelihood function of $y$, we need estimates of the error variance $\sigma^2$, i.e. the conditional variance of $y$ given $x_i$. We consider both the MLE estimator of $\sigma^2$ (which is used in the R implementation of the function \textit{AIC()}) and the unbiased estimator that corrects for the degrees of freedom.

```{r, echo = FALSE}
# (c) alternative
# --> idea: keep x from training data; generate new errors (and hence new responses) for each training point
# --> generate 1000 new responses for each training point to approximate the
#     expectation in the definition of the in-sample error

in_sample_error_loglik <- function(lm, N_error = 1000, sigma_mle = TRUE){
  
  fitted_values <- lm$fitted.values # fitted values = mean of y|x_i
  N <- length(fitted_values) # sample size
  
  sigma_mle <- sqrt(mean((lm$residuals^2))) # MLE of sd of error term
  sigma <- ifelse(sigma_mle, sigma_mle, summary(lm)$sigma)# sd of error term = sd of y|x_i
  
  average_losses <- numeric(length(fitted_values)) # container for approximation of expectation at each data point x_i
  aic_aux <- numeric(length(fitted_values)) # container to manually compute AIC (as implemented in R, but rescaled by sample size)
  
  for (i in 1:length(fitted_values)) { # iterate over data points
    errors <- rnorm(N_error) # generate new errors
    y_new <- fitted_values[i] + errors # generate new responses for y_j = f(x_i) + error_j, j = 1,...,N_error
    
    average_losses[i] <- -2*mean(dnorm(
      y_new, mean = fitted_values[i], sd = sigma, log = TRUE))
    
    aic_aux[i] <- -2*dnorm(y[i], fitted_values[i], sigma_mle, log = TRUE)
  }
  
  # compute estimate of in-sample error
  in_sample_error <- mean(average_losses)
  # compute AIC manually (in contrast to R implementation: divide by sample size)
  my_aic <- mean(aic_aux) + 2*(length(lm$coefficients)+1)/N
  
  return(c(in_sample_error = in_sample_error,
           aic = my_aic))
  
}


lm_list <- list("linear" = lm1, "quadratic" = lm2,
                "cubed" = lm3, "fourth_power" = lm4)
# use mle of sigma to compute likelihood
print("MLE estimator of error variance:")
lapply(lm_list, in_sample_error_loglik)

# use unbiased estimator of sigma to compute likelihood
print("Unbiased estimator of error variance:")
lapply(lm_list, function(lm) in_sample_error_loglik(lm, sigma_mle = FALSE))


```
We find that models 2 to 4 have very similar in-sample errors. For those models, the discrepancy between our in-sample error estimates and the AIC values are rather small. The linear specification yields by far the largest in-sample error and also the gap between the estimated in-sample error and AIC is quite pronounced, which might be due to model misspecification.








<!-- EXERCISE 2 -->

# Exercise 2

In exercise 2, we perform leave-one-out cross-validation (LOOCV), $k$-fold cross-validation ($k$CV) and empirical bootstrapping based on the mean squared error loss that results from fitting the four models using least squares. The simulated data and specifications are the same as in task 1. The results of task (b) across the models are shown in table 1, indicated by "seed1" in the last column. 

(c) Next, we repeat (b) using another random seed (indicated by "seed2" in table 1). For LOOCV we obtain exactly the same results for all four models regardless of the different seed. This is because in general the randomness lies in the data generation.This consistency is expected because LOOCV is deterministic in this context, not relying on random sampling. Each observation is used once as a test set while the rest are used for training, and this process is repeated for each observation in the dataset. Therefore, changing the seed does not affect LOOCV results. For $k$CV we observe slight differences in the errors between seed 1 and seed 2 across the models. This variation can be attributed to the random partitioning of the data into $k$ folds. Each seed leads to a different random split, which can result in slight variations in the training and validation sets used in each fold, thus affecting the error estimates. Similar to $k$CV, the bootstrap error shows variations between the two seeds. Bootstrap resampling involves drawing samples with replacement from the original data set to create "new" data sets. The randomness introduced by the seed affects which observations are selected in each resample, leading to slight differences in the bootstrap error estimates between seed 1 and seed 2.
```{r}
#(a)
set.seed(1)
x <- rnorm(100)
y <- x - 2*x^2 + rnorm(100)
data <- data.frame(x = x, y = y)

#(b)
# List of formulas for the models
model_formulas <- list(
  y ~ x,
  y ~ x + I(x^2),
  y ~ x + I(x^2) + I(x^3),
  y ~ x + I(x^2) + I(x^3) + I(x^4)
)

# Function to calculate MSE
mse <- function(actual, predicted) {
  mean((actual - predicted)^2)
}

# LOOCV Error Calculation
loocv_error <- function(model_formula, data) {
  loocv_results <- sapply(1:nrow(data), function(i) {
    train_data <- data[-i, ]
    test_data <- data[i, ]
    model <- lm(model_formula, data = train_data)
    predicted <- predict(model, newdata = test_data)
    mse(test_data$y, predicted)
  })
  mean(loocv_results)
}

# k-Fold CV Error Calculation
kcv_error <- function(model_formula, data, k) {
  train_control <- trainControl(method = "cv", number = k)
  model <- train(model_formula, data = data, method = "lm", trControl = train_control)
  model$results$RMSE^2 # Squaring RMSE to get MSE
}

# Updated Bootstrap Error Calculation using boot package
bootstrap_error <- function(model_formula, data, nR) {
  bootFunction <- function(data, indices) {
    d <- data[indices, ] # Bootstrap sample
    model <- lm(model_formula, data = d)
    predicted <- predict(model, newdata = d)
    mse(d$y, predicted)
  }
  
  # Perform bootstrapping using the boot function
  results <- boot(data = data, statistic = bootFunction, R = nR)
  mean(results$t) # Return the mean of bootstrap MSE estimates
}

# Applying the methods
set.seed(123) # For reproducibility across methods

errors_seed1 <- lapply(model_formulas, function(formula) {
  list(
    LOOCV = loocv_error(formula, data),
    kCV = kcv_error(formula, data, 10), # Example: 10-fold CV
    Bootstrap = bootstrap_error(formula, data, 1000) # Example: 1000 bootstrap samples
  )
})

# Display the errors for each model and method
#errors_seed1

#(c)
set.seed(456) # change seed

errors_seed2 <- lapply(model_formulas, function(formula) {
  list(
    LOOCV = loocv_error(formula, data),
    kCV = kcv_error(formula, data, 10), # Example: 10-fold CV
    Bootstrap = bootstrap_error(formula, data, 1000) # Example: 1000 bootstrap samples
  )
})

# Display the errors for each model and method
#errors_seed2

# Convert lists to data frames with model and seed identifiers
df1 <- do.call(rbind, lapply(seq_along(errors_seed1), function(i) {
  cbind(data.frame(errors_seed1[[i]]), Model=paste("Model", i), Seed="Seed1")
}))
df2 <- do.call(rbind, lapply(seq_along(errors_seed2), function(i) {
  cbind(data.frame(errors_seed2[[i]]), Model=paste("Model", i), Seed="Seed2")
}))

combined_df <- rbind(df1, df2)
combined_df <- combined_df[order(combined_df$Model, combined_df$Seed), ]
combined_df$Model <- factor(combined_df$Model)
combined_df$Seed <- factor(combined_df$Seed)

#print(combined_df)
knitr::kable(combined_df, caption = "Comparison of LOOCV, kCV, and Bootstrap Errors", digits = 4)
```

(d) The MSEs in table 1 suggest that according to LOOCV the second model is the best. With  $k$CV the third model is the best and for the empirical bootstrap error it is the fourth model. Also, we see that the empirical
bootstrap error decreases further with model complexity, reflecting potential overfitting problems of this method. Remembering the plotted data in the beginning, these results are in line with our expectations since higher order regression equations fit much better to the data than the linear case.

(e) Last, we have a look at the statistical significance of the coefficient estimates that result from fitting each of the models using least squares. The results here align with our previous conclusions, in the sense that the quadratic term has the lowest p-value. At the same time, none of the coefficients corresponding to the higher-order terms in models 3 and 4 are statistically significant at a 5\% or even 10\% level. Thus, while the more complex models performed well in terms of $k$CV and empirical bootstrapping errors, model selection based on $t$-tests for the individual coefficients rejects them.
```{r}
#(e)
lm1 <- lm(y ~ x) # alternative: lm1 <- glm(y ~ x, data, family = gaussian()) 
lm2 <- lm(y ~ x +I(x^2))
lm3 <- lm(y ~ x + I(x^2) + I(x^3))
lm4 <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4))
models <- list(lm1,lm2,lm3,lm4)

print(lapply(models,function(x) summary(x)$coefficients))
```









<!-- EXERCISE 3 -->
# Exercise 3
Here we are using the wage data set available as data object $\textit{Schooling}$ in package $\textbf{Ecda}$. First, we omit observations with missing values and the variable wage76, and mutate variable mar76 into a binary variable. Next, we fit regularized linear regression models with lwage76 as dependent variable using only linear effects for the covariates and varying the $\alpha$ parameter for the elastic from 0 to 1 in step sizes of 0.2. We do this using using the function \textit{cv.glmnet} with 10-fold cross-validation, considering the MSE for a range of penalty values of $\lambda$. The argument "foldid" ensures that the same data partitions are used for each model fitting, making the comparisons fair and consistent. The default plot method for \textit{cv.glmnet} objects is used to visualize the cross-validation curves, which show the mean squared error (MSE) across a range of $\lambda$ values for each $\alpha$. The plots display the $\lambda$ values on the log scale along the bottom $x$-axis and the corresponding MSE on the $y$-axis. The vertical dotted line on the left in each plot indicates the $\lambda$ value that gives the minimum MSE. 

By varying $\alpha$, we essentially move between different regularization methods: $\alpha = 0$ corresponds to ridge regression (penalty on the square of coefficients), $\alpha = 1$ corresponds to lasso regression (penalty on the absolute value of coefficients). The top $x$-axis shows these numbers, which correspond to the model's complexity at different levels of regularization. As $\lambda$ increases, the regularization penalty becomes more severe, leading to more coefficients being shrunk to zero. The dual $x$-axes in the plot serve to simultaneously convey how $\lambda$ affects both the model's predictive accuracy (through the MSE shown on the $y$-axis) and its complexity (in terms of the number of predictors used, shown on the bottom $x$-axis). 

Looking at the different plots, given each value of $\alpha$, we observe with $\alpha$ closer to 1 more pronounced changes in the number of non-zero coefficients as $\lambda$ changes, reflecting the lasso's variable selection property. For ridge regression ($\alpha = 0$), the coefficients are shrunk towards zero but not exactly to zero, so the model complexity in terms of the non-zero coefficient count may not reduce as dramatically as with lasso or elastic net. The visualization helps in choosing an optimal $\lambda$ value, typically via the 1-standard error rule or by selecting the value of $\lambda$ that minimizes the cross-validation error.

```{r, echo=FALSE, fig.width=8, fig.height=5}
data("Schooling", package = "Ecdat")
#out.width='\\textwidth'
#(a)
#Omit observations with missing values and the variable wage76. Mutate the variable mar76 into a binary
##variable which is TRUE for value "married" and FALSE otherwise. 
Schooling <- Schooling %>%
  drop_na() %>%
  select(-wage76) %>% # Remove the wage76 variable
   mutate(mar76 = as.character(mar76),
         mar76 = mar76 == "yes")  

#head(Schooling)

#(b)
mf <- model.frame(lwage76 ~ ., data = Schooling)
X <- model.matrix(lwage76 ~ ., data = mf)[,-1]
y <- model.response(mf)


#y <- Schooling$lwage76 
#X <- as.matrix(Schooling[, setdiff(names(Schooling), "lwage76")]) # --> i think this does not handle the dummies (which are coded as characters) properly

# Generate foldid for consistent folds across all models
set.seed(123) 
foldid <- sample(1:10, size = nrow(X), replace = TRUE) # Example: 10-fold CV

# Sequence of alpha values from 0 to 1 in steps of 0.2
alpha_seq <- seq(0, 1, by = 0.2)

#(c)
cv_models <- lapply(alpha_seq, function(alpha) {
  cv.glmnet(X, y, alpha = alpha, foldid = foldid, family = "gaussian", type.measure = "mse")
})

# Visualize the results for each alpha
for (i in seq_along(cv_models)) {
  plot(cv_models[[i]])
  mtext(paste("Alpha =", alpha_seq[i]), side = 3, line = 3) # Adjust 'line' for positioning
}

# cvlm <- cv_models[[1]]
# 
# cbind(OLS = coef(cvlm, s = 0)[, 1],
#       lambda.min = coef(cvlm, s = "lambda.min")[, 1],
#       lambda.1se = coef(cvlm, s = "lambda.1se")[, 1],
#       Null = coef(cvlm, s = Inf)[, 1])

```



Next, to select the best value for $\lambda$, that balances model simplicity and accuracy, for each fixed value of $\alpha$ we look at either the value that minimizes the cross-validation error (lambda.min) or the 1-SE rule (lambda.1se), and then compare the selected models. We extract these $\lambda$ values from \textit{cv.glmnet} and then fit the final models on the full data set using these selected $\lambda$ value. To compare the selected models based on their complexity (number of non-zero coefficients), predicted values and MSE, we can use the \textit{coef} function to inspect the coefficients, make predictions with the \textit{predict} function and then calculate MSE for each model.

Table 2 summarizes all that. The choice between lambda.min and lambda.1se involves a trade-off between model simplicity and predictive accuracy. lambda.1se typically leads to simpler models (with potentially slightly higher MSE). We observe that as $\alpha$ increases from 0 to 1, the number of non-zero coefficients varies. The model with $\alpha = 0$ (ridge regression) maintains 27 non-zero coefficients for both lambda.min and lambda.1se. For other values of $\alpha$ (moving towards lasso regression), the number of non-zero coefficients tends to decrease, indicating sparser models. This is especially visible for $\alpha = 1.0$ with only 11 non-zero coefficients in case of lambda.1se, suggesting that lasso regularization enforces the most sparsity. 

Regarding predicted values, the CV MSE does not vary significantly across different $\alpha$  values, indicating that the change in $\alpha$ is not drastically affecting the model's predictive ability in this case. The lowest CV MSE for lambda.min appears at $\alpha = 0$, suggesting that the ridge model at its optimal $\lambda$ achieves a marginally better fit in terms of MSE. For the lambda.1se criterion, the CV MSE is slightly higher compared to lambda.min, which is expected as the 1-SE rule tends to select a simpler and more generalizable model at the expense of a slight increase in error. In the end, the choice between lambda.min and lambda.1se would depend on whether the priority is on the lowest possible MSE or on model simplicity.


<!--##Frage: classification performance - only if I have logistic regression and binary outcome or? -->

```{r}
library(glmnet)

# Initialize lists
chosen_lambda_min <- numeric(length(cv_models))
chosen_lambda_1se <- numeric(length(cv_models))
model_summary <- data.frame(alpha = numeric(), lambda = numeric(), criterion = character(),
                            non_zero_coefficients = integer(), cv_mse = numeric(), stringsAsFactors = FALSE)

# Loop over the models to select lambda and summarize models
for (i in seq_along(cv_models)) {
  cv_model <- cv_models[[i]]
  alpha <- alpha_seq[i]

  # Select lambda.min and lambda.1se
  lambda_min <- cv_model$lambda.min
  lambda_1se <- cv_model$lambda.1se
  chosen_lambda_min[i] <- lambda_min
  chosen_lambda_1se[i] <- lambda_1se

  # Summarize the model with lambda.min
  model_min <- glmnet(X, y, alpha = alpha, lambda = lambda_min, family = "gaussian")
  cv_mse_min <- cv_model$cvm[cv_model$lambda == lambda_min]
  model_summary <- rbind(model_summary,
                         data.frame(alpha = alpha, lambda = lambda_min, criterion = "min",
                                    non_zero_coefficients = sum(coef(model_min) != 0), cv_mse = cv_mse_min))

  # Summarize the model with lambda.1se
  model_1se <- glmnet(X, y, alpha = alpha, lambda = lambda_1se, family = "gaussian")
  cv_mse_1se <- cv_model$cvm[cv_model$lambda == lambda_1se]
  model_summary <- rbind(model_summary,
                         data.frame(alpha = alpha, lambda = lambda_1se, criterion = "1se",
                                    non_zero_coefficients = sum(coef(model_1se) != 0), cv_mse = cv_mse_1se))
}

# Display the model summary
#print(model_summary)
kable(model_summary)
```

Last, we inspect the best solution for $\alpha = 1$  (lasso regression) using the 1-SE rule. This approach tends to prefer simpler models with fewer non-zero coefficients, which can be advantageous for interpretability and generalization. To see which variables were selected (i.e., have non-zero coefficients) and their estimated coefficients, we use the \textit{coef} function from the \textbf{glmnet} package. We observe that only 10 variables (plus the intercept) are selected and that all of them have rather small coefficients and thus little influence on the response variable 'lwage76'. 


```{r}
#(e)
# Identify the model for alpha = 1
lambda_alpha_one_1se <- cv_models[[which(alpha_seq == 1)]]$lambda.1se

# Fit the final model using the selected lambda value
finmod_alpha_one_1se <- glmnet(X, y, alpha = 1, lambda = lambda_alpha_one_1se, family = "gaussian")

# Extract and display the non-zero coefficients
coef_alpha_one_1se <- coef(finmod_alpha_one_1se, s = lambda_alpha_one_1se)
print("Non-zero coefficients (including intercept):")
print(coef_alpha_one_1se)



```


To assess the goodness-of-fit, we calculate the correlation between the predicted values from the model and the observed values. Values closer to 1 or -1 indicating a stronger linear relationship between predictions and actual outcomes. In this case the correlation lies at roughly 0.5. This indicates a moderate positive linear relationship between the predicted and observed values. The model therefore has some predictive power, but it is not capturing all of the variability in the dependent variable.

```{r}
# Predict the values using the final model
pred_alpha_one_1se <- predict(finmod_alpha_one_1se, s = lambda_alpha_one_1se, newx = X) ### Question: seperate into train and test beginning? not in task though but slides.

# Calculate and display the correlation between predicted and observed values
cor_alpha_one_1se <- cor(pred_alpha_one_1se, y)
print(paste("Correlation between predicted and observed values for alpha = 1 (1-SE rule):", round(cor_alpha_one_1se,digits = 4)))
```





\newpage
<!-- EXERCISE 4 -->
# Exercise 4

In this exercise we use the South African heart disease data available as data object \textit{SAheart} in the package \textbf{Elem-StatLearn}. We fit a logistic regression model with Lasso penalty using only linear effects for the covariates and perform 20-fold cross-validation. We consider the deviance loss function, which is the default in the \textit{cv.glmnet} function of the \textbf{glmnet} package. Once again, we visualize the results using the default plot method. 

```{r, echo=FALSE}
data("SAheart", package = "ElemStatLearn")
SAheart <- SAheart

mf <- model.frame(chd ~ ., data = SAheart)
X <- model.matrix(chd ~ ., data = mf)[, -1] # covariates
y <- model.response(mf) # response variable

cv_logistic <- cv.glmnet(X,y, nfolds = 20, family = "binomial")
plot(cv_logistic)


```
Next, we want to focus on two particular values of the penalty $\lambda$, namely the one that minimizes the CV deviance loss and the one based on the 1-SE rule. These are given by
```{r, echo=FALSE}
print(cv_logistic)
```
We immediately see that lambda.min yields a more complex model (with 7 non-zero coefficients) than lambda.1se (with 5 non-zero coefficients). Looking at the coefficient estimates, we do find some notable differences between the two models, especially when it comes to the effect of 'famhistPresent':
```{r, echo = FALSE}

coef_output <- cbind(
  coef(cv_logistic, s = "lambda.min"),
  coef(cv_logistic, s = "lambda.1se")
)
colnames(coef_output) <- c("lambda.min", "lambda.1se")
print(coef_output)

```
Using the \textit{predict} function, we can now have a look at the predicted values of the two models, i.e. the predicted probabilities of coronary heart disease. Since we have not distinguished between test and training data in this exercise, we compute predictions for the whole sample.
```{r, echo = FALSE}
pred_prob_min <- predict(cv_logistic, s = "lambda.min", newx = X,
                         type = "response")
pred_prob_1se <- predict(cv_logistic, s = "lambda.1se", newx = X,
                         type = "response")

pred_prob_cv_logistic <- cbind(pred_prob_min, pred_prob_1se)

```
We can summarize the predictive performance of the two models by comparing the MSE for the whole sample:
```{r, echo = FALSE}
mse_prob_cv_logistic <- apply(pred_prob_cv_logistic, 2, 
                              function(fitted) mean((y-fitted)^2))
print(mse_prob_cv_logistic)
```
Since we consider in-sample predictions, it is not very surprising that the more complex model does better in terms of MSE. Moreover, the MSE might not be a suitable performance measure in the context of logistic regression. Hence, we now compare the misclassification rates of the two models.
```{r, echo = FALSE}

pred_class_min <- predict(cv_logistic, s = "lambda.min", newx = X,
                         type = "class")
pred_class_1se <- predict(cv_logistic, s = "lambda.1se", newx = X,
                         type = "class")

pred_class_cv_logistic <- cbind(pred_class_min, pred_class_1se)
miscl_rates_cv_logistic <- apply(pred_class_cv_logistic, 2,
                                 function(fitted) sum(y!=fitted)/length(y))
print(miscl_rates_cv_logistic)


```
We find that the two models yield the same misclassification rate in-sample. On the contrary, they quite drastically differ with respect to their false-positive and false-negative rates:  
```{r, echo = FALSE}
fp_rates_cv_logistic <- apply(pred_class_cv_logistic, 2,
                              function(fitted){
                                sum(fitted[y==0] == 1)/length(y[y==0])
                              }) 
print("False-positive rates:")
print(fp_rates_cv_logistic)

fn_rates_cv_logistic <- apply(pred_class_cv_logistic, 2,
                              function(fitted){
                                sum(fitted[y==1] == 0)/sum(y)
                              }) 

print("False-negative rates:")
print(fn_rates_cv_logistic)

positives_cv_logistic <- apply(pred_class_cv_logistic, 2,
                               function(fitted) sum(as.numeric(fitted)))

```
Overall, we find that the false-positive rates of both models are rather low, while the false-negative rates are fairly high, especially for the simpler model corresponding to lambda.1se. This is in line with the observation that `r round(100*sum(y)/length(y),1)`\% of the individuals in the sample suffer from coronary heart disease while our models predict a prevalence of `r round(100*sum(pred_class_min==1)/length(y),1)`\% and `r round(100*sum(pred_class_1se==1)/length(y),1)`\%, respectively.

On a final note, we have so far only considered models trained in the cross-validation procedure. In general, it seems more appropriate to re-estimate the models for the selected values of $\lambda$ using the full data available. However, in this exercise the resulting differences in parameter estimates are negligible and the predicted classifications do not change at all.
```{r include=FALSE}
full_logistic <- glmnet(X,y, family = "binomial", nfolds = 20,
                        lambda = c(cv_logistic$lambda.min, cv_logistic$lambda.1se))
coef_full <- coef(full_logistic)
pred_class_full_min <- predict(full_logistic, s = cv_logistic$lambda.min, newx = X,
                               type = "class")
pred_class_full_1se <- predict(full_logistic, s = cv_logistic$lambda.1se, newx = X,
                               type = "class")

sum(pred_class_full_1se!=pred_class_1se)
sum(pred_class_full_min!=pred_class_min)

```









<!-- EXERCISE 5 -->
# Exercise 5

We load the acoustic-phonetic continuous speech corpus dataset \textit{phoneme} from the package \textit{ElemStatLearn}.
There are five classes contained in the dataset. The covariates are log-periodograms of length 256. In the following two-group classification is performed using only the classes "aa" and "ao". Therefore, we subset the data accordingly.

Afterwards, we visualize the data by plotting the covariate values on the y-axis and the index on the x-axis using line plots. "aa" is plotted in \textit{magenta} and "ao" in \textit{blue}. "aa" seems to have overall slightly higher values than "ao".

```{r}
data("phoneme", package = "ElemStatLearn") # load the data phoneme
set.seed(12345) # set the seed
phoneme <- subset(phoneme, g=="aa"| g=="ao") #g ... a factor with levels "aa" "ao" "dcl" "iy" and "sh"

phoneme$phonclass <- ifelse(phoneme$g == "aa", 0, 1) # 0 for "aa" and 1 for "ao"

phonclass <- phoneme[,259] # dependent variable, classes
phonX <- as.matrix(phoneme[,1:256]) # covariates
phondata <- as.data.frame(cbind(phonclass,phonX)) # data frame

matplot(1:256, t(phonX[1:1717,]), type= "l", ylab = "log-periodograms/coveriate values", xlab= "index", col = ifelse(phonclass==0, "magenta", "blue"), main="Line plot" )
legend("bottomleft", legend = c("aa", "ao"), col=c("magenta", "blue"),lty = c("solid","solid"),title="class")

```

We select 1000 samples as training data and use the remaining ones as test data.
```{r}
phontraini <- sample(1:nrow(phondata),1000,replace=FALSE) # sample indicator
phontrain <- phondata[phontraini,] # select sample data
phonX_train <- as.matrix(phontrain[,-1]) # covariates of sample
phonclass_train <- phontrain[,1] # dependent variable of sample


phontest <- phondata[-phontraini,] # select test data, remaining observations
phonX_test <- as.matrix(phontest[,-1]) # covariates of test
phonclass_test <- phontest[,1] # dependent variable of test
```

We then fit a logistic regression model to the training data using all covariates and determine the misclassification rate and the average log-likelihood value on the training and test data.

```{r}
phonfit.train <- glm(phonclass ~ ., data=phontrain, family=binomial) # fit logistic regression with all covariates

phonprob.train <- predict(phonfit.train, type = "response") # predict probabilities of being class aa or ao in training

phonprob.test <- predict(phonfit.train, newdata= phontest, type = "response") # predict probabilities of being class aa or ao in test

```

We calculate the average log-likelihood value on the training data in two ways. The first value is obtained by the \textit{logLik} function. 
The second value is obtained via the likelihood function of the (multivariate) Bernoulli distribution, which is given by 
$$\prod_{i=1}^n p^{x_i}(1-p)^{1-x_i}$$
Taking the logarithm gives us the average log-lieklihood.
$$ \sum_{i=1}^n x_i \log(p)+(1-x_i)\log(1-p) $$
```{r}
# Averge log-likelihood
logLik(phonfit.train)
logLik.phontrain <- sum(phontrain$phonclass * log(phonprob.train)+(1-phontrain$phonclass)* log(1-phonprob.train))
logLik.phontrain
```
Of course, they are identical. 

The average log-likelihood value on the test data is calculated with the second approach.
```{r}
logLik.phontest <- sum(phontest$phonclass * log(phonprob.test)+(1-phontest$phonclass)* log(1-phonprob.test))
logLik.phontest
```

Higher average log-likelihood values speak for a better fit of the model. Therefore, we also see a lower value for the test data.

To calculate the misclassifications rate we turn the predicted probabilities into binary outcomes (1 if $>0.5$, 0 otherwise). Comparing the prediction to the actual class gives us the misclassification rate for the training data.

```{r}
# Misclassifications rate

phontrain$phonclass.pred <- ifelse(phonprob.train >0.5,1,0) # Make the prob. binary. for train 

phontrain.misclass <- nrow(phontrain[phontrain$phonclass != phontrain$phonclass.pred,]) / nrow(phontrain)
phontrain.misclass
```
As well as for the test data.
```{r}
phontest$phonclass.pred <- ifelse(phonprob.test >0.5,1,0) # Make the prob. binary. for test

phontest.misclass <- nrow(phontest[phontest$phonclass != phontest$phonclass.pred,]) / nrow(phontest)
phontest.misclass
```
As expected, the misclassification rate is higher for the test data.

The complexity of this model can be reduced by restricting the regression coefficients to vary only smoothly
over the covariates, i.e., regression coefficients for close covariates are similar.
To achieve this, we use splines. 
We create a 12-dimensional model matrix $X^*$ based on natural cubic splines which we use to fit the logistic regression model instead of the
256-dimensional X.
Specifically, we fit a logistic regression model to the training data using $X^*$ as model matrix and determine the misclassification rate and the average log-likelihood value on the training and test data.


```{r}
library("splines")
H <-ns(1:256, df = 12) # 12-dimensional natural cubic splines
phonX.star_train <- phonX_train %*% H # new train matrix
phontrain.star <- as.data.frame(cbind(phonclass_train, phonX.star_train))

phonX.star_test <- phonX_test %*% H # new test matrix
phontest.star <- as.data.frame(cbind(phonclass_test, phonX.star_test))

phonfit.train.star <- glm(phonclass_train ~ ., data=phontrain.star, family=binomial)

phonprob.train.star <- predict(phonfit.train.star, type = "response")

phonprob.test.star <- predict(phonfit.train.star, newdata= phontest.star, type = "response")
```

The average log-likelihood value on the training data is calculated as before.

```{r}
# Averge log-likelihood
logLik(phonfit.train.star)
logLik.phontrain.star <- sum(phontrain.star$phonclass_train * log(phonprob.train.star)+(1-phontrain.star$phonclass_train)* log(1-phonprob.train.star))
logLik.phontrain.star
```
The average log-likelihood value on the test data now is
```{r}
logLik.phontest.star <- sum(phontest.star$phonclass_test * log(phonprob.test.star)+(1-phontest.star$phonclass_test)* log(1-phonprob.test.star))
logLik.phontest.star
```

For the misclassification rate of the training data we also perform the same steps as before.

```{r}
# Misclassifications rate
phontrain.star$phonclass.pred <- ifelse(phonprob.train.star >0.5,1,0)
phontrain.misclass.star <- nrow(phontrain.star[phontrain.star$phonclass_train != phontrain.star$phonclass.pred,]) / nrow(phontrain.star)
phontrain.misclass.star
```


The misclassification rate of the test data is
```{r}
phontest.star$phonclass.pred <- ifelse(phonprob.test.star >0.5,1,0)
phontest.misclass.star <- nrow(phontest.star[phontest.star$phonclass_test != phontest.star$phonclass.pred,]) / nrow(phontest.star)
phontest.misclass.star
```

Next, we vary the degrees of freedom in the spline basis expansion using 2 to the power of 1 to 8, i.e., 2,4,8,16,32,64,128 and 256.
Again, we calculate the misclassifiation rate and the average log-likelihood on the training and test data for each of the fitted models. We create helper functions that generalise the steps of the previous 12-dimesional model to x-dimesional models.

```{r}
# Help function to fit logistic regression
help.func1 <- function(x){
  H.x <- ns(1:256,df=x)
  phonX.star_trainx <- phonX_train %*% H.x
  phontrain.starx <- as.data.frame(cbind(phonclass_train, phonX.star_trainx))
  phonX.star_testx <- phonX_test %*% H.x
  phontest.starx <- as.data.frame(cbind(phonclass_test, phonX.star_testx))

  phonfit.train.starx <- glm(phonclass_train ~ ., data=phontrain.starx, family=binomial)

  return(phonfit.train.starx) }

# Help function for train data
help.func2 <- function(x){
  H.x <- ns(1:256,df=x)
  phonX.star_trainx <- phonX_train %*% H.x
  phontrain.starx <- as.data.frame(cbind(phonclass_train, phonX.star_trainx))

  phonfit.train.starx <- glm(phonclass_train ~ ., data=phontrain.starx, family=binomial)

  phontrain.starx$phonprob<- predict(phonfit.train.starx, type = "response")

  # Averge log-likelihood
  phontrain.starx$logLik<- sum(phontrain.starx$phonclass_train * log(phontrain.starx$phonprob)+(1-phontrain.starx$phonclass_train)* log(1-phontrain.starx$phonprob))
  
  # Misclassifications rate
  phontrain.starx$phonclass.pred <- ifelse(phontrain.starx$phonprob >0.5,1,0)
  phontrain.starx$misclass <- nrow(phontrain.starx[phontrain.starx$phonclass_train != phontrain.starx$phonclass.pred,]) / nrow(phontrain.starx)
  
  return(phontrain.starx)
  
}

# Help function for test data
help.func3 <- function(x){
  H.x <- ns(1:256,df=x)
  phonX.star_trainx <- phonX_train %*% H.x
  phontrain.starx <- as.data.frame(cbind(phonclass_train, phonX.star_trainx))
  phonX.star_testx <- phonX_test %*% H.x
  phontest.starx <- as.data.frame(cbind(phonclass_test, phonX.star_testx))

  phonfit.train.starx <- glm(phonclass_train ~ ., data=phontrain.starx, family=binomial)
  
  phontest.starx$phonprob <- predict(phonfit.train.starx, newdata= phontest.starx, type = "response")
  
  # Averge log-likelihood
  phontest.starx$logLik <- sum(phontest.starx$phonclass_test * log(phontest.starx$phonprob)+(1-phontest.starx$phonclass_test)* log(1-phontest.starx$phonprob))
  
  # Misclassifications rate
  phontest.starx$phonclass.pred <- ifelse(phontest.starx$phonprob >0.5,1,0)
  phontest.starx$misclass <- nrow(phontest.starx[phontest.starx$phonclass_test != phontest.starx$phonclass.pred,]) / nrow(phontest.starx)
  
  return(phontest.starx)
}

# Use the functions above for 2,4,8,16,32,64,128 and 256
phonfit2 <- help.func1(2)
phontrain2 <- help.func2(2)
phontest2 <- help.func3(2)

phonfit4 <- help.func1(4)
phontrain4 <- help.func2(4)
phontest4 <- help.func3(4)

phonfit8 <- help.func1(8)
phontrain8 <- help.func2(8)
phontest8 <- help.func3(8)

phonfit16 <- help.func1(16)
phontrain16 <- help.func2(16)
phontest16 <- help.func3(16)

phonfit32 <- help.func1(32)
phontrain32 <- help.func2(32)
phontest32 <- help.func3(32)

phonfit64 <- help.func1(64)
phontrain64 <- help.func2(64)
phontest64 <- help.func3(64)

phonfit128 <- help.func1(128)
phontrain128 <- help.func2(128)
phontest128 <- help.func3(128)

phonfit256 <- help.func1(256)
phontrain256 <- help.func2(256)
phontest256 <- help.func3(256)

# All misclass into a list
phontrainsum.misclass <- list(phontrain2$misclass[1],phontrain4$misclass[1],phontrain8$misclass[1],phontrain16$misclass[1],phontrain32$misclass[1],phontrain64$misclass[1],phontrain128$misclass[1],phontrain256$misclass[1])
phontestsum.misclass <- list(phontest2$misclass[1],phontest4$misclass[1],phontest8$misclass[1],phontest16$misclass[1],phontest32$misclass[1],phontest64$misclass[1],phontest128$misclass[1],phontest256$misclass[1])

# All logLik into a list
phontrainsum.logLik <- list(phontrain2$logLik[1],phontrain4$logLik[1],phontrain8$logLik[1],phontrain16$logLik[1],phontrain32$logLik[1],phontrain64$logLik[1],phontrain128$logLik[1],phontrain256$logLik[1])
phontestsum.logLik <- list(phontest2$logLik[1],phontest4$logLik[1],phontest8$logLik[1],phontest16$logLik[1],phontest32$logLik[1],phontest64$logLik[1],phontest128$logLik[1],phontest256$logLik[1])

```

After running all the helper functions for all x-dimesions we are able to compare the misclassification rates and average log-likelihoods based on training and test data sets visually for the different degrees of freedom.

```{r, echo=FALSE, fig.width=8, fig.height=5}
phontrainmisclass.data <- as.data.frame(cbind(1:8,unlist(phontrainsum.misclass))) # into data frame

dfreedom <- c("2" ,"4", "8", "16", "32", "64", "128", "256") # x axis labelling

plot(phontrainmisclass.data, xlab="degrees of freedom", ylab="misclassification rate", xaxt="n", main= "Misclassification: Training data", pch=17, ylim=c(0,1),col="blue")
grid()
axis(1, at = 1:8, labels= dfreedom , las=2)


phontestmisclass.data <- as.data.frame(cbind(1:8,unlist(phontestsum.misclass)))


plot(phontestmisclass.data, xlab="degrees of freedom", ylab="misclassification rate", xaxt="n", main= "Misclassification: Test data", pch=17, ylim=c(0,1),col="magenta")
grid()
axis(1, at = 1:8, labels= dfreedom , las=2)


phontrainlogLik.data <- as.data.frame(cbind(1:8,unlist(phontrainsum.logLik)))


plot(phontrainlogLik.data, xlab="degrees of freedom", ylab="log likelihood", xaxt="n", main= "Log likelihood: Training data", pch=18, ylim=c(-900,-150),col="blue")
grid()
axis(1, at = 1:8, labels= dfreedom , las=2)

phontestlogLik.data <- as.data.frame(cbind(1:8,unlist(phontestsum.logLik)))


plot(phontestlogLik.data, xlab="degrees of freedom", ylab="log likelihood", xaxt="n", main= "Log likelihood: Test data", pch=18, ylim=c(-900,-150), col="magenta")
grid()
axis(1, at = 1:8, labels= dfreedom , las=2)


```
The more we restrict the regression coefficients to vary only smoothly over the covariates (lower degrees of freedom), the higher is the misclassification rate for the training data. Theoretically, higher smooth avoids overfitting to the training data. But it can also loose important information. By looking at the misclassification rate of the test data, we can see that medium levels of degrees of freedom (16-64) result in the lowest misclassification rate. Low smoot, or in other words high degrees of freedom, show the expected overfitting. Too much sommot, low degrees of freedom, seems to loose important information and hence also increases the misclassification rate on the test data.

On the training data, the average log-likelihood worsens with the smooth of the covariates. Looking at the average log-likelihood of the test data we see that medium levels of degrees of freedom (16-64) are the best choice. They avoid overfitting and don't loose too much information. Note the very low average log-likelihood due to overiftting for 256 degrees of freedom. On the other end, with high smooth the loss isn't as high.