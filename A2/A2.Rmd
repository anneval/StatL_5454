---
title: "Statistical Learning (5454) - Assignment 2"  
author: "Matthias Hochholzer, Lukas Pirnbacher, Anne Valder"
date: "Due: 2024-04-22"
output: 
  pdf_document:
    keep_tex: yes
header-includes:
   - \usepackage{titlesec}
   - \titleformat*{\section}{\normalfont\Large\bfseries\flushleft}
   - \titleformat*{\subsection}{\normalfont\large\bfseries\flushleft}
   - \titleformat*{\subsubsection}{\normalfont\normalsize\bfseries\flushleft}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)
```


```{r,include=FALSE}
# Clear memory
rm(list=ls())
```

```{r, echo=FALSE, include=FALSE, warning=FALSE}
# Import libraries
library(tidyverse)
library(readr)
library(dplyr)
library(knitr)
library(Ecdat)
library(glmnet)
# change path! ElemStatLearn not directly availabe from CRAN
if (!require("ElemStatLearn")) install.packages("C:/Users/avalder/Downloads/ElemStatLearn_2015.6.26 (1).tar.gz", repos = NULL, type = "source")
library(ElemStatLearn)
library(boot)
library(caret)
```

<!-- EXERCISE 1 -->
# Exercise 1

After generating the simulated data we fit four models using least squares, we calculate the AIC values and  determine the in-sample error estimates by drawing suitable test data from the data generating process using twice the negative log-likelihood as loss function. Plotting the data we see a clearly non-linear relationship. In line with this we observe that the most preferable model in terms of AIC value is the second model. In terms of negative log-likelihood the fourth model appears to be the best. However, closely followed by model 2. The linear model (model 1) performs the worst out of all models measured by AIC and the negative log-likelihood.
```{r, echo=TRUE}
#(a)
set.seed(1)
x <- rnorm(100)
y <- x - 2*x^2 + rnorm(100)
plot(x,y)

#(b) model specifications:
lm1 <- lm(y ~ x) # alternative: lm1 <- glm(y ~ x, data, family = gaussian()) 
lm2 <- lm(y ~ x +I(x^2))
lm3 <- lm(y ~ x + I(x^2) + I(x^3))
lm4 <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4))
```


```{r}
# Calculate AIC values
aic1 <- AIC(lm1)
aic2 <- AIC(lm2)
aic3 <- AIC(lm3)
aic4 <- AIC(lm4)

# Print AIC values
aic_values <- c(aic1, aic2, aic3, aic4)
print(aic_values)

#(c)
# Generate test data from the data generating process
x_test <- rnorm(100)
y_test <- x_test - 2*x_test^2 + rnorm(100)

# Predict and calculate twice the negative log-likelihood as loss function
lm1_test <- lm(y_test ~ x_test)
lm2_test <- lm(y_test ~ x_test +I(x_test^2))
lm3_test <- lm(y_test ~ x_test + I(x_test) + I(x_test^3))
lm4_test <- lm(y_test ~ x_test + I(x_test^2) + I(x_test^3) + I(x_test^4))

lm_test <- list(lm1_test,lm2_test,lm3_test,lm4_test)

error_est <- lapply(lm_test, function(x) -2*logLik(x))
print(error_est)
```

<!-- EXERCISE 2 -->

# Exercise 2

In exercise 2, we perform leave-one-out cross-validation (LOOCV), k-fold cross-validation (kCV) and empirical bootstrapping based on the mean squared error loss that result from fitting the four models using least squares. The simulated data and specifications are the same as in task 1. The results of task (b) across the models are shown in table 1, incidcated by "seed1" in the last column. 

(c) Next, we repeat (b) using another random seed (indicated by "seed2" in table 1). For LOOCV we obtain exactly the same results for all four models regardless of the different seed. This is because in general the randomness lies in the data generation.This consistency is expected because LOOCV is deterministic in this context, not relying on random sampling. Each observation is used once as a test set while the rest are used for training, and this process is repeated for each observation in the dataset. Therefore, changing the seed does not affect LOOCV results. For kCV we observe slight differences in the errors between seed 1 and seed 2 across the models. This variation can be attributed to the random partitioning of the data into k folds. Each seed leads to a different random split, which can result in slight variations in the training and validation sets used in each fold, thus affecting the error estimates. Similar to kCV, the bootstrap error shows variations between the two seeds. Bootstrap resampling involves drawing samples with replacement from the original data set to create "new" data sets. The randomness introduced by the seed affects which observations are selected in each resample, leading to slight differences in the bootstrap error estimates between seed 1 and seed 2.
```{r}
#(a)
set.seed(1)
x <- rnorm(100)
y <- x - 2*x^2 + rnorm(100)
data <- data.frame(x = x, y = y)

#(b)
# List of formulas for the models
model_formulas <- list(
  y ~ x,
  y ~ x + I(x^2),
  y ~ x + I(x^2) + I(x^3),
  y ~ x + I(x^2) + I(x^3) + I(x^4)
)

# Function to calculate MSE
mse <- function(actual, predicted) {
  mean((actual - predicted)^2)
}

# LOOCV Error Calculation
loocv_error <- function(model_formula, data) {
  loocv_results <- sapply(1:nrow(data), function(i) {
    train_data <- data[-i, ]
    test_data <- data[i, ]
    model <- lm(model_formula, data = train_data)
    predicted <- predict(model, newdata = test_data)
    mse(test_data$y, predicted)
  })
  mean(loocv_results)
}

# k-Fold CV Error Calculation
kcv_error <- function(model_formula, data, k) {
  train_control <- trainControl(method = "cv", number = k)
  model <- train(model_formula, data = data, method = "lm", trControl = train_control)
  model$results$RMSE^2 # Squaring RMSE to get MSE
}

# Updated Bootstrap Error Calculation using boot package
bootstrap_error <- function(model_formula, data, nR) {
  bootFunction <- function(data, indices) {
    d <- data[indices, ] # Bootstrap sample
    model <- lm(model_formula, data = d)
    predicted <- predict(model, newdata = d)
    mse(d$y, predicted)
  }
  
  # Perform bootstrapping using the boot function
  results <- boot(data = data, statistic = bootFunction, R = nR)
  mean(results$t) # Return the mean of bootstrap MSE estimates
}

# Applying the methods
set.seed(123) # For reproducibility across methods

errors_seed1 <- lapply(model_formulas, function(formula) {
  list(
    LOOCV = loocv_error(formula, data),
    kCV = kcv_error(formula, data, 10), # Example: 10-fold CV
    Bootstrap = bootstrap_error(formula, data, 1000) # Example: 1000 bootstrap samples
  )
})

# Display the errors for each model and method
#errors_seed1

#(c)
set.seed(456) # change seed

errors_seed2 <- lapply(model_formulas, function(formula) {
  list(
    LOOCV = loocv_error(formula, data),
    kCV = kcv_error(formula, data, 10), # Example: 10-fold CV
    Bootstrap = bootstrap_error(formula, data, 1000) # Example: 1000 bootstrap samples
  )
})

# Display the errors for each model and method
#errors_seed2

# Convert lists to data frames with model and seed identifiers
df1 <- do.call(rbind, lapply(seq_along(errors_seed1), function(i) {
  cbind(data.frame(errors_seed1[[i]]), Model=paste("Model", i), Seed="Seed1")
}))
df2 <- do.call(rbind, lapply(seq_along(errors_seed2), function(i) {
  cbind(data.frame(errors_seed2[[i]]), Model=paste("Model", i), Seed="Seed2")
}))

combined_df <- rbind(df1, df2)
combined_df <- combined_df[order(combined_df$Model, combined_df$Seed), ]
combined_df$Model <- factor(combined_df$Model)
combined_df$Seed <- factor(combined_df$Seed)

#print(combined_df)
knitr::kable(combined_df, caption = "Comparison of LOOCV, kCV, and Bootstrap Errors", digits = 4)
```

(d) The MSEs in table 1 suggest that according to LOOCV the second model is the best. With  kCV the third model is the best and for the empirical bootstrap error it is the fourth model. Also, we see that the empirical
bootstrap error decreases further with model complexity, reflecting potential overfitting problems of this method. Remembering the plotted data in the beginning these results are in line with our expectations since higher order regression equations fit much better to the data than the linear case.

(e) Last, we have a look at the statistical significance of the coefficient estimates that result from fitting each of the models using least squares. The results here align with our previous conclusions, as the quadratic term has the lowest p-value.
```{r}
#(e)
lm1 <- lm(y ~ x) # alternative: lm1 <- glm(y ~ x, data, family = gaussian()) 
lm2 <- lm(y ~ x +I(x^2))
lm3 <- lm(y ~ x + I(x^2) + I(x^3))
lm4 <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4))
models <- list(lm1,lm2,lm3,lm4)

knitr::kable(lapply(models,function(x) summary(x)$coefficients))
```


<!-- EXERCISE 3 -->
# Exercise 3
Here we are using the wage data set available as data object $\textit{Schooling}$ in package $\textbf{Ecda}$. First, we omit observations with missing values and the variable wage76, and mutate variable mar76 into a binary variable. Next, we fit regularized linear regression models with lwage76 as dependent variable using only linear effects for the covariates and varying the $\alpha$ parameter for the elastic from 0 to 1 in step sizes of 0.2. We do this using using cv.glmnet with 10-fold cross-validation considering the MSE for a range of penalty values of $\lambda$. The argument foldid ensures that the same data partitions are used for each model fitting, making the comparisons fair and consistent. The default plot method for cv.glmnet objects is used to visualize the cross-validation curves, which show the mean squared error (MSE) across a range of $\lambda$ values for each $\alpha$. The plot displays the $\lambda$ values on the log scale along the bottom x-axis and the corresponding MSE on the y-axis. The vertical dotted line in each plot indicates the $\lambda$ value that gives the minimum MSE. By varying $\alpha$, we essentially move between different regularization methods: $\alpha = 0$ corresponds to ridge regression (penalty on the square of coefficients),$\alpha = 1$ corresponds to lasso regression (penalty on the absolute value of coefficients). The top x-axis shows these numbers, which correspond to the model's complexity at different levels of regularization. As lambda increases, the regularization penalty becomes more severe, leading to more coefficients being shrunk to zero.The dual x-axes in the plot serve to simultaneously convey how lambda affects both the model's predictive accuracy (through the MSE shown on the y-axis) and its complexity (in terms of the number of predictors used, shown on the bottom x-axis). Looking at the different plots given each value of $\alpha$ we observe with $\alpha$ closer to 1 more pronounced changes in the number of non-zero coefficients as lambda changes, reflecting the lasso's variable selection property. For ridge regression ($\alpha = 0$ ), the coefficients are shrunk towards zero but not exactly to zero, so the model complexity in terms of the non-zero coefficient count may not reduce as dramatically as with lasso or elastic net.

```{r, echo=FALSE, fig.width=8, fig.height=5, out.width='\\textwidth'}
data("Schooling", package = "Ecdat")
#(a)
#Omit observations with missing values and the variable wage76. Mutate the variable mar76 into a binary
##variable which is TRUE for value "married" and FALSE otherwise. 
Schooling <- Schooling %>%
  drop_na() %>%
  select(-wage76) %>% # Remove the wage76 variable
   mutate(mar76 = as.character(mar76),
         mar76 = mar76 == "yes")  

#head(Schooling)

#(b)
y <- Schooling$lwage76
X <- as.matrix(Schooling[, setdiff(names(Schooling), "lwage76")])

# Generate foldid for consistent folds across all models
set.seed(123) 
foldid <- sample(1:10, size = nrow(X), replace = TRUE) # Example: 10-fold CV

# Sequence of alpha values from 0 to 1 in steps of 0.2
alpha_seq <- seq(0, 1, by = 0.2)

#(c)
cv_models <- lapply(alpha_seq, function(alpha) {
  cv.glmnet(X, y, alpha = alpha, foldid = foldid, family = "gaussian", type.measure = "mse")
})

# Visualize the results for each alpha
for (i in seq_along(cv_models)) {
  plot(cv_models[[i]])
  mtext(paste("Alpha =", alpha_seq[i]), side = 3, line = 3) # Adjust 'line' for positioning
}

# cvlm <- cv_models[[1]]
# 
# cbind(OLS = coef(cvlm, s = 0)[, 1],
#       lambda.min = coef(cvlm, s = "lambda.min")[, 1],
#       lambda.1se = coef(cvlm, s = "lambda.1se")[, 1],
#       Null = coef(cvlm, s = Inf)[, 1])

```

This visualization helps in choosing an optimal lambda value , typically via the 1-standard error rule or by selecting the lambda that minimizes the cross-validation error.

Next, to select the best value for $\lambda$, that balances model simplicity and accuracy, for each fixed value of $\alpha$ we look at either the value that minimizes the cross-validation error (lambda.min) or the 1-SE rule (lambda.1se), and then compare the selected models. We extract these $\lambda$ values from cv.glmnet and then fit the final models on the full data set using these selected $\lambda$ value. To compare the selected models based on their complexity (number of non-zero coefficients), predicted values, and MSE, we can use the coef function to inspect the coefficients, make predictions with the predict function, and then calculate MSE for each model.
Table 2 summarizes all that. The choice between lambda.min and lambda.1se involves a trade-off between model simplicity and predictive accuracy. lambda.1se typically leads to simpler models (with potentially slightly higher MSE). We observe that as $\alpha$ increases from 0.0 to 1.0, the number of non-zero coefficients varies. The model with $\alpha$ = 0.0 (ridge regression) maintains 10 non-zero coefficients for both lambda.min and lambda.1se. For other values of $\alpha$ (moving towards lasso regression), the number of non-zero coefficients tends to decrease, indicating sparser models. This is especially visible for $\alpha = 1.0$ with only 5 or 6 non-zero coefficients, suggesting that lasso regularization enforces the most sparsity. The least complex model is the one with  $\alpha = 0.2$  using the lambda.1se criterion, having only 5 non-zero coefficients, whereas the most complex models with respect to the number of coefficients are all those with $\alpha = 0.0$.  Regarding predicited values, the cv_mse does not vary significantly across different $\alpha$  values, indicating that the change in $\alpha$  is not drastically affecting the model's predictive ability in this case. The lowest cv_mse for lambda.min appears at $\alpha = 1.0$ , suggesting that the lasso model at its optimal lambda achieves a marginally better fit in terms of MSE. For the lambda.1se criterion, the cv_mse is slightly higher compared to the lambda.min, which is expected as the 1-SE rule tends to select a simpler and more generalizable model at the expense of a slight increase in error. In the end, the choice between lambda.min and lambda.1se would depend on whether the priority is on the lowest possible MSE or on model simplicity.


##Frage: classification performance - only if I have logistic regression and binary outcome or? 

```{r}
library(glmnet)

# Initialize lists
chosen_lambda_min <- numeric(length(cv_models))
chosen_lambda_1se <- numeric(length(cv_models))
model_summary <- data.frame(alpha = numeric(), lambda = numeric(), criterion = character(), 
                            non_zero_coefficients = integer(), cv_mse = numeric(), stringsAsFactors = FALSE)

# Loop over the models to select lambda and summarize models
for (i in seq_along(cv_models)) {
  cv_model <- cv_models[[i]]
  alpha <- alpha_seq[i]
  
  # Select lambda.min and lambda.1se
  lambda_min <- cv_model$lambda.min
  lambda_1se <- cv_model$lambda.1se
  chosen_lambda_min[i] <- lambda_min
  chosen_lambda_1se[i] <- lambda_1se
  
  # Summarize the model with lambda.min
  model_min <- glmnet(X, y, alpha = alpha, lambda = lambda_min, family = "gaussian")
  cv_mse_min <- cv_model$cvm[cv_model$lambda == lambda_min]
  model_summary <- rbind(model_summary, 
                         data.frame(alpha = alpha, lambda = lambda_min, criterion = "min", 
                                    non_zero_coefficients = sum(coef(model_min) != 0), cv_mse = cv_mse_min))
  
  # Summarize the model with lambda.1se
  model_1se <- glmnet(X, y, alpha = alpha, lambda = lambda_1se, family = "gaussian")
  cv_mse_1se <- cv_model$cvm[cv_model$lambda == lambda_1se]
  model_summary <- rbind(model_summary, 
                         data.frame(alpha = alpha, lambda = lambda_1se, criterion = "1se", 
                                    non_zero_coefficients = sum(coef(model_1se) != 0), cv_mse = cv_mse_1se))
}

# Display the model summary
#print(model_summary)
kable(model_summary)
```

Last, we inspect the best solution for $\alpha = 1$  (lasso regression) using the 1 âˆ’ SE rule. This approach tends to prefer simpler models with fewer non-zero coefficients, which can be advantageous for interpretability and generalization. To see which variables were selected (i.e., have non-zero coefficients) and their estimated coefficients, we use the coef function from the glmnet package. We observe here only 4 variables are selcted, and all have rather small coefficients ans thus little influence on the response variable 'lwage76'. To assess the goodness-of-fit, we then calculate the correlation between the predicted values from the model and the observed values. Values closer to 1 or -1 indicating a stronger linear relationship between predictions and actual outcomes. In this case the correlation lies at 0.4293. This indicates a moderate postive linear relationship between the predicted and observed values.The model has some predictive power, but it is not capturing all of the variability in the dependent variable.
```{r}
#(e)
# Identify the model for alpha = 1
lambda_alpha_one_1se <- cv_models[[which(alpha_seq == 1)]]$lambda.1se

# Fit the final model using the selected lambda value
finmod_alpha_one_1se <- glmnet(X, y, alpha = 1, lambda = lambda_alpha_one_1se, family = "gaussian")

# Extract and display the non-zero coefficients
coef_alpha_one_1se <- coef(finmod_alpha_one_1se, s = lambda_alpha_one_1se)
print("Non-zero coefficients (including intercept):")
print(coef_alpha_one_1se)

# Predict the values using the final model
pred_alpha_one_1se <- predict(finmod_alpha_one_1se, s = lambda_alpha_one_1se, newx = X) ### Question: seperate into train and test beginning? not in task though but slides. 

# Calculate and display the correlation between predicted and observed values
cor_alpha_one_1se <- cor(pred_alpha_one_1se, y)
print(paste("Correlation between predicted and observed values for alpha = 1 (1-SE rule):", round(cor_alpha_one_1se,digits = 4)))

```

<!-- EXERCISE 4 -->
# Exercise 4

```{r}
data("SAheart", package = "ElemStatLearn")
SAheart <-SAheart
```

<!-- EXERCISE 5 -->
# Exercise 5
```{r}
data("phoneme", package = "ElemStatLearn")
phoneme <- phoneme
```