###################################################
data("prostate", package = "ElemStatLearn")
prostate.test <- prostate |>
dplyr::filter(!train) |>
dplyr::select(-train)
prostate <- prostate |>
dplyr::filter(train) |>
dplyr::select(-train)
formula <- lpsa ~ .
mf <- model.frame(formula, data = prostate)
y <- model.response(mf)
X <- model.matrix(formula, mf)[, -1]
library("glmnet")
cvlm <- cv.glmnet(X, y)
cvlm
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)
# Clear memory
rm(list=ls())
# Import libraries
library(tidyverse)
library(readr)
library(dplyr)
library(knitr)
library(Ecdat)
library(glmnet)
# change path! ElemStatLearn not directly availabe from CRAN
if (!require("ElemStatLearn")) install.packages("C:/Users/avalder/Downloads/ElemStatLearn_2015.6.26 (1).tar.gz", repos = NULL, type = "source")
library(ElemStatLearn)
library(boot)
library(caret)
data("Schooling", package = "Ecdat")
#(a)
#Omit observations with missing values and the variable wage76. Mutate the variable mar76 into a binary
##variable which is TRUE for value "married" and FALSE otherwise.
Schooling <- Schooling %>%
drop_na() %>%
select(-wage76) %>% # Remove the wage76 variable
mutate(mar76 = as.character(mar76),
mar76 = mar76 == "yes")
#head(Schooling)
#(b)
y <- Schooling$lwage76
X <- as.matrix(Schooling[, setdiff(names(Schooling), "lwage76")])
# Generate foldid for consistent folds across all models
set.seed(123)
foldid <- sample(1:10, size = nrow(X), replace = TRUE) # Example: 10-fold CV
# Sequence of alpha values from 0 to 1 in steps of 0.2
alpha_seq <- seq(0, 1, by = 0.2)
#(c)
cv_models <- lapply(alpha_seq, function(alpha) {
cv.glmnet(X, y, alpha = alpha, foldid = foldid, family = "gaussian", type.measure = "mse")
})
# Visualize the results for each alpha
for (i in seq_along(cv_models)) {
plot(cv_models[[i]])
mtext(paste("Alpha =", alpha_seq[i]), side = 3, line = 3) # Adjust 'line' for positioning
}
View(cv_models)
cv_models[[1]]
###################################################
### Prostate Cancer
###################################################
data("prostate", package = "ElemStatLearn")
prostate.test <- prostate |>
dplyr::filter(!train) |>
dplyr::select(-train)
prostate <- prostate |>
dplyr::filter(train) |>
dplyr::select(-train)
formula <- lpsa ~ .
mf <- model.frame(formula, data = prostate)
y <- model.response(mf)
X <- model.matrix(formula, mf)[, -1]
library("glmnet")
cvlm <- cv.glmnet(X, y)
cvlm
plot(cvlm)
options(digits = 3)
cbind(OLS = coef(cvlm, s = 0)[, 1],
lambda.min = coef(cvlm, s = "lambda.min")[, 1],
lambda.1se = coef(cvlm, s = "lambda.1se")[, 1],
Null = coef(cvlm, s = Inf)[, 1])
cvlm <- cv_models[[1]]
cbind(OLS = coef(cvlm, s = 0)[, 1],
lambda.min = coef(cvlm, s = "lambda.min")[, 1],
lambda.1se = coef(cvlm, s = "lambda.1se")[, 1],
Null = coef(cvlm, s = Inf)[, 1])
cv_models[[1]]
cbind(OLS = coef(cvlm, s = 0)[, 1],
lambda.min = coef(cvlm, s = "lambda.min")[, 1],
lambda.1se = coef(cvlm, s = "lambda.1se")[, 1],
Null = coef(cvlm, s = Inf)[, 1])
mf.test <- model.frame(formula, data = prostate.test)
mf.test
View(mf.test)
y.test <- model.response(mf.test)
X.test <- model.matrix(formula, mf.test)[,-1]
pred <- cbind(
OLS = predict(cvlm, newx = X.test, s = 0)[,1],
predict(cvlm, newx = X.test, s = "lambda.min"),
predict(cvlm, newx = X.test),
Null = predict(cvlm, newx = X.test, s = Inf)[,1])
head(pred, n = 3L)
###################################################
### Prostate Cancer
###################################################
data("prostate", package = "ElemStatLearn")
prostate.test <- prostate |>
dplyr::filter(!train) |>
dplyr::select(-train)
prostate <- prostate |>
dplyr::filter(train) |>
dplyr::select(-train)
formula <- lpsa ~ .
mf <- model.frame(formula, data = prostate)
y <- model.response(mf)
X <- model.matrix(formula, mf)[, -1]
library("glmnet")
cvlm <- cv.glmnet(X, y)
cvlm
plot(cvlm)
options(digits = 3)
cbind(OLS = coef(cvlm, s = 0)[, 1],
lambda.min = coef(cvlm, s = "lambda.min")[, 1],
lambda.1se = coef(cvlm, s = "lambda.1se")[, 1],
Null = coef(cvlm, s = Inf)[, 1])
mf.test <- model.frame(formula, data = prostate.test)
y.test <- model.response(mf.test)
X.test <- model.matrix(formula, mf.test)[,-1]
pred <- cbind(
OLS = predict(cvlm, newx = X.test, s = 0)[,1],
predict(cvlm, newx = X.test, s = "lambda.min"),
predict(cvlm, newx = X.test),
Null = predict(cvlm, newx = X.test, s = Inf)[,1])
head(pred, n = 3L)
colMeans(sweep(pred, 1, y.test, "-")^2)
library(glmnet)
# Initialize lists to store the chosen lambda values and model summaries
chosen_lambda_min <- numeric(length(cv_models))
chosen_lambda_1se <- numeric(length(cv_models))
model_summary <- data.frame(alpha = numeric(), lambda = numeric(), criterion = character(),
non_zero_coefficients = integer(), cv_mse = numeric(), stringsAsFactors = FALSE)
# Loop over the models to select lambda and summarize models
for (i in seq_along(cv_models)) {
cv_model <- cv_models[[i]]
alpha <- alpha_seq[i]
# Select lambda.min and lambda.1se
lambda_min <- cv_model$lambda.min
lambda_1se <- cv_model$lambda.1se
chosen_lambda_min[i] <- lambda_min
chosen_lambda_1se[i] <- lambda_1se
# Summarize the model with lambda.min
model_min <- glmnet(X, y, alpha = alpha, lambda = lambda_min, family = "gaussian")
cv_mse_min <- cv_model$cvm[cv_model$lambda == lambda_min]
model_summary <- rbind(model_summary,
data.frame(alpha = alpha, lambda = lambda_min, criterion = "min",
non_zero_coefficients = sum(coef(model_min) != 0), cv_mse = cv_mse_min))
# Summarize the model with lambda.1se
model_1se <- glmnet(X, y, alpha = alpha, lambda = lambda_1se, family = "gaussian")
cv_mse_1se <- cv_model$cvm[cv_model$lambda == lambda_1se]
model_summary <- rbind(model_summary,
data.frame(alpha = alpha, lambda = lambda_1se, criterion = "1se",
non_zero_coefficients = sum(coef(model_1se) != 0), cv_mse = cv_mse_1se))
}
# Display the model summary
print(model_summary)
View(model_summary)
# Display the model summary
kable(model_summary)
# Display the model summary
print(model_summary)
model_summary
# Assuming cv_models and alpha_seq are already defined
# Find the index for alpha = 1
alpha_one_index <- which(alpha_seq == 1)
# Extract the model for alpha = 1 using 1-SE rule
model_alpha_one_1se <- final_models_1se[[paste0("Alpha_", 1)]]
# Identify the model for α = 1
alpha_one_index <- which(alpha_seq == 1)
cv_model_alpha_one <- cv_models[[alpha_one_index]]
# Extract the lambda value using the 1-SE rule for α = 1
lambda_alpha_one_1se <- cv_model_alpha_one$lambda.1se
# Fit the final model using the selected lambda value
final_model_alpha_one_1se <- glmnet(X, y, alpha = 1, lambda = lambda_alpha_one_1se, family = "gaussian")
# Extract and display the non-zero coefficients
coef_alpha_one_1se <- coef(final_model_alpha_one_1se, s = lambda_alpha_one_1se)
print("Non-zero coefficients (including intercept):")
print(coef_alpha_one_1se)
# Predict the values using the final model
predicted_values_alpha_one_1se <- predict(final_model_alpha_one_1se, s = lambda_alpha_one_1se, newx = X)
# Calculate and display the correlation between predicted and observed values
correlation_alpha_one_1se <- cor(predicted_values_alpha_one_1se, y)
print(paste("Correlation between predicted and observed values for alpha = 1 (1-SE rule):", correlation_alpha_one_1se))
# Identify the model for α = 1
alpha_one_index <- which(alpha_seq == 1)
alpha_one_index <- which(alpha_seq == 1)
cv_model_alpha_one <- cv_models[[alpha_one_index]]
lambda_alpha_one_1se <- cv_model_alpha_one$lambda.1se
final_model_alpha_one_1se <- glmnet(X, y, alpha = 1, lambda = lambda_alpha_one_1se, family = "gaussian")
coef_alpha_one_1se <- coef(final_model_alpha_one_1se, s = lambda_alpha_one_1se)
coef_alpha_one_1se <- coef(final_model_alpha_one_1se, s = lambda_alpha_one_1se)
print("Non-zero coefficients (including intercept):")
print(coef_alpha_one_1se)
print("Non-zero coefficients (including intercept):")
print(coef_alpha_one_1se)
# Predict the values using the final model
predicted_values_alpha_one_1se <- predict(final_model_alpha_one_1se, s = lambda_alpha_one_1se, newx = X)
# Calculate and display the correlation between predicted and observed values
correlation_alpha_one_1se <- cor(predicted_values_alpha_one_1se, y)
print(paste("Correlation between predicted and observed values for alpha = 1 (1-SE rule):", correlation_alpha_one_1se))
print(paste("Correlation between predicted and observed values for alpha = 1 (1-SE rule):", correlation_alpha_one_1se))
coef_alpha_one_1se <- coef(final_model_alpha_one_1se, s = lambda_alpha_one_1se)
print("Non-zero coefficients (including intercept):")
print(coef_alpha_one_1se)
alpha_seq
# Identify the model for α = 1
cvmod_alpha_one <- cv_models[[which(alpha_seq == 1)]]
# Identify the model for α = 1
lambda_alpha_one_1s <- cv_models[[which(alpha_seq == 1)]]$lambda.1se
lambda_alpha_one_1se
# Identify the model for α = 1
lambda_alpha_one_1se <- cv_models[[which(alpha_seq == 1)]]$lambda.1se
# Fit the final model using the selected lambda value, CV?
finmod_alpha_one_1se <- glmnet(X, y, alpha = 1, lambda = lambda_alpha_one_1se, family = "gaussian")
coef_alpha_one_1se <- coef(finmod_alpha_one_1se, s = lambda_alpha_one_1se)
print("Non-zero coefficients (including intercept):")
print(coef_alpha_one_1se)
# Predict the values using the final model
pred_alpha_one_1se <- predict(finmod_alpha_one_1se, s = lambda_alpha_one_1se, newx = X)
# Calculate and display the correlation between predicted and observed values
cor_alpha_one_1se <- cor(pred_alpha_one_1se, y)
cor_alpha_one_1se <- cor(pred_alpha_one_1se, y)
print(paste("Correlation between predicted and observed values for alpha = 1 (1-SE rule):", cor_alpha_one_1se))
print(paste("Correlation between predicted and observed values for alpha = 1 (1-SE rule):", round(cor_alpha_one_1se,digits = 4)))
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)
# Clear memory
rm(list=ls())
# Import libraries
library(tidyverse)
library(readr)
library(dplyr)
library(knitr)
library(Ecdat)
library(glmnet)
# change path! ElemStatLearn not directly availabe from CRAN
if (!require("ElemStatLearn")) install.packages("C:/Users/avalder/Downloads/ElemStatLearn_2015.6.26 (1).tar.gz", repos = NULL, type = "source")
library(ElemStatLearn)
library(boot)
library(caret)
#(a)
set.seed(1)
x <- rnorm(100)
y <- x - 2*x^2 + rnorm(100)
plot(x,y)
#(b) model specifications:
lm1 <- lm(y ~ x) # alternative: lm1 <- glm(y ~ x, data, family = gaussian())
lm2 <- lm(y ~ x +I(x^2))
lm3 <- lm(y ~ x + I(x^2) + I(x^3))
lm4 <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4))
# Calculate AIC values
aic1 <- AIC(lm1)
aic2 <- AIC(lm2)
aic3 <- AIC(lm3)
aic4 <- AIC(lm4)
# Print AIC values
aic_values <- c(aic1, aic2, aic3, aic4)
print(aic_values)
#(c)
# Generate test data from the data generating process
x_test <- rnorm(100)
y_test <- x_test - 2*x_test^2 + rnorm(100)
# Predict and calculate twice the negative log-likelihood as loss function
lm1_test <- lm(y_test ~ x_test)
lm2_test <- lm(y_test ~ x_test +I(x_test^2))
lm3_test <- lm(y_test ~ x_test + I(x_test) + I(x_test^3))
lm4_test <- lm(y_test ~ x_test + I(x_test^2) + I(x_test^3) + I(x_test^4))
lm_test <- list(lm1_test,lm2_test,lm3_test,lm4_test)
error_est <- lapply(lm_test, function(x) -2*logLik(x))
print(error_est)
#(a)
set.seed(1)
x <- rnorm(100)
y <- x - 2*x^2 + rnorm(100)
data <- data.frame(x = x, y = y)
#(b)
# List of formulas for the models
model_formulas <- list(
y ~ x,
y ~ x + I(x^2),
y ~ x + I(x^2) + I(x^3),
y ~ x + I(x^2) + I(x^3) + I(x^4)
)
# Function to calculate MSE
mse <- function(actual, predicted) {
mean((actual - predicted)^2)
}
# LOOCV Error Calculation
loocv_error <- function(model_formula, data) {
loocv_results <- sapply(1:nrow(data), function(i) {
train_data <- data[-i, ]
test_data <- data[i, ]
model <- lm(model_formula, data = train_data)
predicted <- predict(model, newdata = test_data)
mse(test_data$y, predicted)
})
mean(loocv_results)
}
# k-Fold CV Error Calculation
kcv_error <- function(model_formula, data, k) {
train_control <- trainControl(method = "cv", number = k)
model <- train(model_formula, data = data, method = "lm", trControl = train_control)
model$results$RMSE^2 # Squaring RMSE to get MSE
}
# Updated Bootstrap Error Calculation using boot package
bootstrap_error <- function(model_formula, data, nR) {
bootFunction <- function(data, indices) {
d <- data[indices, ] # Bootstrap sample
model <- lm(model_formula, data = d)
predicted <- predict(model, newdata = d)
mse(d$y, predicted)
}
# Perform bootstrapping using the boot function
results <- boot(data = data, statistic = bootFunction, R = nR)
mean(results$t) # Return the mean of bootstrap MSE estimates
}
# Applying the methods
set.seed(123) # For reproducibility across methods
errors_seed1 <- lapply(model_formulas, function(formula) {
list(
LOOCV = loocv_error(formula, data),
kCV = kcv_error(formula, data, 10), # Example: 10-fold CV
Bootstrap = bootstrap_error(formula, data, 1000) # Example: 1000 bootstrap samples
)
})
# Display the errors for each model and method
#errors_seed1
#(c)
set.seed(456) # change seed
errors_seed2 <- lapply(model_formulas, function(formula) {
list(
LOOCV = loocv_error(formula, data),
kCV = kcv_error(formula, data, 10), # Example: 10-fold CV
Bootstrap = bootstrap_error(formula, data, 1000) # Example: 1000 bootstrap samples
)
})
# Display the errors for each model and method
#errors_seed2
# Convert lists to data frames with model and seed identifiers
df1 <- do.call(rbind, lapply(seq_along(errors_seed1), function(i) {
cbind(data.frame(errors_seed1[[i]]), Model=paste("Model", i), Seed="Seed1")
}))
df2 <- do.call(rbind, lapply(seq_along(errors_seed2), function(i) {
cbind(data.frame(errors_seed2[[i]]), Model=paste("Model", i), Seed="Seed2")
}))
combined_df <- rbind(df1, df2)
combined_df <- combined_df[order(combined_df$Model, combined_df$Seed), ]
combined_df$Model <- factor(combined_df$Model)
combined_df$Seed <- factor(combined_df$Seed)
#print(combined_df)
knitr::kable(combined_df, caption = "Comparison of LOOCV, kCV, and Bootstrap Errors", digits = 4)
#(e)
lm1 <- lm(y ~ x) # alternative: lm1 <- glm(y ~ x, data, family = gaussian())
lm2 <- lm(y ~ x +I(x^2))
lm3 <- lm(y ~ x + I(x^2) + I(x^3))
lm4 <- lm(y ~ x + I(x^2) + I(x^3) + I(x^4))
models <- list(lm1,lm2,lm3,lm4)
knitr::kable(lapply(models,function(x) summary(x)$coefficients))
data("Schooling", package = "Ecdat")
#(a)
#Omit observations with missing values and the variable wage76. Mutate the variable mar76 into a binary
##variable which is TRUE for value "married" and FALSE otherwise.
Schooling <- Schooling %>%
drop_na() %>%
select(-wage76) %>% # Remove the wage76 variable
mutate(mar76 = as.character(mar76),
mar76 = mar76 == "yes")
#head(Schooling)
#(b)
y <- Schooling$lwage76
X <- as.matrix(Schooling[, setdiff(names(Schooling), "lwage76")])
# Generate foldid for consistent folds across all models
set.seed(123)
foldid <- sample(1:10, size = nrow(X), replace = TRUE) # Example: 10-fold CV
# Sequence of alpha values from 0 to 1 in steps of 0.2
alpha_seq <- seq(0, 1, by = 0.2)
#(c)
cv_models <- lapply(alpha_seq, function(alpha) {
cv.glmnet(X, y, alpha = alpha, foldid = foldid, family = "gaussian", type.measure = "mse")
})
# Visualize the results for each alpha
for (i in seq_along(cv_models)) {
plot(cv_models[[i]])
mtext(paste("Alpha =", alpha_seq[i]), side = 3, line = 3) # Adjust 'line' for positioning
}
# cvlm <- cv_models[[1]]
#
# cbind(OLS = coef(cvlm, s = 0)[, 1],
#       lambda.min = coef(cvlm, s = "lambda.min")[, 1],
#       lambda.1se = coef(cvlm, s = "lambda.1se")[, 1],
#       Null = coef(cvlm, s = Inf)[, 1])
library(glmnet)
# Initialize lists
chosen_lambda_min <- numeric(length(cv_models))
chosen_lambda_1se <- numeric(length(cv_models))
model_summary <- data.frame(alpha = numeric(), lambda = numeric(), criterion = character(),
non_zero_coefficients = integer(), cv_mse = numeric(), stringsAsFactors = FALSE)
# Loop over the models to select lambda and summarize models
for (i in seq_along(cv_models)) {
cv_model <- cv_models[[i]]
alpha <- alpha_seq[i]
# Select lambda.min and lambda.1se
lambda_min <- cv_model$lambda.min
lambda_1se <- cv_model$lambda.1se
chosen_lambda_min[i] <- lambda_min
chosen_lambda_1se[i] <- lambda_1se
# Summarize the model with lambda.min
model_min <- glmnet(X, y, alpha = alpha, lambda = lambda_min, family = "gaussian")
cv_mse_min <- cv_model$cvm[cv_model$lambda == lambda_min]
model_summary <- rbind(model_summary,
data.frame(alpha = alpha, lambda = lambda_min, criterion = "min",
non_zero_coefficients = sum(coef(model_min) != 0), cv_mse = cv_mse_min))
# Summarize the model with lambda.1se
model_1se <- glmnet(X, y, alpha = alpha, lambda = lambda_1se, family = "gaussian")
cv_mse_1se <- cv_model$cvm[cv_model$lambda == lambda_1se]
model_summary <- rbind(model_summary,
data.frame(alpha = alpha, lambda = lambda_1se, criterion = "1se",
non_zero_coefficients = sum(coef(model_1se) != 0), cv_mse = cv_mse_1se))
}
# Display the model summary
#print(model_summary)
kable(model_summary)
#(e)
# Identify the model for alpha = 1
lambda_alpha_one_1se <- cv_models[[which(alpha_seq == 1)]]$lambda.1se
# Fit the final model using the selected lambda value, CV?
finmod_alpha_one_1se <- glmnet(X, y, alpha = 1, lambda = lambda_alpha_one_1se, family = "gaussian")
# Extract and display the non-zero coefficients
coef_alpha_one_1se <- coef(finmod_alpha_one_1se, s = lambda_alpha_one_1se)
print("Non-zero coefficients (including intercept):")
print(coef_alpha_one_1se)
# Predict the values using the final model
pred_alpha_one_1se <- predict(finmod_alpha_one_1se, s = lambda_alpha_one_1se, newx = X)
# Calculate and display the correlation between predicted and observed values
cor_alpha_one_1se <- cor(pred_alpha_one_1se, y)
print(paste("Correlation between predicted and observed values for alpha = 1 (1-SE rule):", round(cor_alpha_one_1se,digits = 4)))
# Extract and display the non-zero coefficients
coef_alpha_one_1se <- coef(finmod_alpha_one_1se, s = 0)
print("Non-zero coefficients (including intercept):")
print(coef_alpha_one_1se)
# Extract and display the non-zero coefficients
coef_alpha_one_1se <- coef(finmod_alpha_one_1se, s = lambda_alpha_one_1se)
print("Non-zero coefficients (including intercept):")
print(coef_alpha_one_1se)
###################################################
### Prostate Cancer
###################################################
data("prostate", package = "ElemStatLearn")
prostate.test <- prostate |>
dplyr::filter(!train) |>
dplyr::select(-train)
prostate <- prostate |>
dplyr::filter(train) |>
dplyr::select(-train)
formula <- lpsa ~ .
mf <- model.frame(formula, data = prostate)
y <- model.response(mf)
X <- model.matrix(formula, mf)[, -1]
library("glmnet")
cvlm <- cv.glmnet(X, y)
cvlm
plot(cvlm)
```{r}
library(glmnet)
# Initialize lists
chosen_lambda_min <- numeric(length(cv_models))
chosen_lambda_1se <- numeric(length(cv_models))
model_summary <- data.frame(alpha = numeric(), lambda = numeric(), criterion = character(),
non_zero_coefficients = integer(), cv_mse = numeric(), stringsAsFactors = FALSE)
# Loop over the models to select lambda and summarize models
for (i in seq_along(cv_models)) {
cv_model <- cv_models[[i]]
alpha <- alpha_seq[i]
# Select lambda.min and lambda.1se
lambda_min <- cv_model$lambda.min
lambda_1se <- cv_model$lambda.1se
chosen_lambda_min[i] <- lambda_min
chosen_lambda_1se[i] <- lambda_1se
# Summarize the model with lambda.min
model_min <- glmnet(X, y, alpha = alpha, lambda = lambda_min, family = "gaussian")
cv_mse_min <- cv_model$cvm[cv_model$lambda == lambda_min]
model_summary <- rbind(model_summary,
data.frame(alpha = alpha, lambda = lambda_min, criterion = "min",
non_zero_coefficients = sum(coef(model_min) != 0), cv_mse = cv_mse_min))
# Summarize the model with lambda.1se
model_1se <- glmnet(X, y, alpha = alpha, lambda = lambda_1se, family = "gaussian")
cv_mse_1se <- cv_model$cvm[cv_model$lambda == lambda_1se]
model_summary <- rbind(model_summary,
data.frame(alpha = alpha, lambda = lambda_1se, criterion = "1se",
non_zero_coefficients = sum(coef(model_1se) != 0), cv_mse = cv_mse_1se))
}
# Display the model summary
#print(model_summary)
kable(model_summary)
# Calculate and display the correlation between predicted and observed values
cor_alpha_one_1se <- cor(pred_alpha_one_1se, y)
print(paste("Correlation between predicted and observed values for alpha = 1 (1-SE rule):", round(cor_alpha_one_1se,digits = 4)))
warnings()
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)
```{r}
kable(lapply(models,function(x) summary(x)$coefficients))
#(e)
lm1 <- lm(y ~ x) # alternative: lm1 <- glm(y ~ x, data, family = gaussian())
