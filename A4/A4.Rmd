---
title: "Statistical Learning (5454) - Assignment 4"  
author: "Matthias Hochholzer, Lukas Pirnbacher, Anne Valder"
date: "Due: 2024-06-10"
output: 
  pdf_document: 
    keep_tex: yes
header-includes:
   - \usepackage{titlesec}
   - \titleformat*{\section}{\normalfont\Large\bfseries\flushleft}
   - \titleformat*{\subsection}{\normalfont\large\bfseries\flushleft}
   - \titleformat*{\subsubsection}{\normalfont\normalsize\bfseries\flushleft}
   - \usepackage{amsmath}
   - \newcommand*{\defeq}{\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}=}
   - \newcommand*{\eqdef}{=\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}}
                     
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)
```


```{r,include=FALSE}
# Clear memory
rm(list=ls())
```

```{r, echo=FALSE, include=FALSE, warning=FALSE}
library("nnet")
library(ISLR2)
library(torch)
library(caret)
library(ggplot2)
library(MASS)

```



<!-- EXERCISE 1 -->
# Exercise 1

We generate data from the following additive error model $Y = f(X_1,X_2) + \epsilon$. The sum of sigmoids is
$$ Y = \sigma(a_1^\top X_1) + \sigma(a_2^\top X_2) + \epsilon, $$
with $a_1 = (3, 3)$, $a_2 = (3,−3)$.
– Each $X_j$ , $j = 1, 2$, is a standard Gaussian variate with $p = 2$.
– $\epsilon$ is an independent Gaussian error with variance chosen such that the signal-to-noise ratio as measured
by the respective variances equals four.

We Generate a training set of size 100 and a test sample of size 10,000.
```{r}
set.seed(123)

a1 <- c(3,3)
a2 <- c(3,-3)
n_train <- 100
n_test <- 10000
p <- 2

# X1,X2 richtig generiret?
# variance von f(...)?
# learning rate?

# Generate data
generate_data <- function(n, a1, a2, p) {
  X <- mvrnorm(n, mu = rep(0, p), Sigma = diag(p))
  epsilon <- rnorm(n, mean = 0, sd = sqrt(var(X %*% a1) / 4)) # SNR=4
  Y <- 1 / (1 + exp(-X %*% a1)) + 1 / (1 + exp(-X %*% a2)) + epsilon
  list(X = X, Y = Y)
}

# Generatethe train and test set
train_data <- generate_data(n_train, a1, a2, p)
test_data <- generate_data(n_test, a1, a2, p)

x_train <- train_data$X
y_train <- train_data$Y
x_test <- test_data$X
y_test <- test_data$Y
```

We then fit a neural networks with weight decay of 0.0005 and we vary the number of hidden units from 0 to 10 and record the average test error $E_{Test}(Y − \hat{f}(X_1,X_2))^2$ for each of 10 random starting weights. I.e. for each of the settings with a different number of hidden units we fit a neural network 10 times with different initial values.

```{r, warning=F}
# Define the neural network model
net <- nn_module(
  initialize = function(hidden_units) {
    self$hidden_units <- hidden_units
    if (hidden_units > 0) {
      self$fc1 <- nn_linear(p, hidden_units)
      self$fc2 <- nn_linear(hidden_units, 1)
    } else {
      self$fc <- nn_linear(p, 1)
    }
  },
  forward = function(x) {
    if (self$hidden_units > 0) {
      x %>%
        self$fc1() %>%
        nnf_relu() %>%
        self$fc2()
    } else {
      x %>% self$fc()
    }
  }
)


hidden_units_list <- 0:5
n_repeats <- 10
results <- data.frame()

for (hidden_units in hidden_units_list) {
  cat(sprintf("Training with %d hidden units\n", hidden_units))
  test_errors <- numeric(n_repeats)
  
  for (rep in 1:n_repeats) {
    model <- net(hidden_units)
    optimizer <- optim_adam(model$parameters, lr = 0.001, weight_decay = 0.0005)
    criterion <- nn_mse_loss()
    
    x_train_tensor <- torch_tensor(as.matrix(x_train), dtype = torch_float())
    y_train_tensor <- torch_tensor(y_train, dtype = torch_float())#$unsqueeze(2)
    x_test_tensor <- torch_tensor(as.matrix(x_test), dtype = torch_float())
    y_test_tensor <- torch_tensor(y_test, dtype = torch_float())#$unsqueeze(2)
    
    # Training loop
    model$train()
    for (epoch in 1:200) {
      optimizer$zero_grad()
      output <- model(x_train_tensor)
      loss <- criterion(output, y_train_tensor)
      loss$backward()
      optimizer$step()
    }
    
    # Evaluate the model on the test data
    model$eval()
    with_no_grad({
      output_test <- model(x_test_tensor)
      test_error <- criterion(output_test, y_test_tensor)$item()
    })
    
    test_errors[rep] <- test_error
  }
  
  mean_test_error <- mean(test_errors)
  sd_test_error <- sd(test_errors)
  
  results <- rbind(results, data.frame(hidden_units = hidden_units, mean_test_error = mean_test_error, sd_test_error = sd_test_error))
}

print(results)

```

Let's now visualize the results and interpret them.

```{r}
ggplot(results, aes(x = hidden_units, y = mean_test_error)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = mean_test_error - sd_test_error, ymax = mean_test_error + sd_test_error), width = 0.2) +
  labs(title = "Test Error vs. Number of Hidden Units",
       x = "Number of Hidden Units",
       y = "Mean Test Error") +
  theme_minimal()

```

```{r}

```

\newpage
<!-- EXERCISE 2 -->

# Exercise 2

The data sets \texit{zip.train} and \textit{zip.test} from package \textbf{ElemStatLearn} contain the information on the gray color
values of the pixels on a $16 \times 16$ pixel image of hand-written digits. We first visualize for each digit one randomly selected observation.

```{r}

```

we then fit a multinomial logistic regression model to the training data and evaluate it on the training and the test data. We determine the overall misclassification rate on the training and the test data and the digit-specific misclassification rates on the test data. 

```{r}

```

Which digits are the most difficult and the easiest to classify?

We add a positive weight decay of 0.05 when fitting the multinomial logistic regression model to the training data and evaluate the model on the training and the test data. We determine the overall misclassification rate on the training and the test data. 

```{r}

```


Explain why it makes sense to also include weight decay when fitting this multinomial logistic regression model.

\newpage
<!-- EXERCISE 3 -->
# Exercise 3

The data sets \textit{zip.train} and \textit{zip.test} from package \textbf{ElemStatLearn} contain the information on the gray color values of the pixels on a $16 \times 16$ pixel image of hand-written digits. Use only a subset from \textit{zip.train} of size 320 observations with an equal number of observations for each
digit to fit a multinomial logistic regression model and a neural network.

```{r}

```

Use all remaining training observations and the test data set to evaluate the fitted models.

```{r}

```

For this small training data overfitting is an issue. Visualize the performance on the test data in dependence of the training epochs when fitting the models.

```{r}

```



\newpage
<!-- EXERCISE 4 -->
# Exercise 4

In the following we will estimate a predictive model for the Default data from the \textbf{ISLR2} pacakge. We fit a neural network using a single hidden layer with 10 units and dropout regularization. We use the hint and use James et al. (2021, Chapter 10).

```{r}
rm(list=ls())

data("Default", package = "ISLR2") # Load data
head(Default)
#Converting the default column to a binary numeric variable for the neural network model.

Default$default <- ifelse(Default$default == "Yes", 1, 0)
Default$student <- ifelse(Default$student == "Yes", 1, 0)

# seperating the data into train and test
set.seed(123)
n <- nrow(Default)
ntest <- trunc(n/3)
testid <- sample(1:n, ntest)

Default_test <- Default[testid,]
Default_train <- Default[-testid, ]

# Logistic regression model
logit_model <- glm(default ~ ., data = Default_train, family = binomial)
# Predicting on test data
logit_pred <- predict(logit_model, newdata = Default_test, type = "response")
logit_pred_class <- ifelse(logit_pred > 0.5,1,0)
y_test <- Default[testid,]$default
accuracy_logistic <- mean(y_test == logit_pred_class)

# scaling the data for neural networks to ensure that the model converges more efficiently

maxs <- apply(Default[, -1], 2, max)
mins <- apply(Default[, -1], 2, min)
x <-  sweep(sweep(Default[,-1], 2, mins, "-"), 2, maxs - mins, "/")

#Convert the data to torch tensors
x_train <- torch_tensor(as.matrix(x[-testid, ]), dtype = torch_float())
y_train <- torch_tensor(matrix(Default[-testid,1]), dtype = torch_float())$unsqueeze(2)
x_test <- torch_tensor(as.matrix(x[testid, ]), dtype = torch_float())
y_test <- torch_tensor(matrix(Default[testid,1]), dtype = torch_float())$unsqueeze(2)


# Neural network model

## Define the neural network model
model_nn <- nn_module(
  initialize = function() {
    self$fc1 <- nn_linear(ncol(x_train) , 10)
    self$dropout <- nn_dropout(p = 0.4)
    self$fc2 <- nn_linear(10, 1)
  },
  forward = function(x) {
    x %>% 
      self$fc1() %>% 
      nnf_relu() %>% 
      self$dropout() %>% 
      self$fc2() %>% 
      nnf_sigmoid()
  }
)

net <- model_nn()
optimizer <- optim_adam(net$parameters, lr = 0.001)


# Training the model and recording loss
epochs <- 10
train_loss <- numeric(epochs)
valid_loss <- numeric(epochs)

for (epoch in 1:epochs) {
  net$train()
  optimizer$zero_grad()
  output <- net(x_train)
  loss <- nnf_binary_cross_entropy(output, y_train)
  loss$backward()
  optimizer$step()
  train_loss[epoch] <- loss$item()
  
  # Calculate validation loss
  net$eval()
  with_no_grad({
    output_valid <- net(x_test) #nnf_binary_cross_entropy
    loss_valid <- nnf_binary_cross_entropy(output_valid, y_test)
    valid_loss[epoch] <- loss_valid$item()
  })
  
  if (epoch %% 10 == 0) {
    cat(sprintf("Epoch: %d, Train Loss: %3f, Validation Loss: %3f\n", epoch, train_loss[epoch], valid_loss[epoch]))
  }
}

  
# Predicting on test data
nn_pred_prob <- net(x_test)
nn_pred <- ifelse(nn_pred_prob > 0.5, 1,0)
y_test <- Default[testid,]$default
accuracy_nn <- mean(y_test == nn_pred)

# Plot training and validation loss
plot(1:epochs, train_loss, type = "l", col = "red", ylim = range(c(train_loss, valid_loss)), 
     xlab = "Epoch", ylab = "Loss", main = "Training and Validation Loss")
lines(1:epochs, valid_loss, col = "green")
legend("topright", legend = c("Train", "Validation"), col = c("red", "green"), lwd = 2)

```

We ompare the classification performance of this model with that of linear logistic regression.

```{r}
# Plot ROC Curves and Calculate AUC
library(pROC)

roc_nn <- roc(Default_test$default, as.numeric(nn_pred_prob))
roc_logit <- roc(Default_test$default, logit_pred)

# Plot ROC curve
plot(roc_nn, col = "blue", main = "ROC Curves for Neural Network and Logistic Regression")
lines(roc_logit, col = "red")
legend("bottomright", legend = c("Neural Network", "Logistic Regression"), col = c("blue", "red"), lwd = 2)

# Print AUC values
cat("AUC for Neural Network:", auc(roc_nn), "\n")
cat("AUC for Logistic Regression:", auc(roc_logit), "\n")


# comparing model performance
# Confusion Matrix for Neural Network
nn_conf_matrix <- confusionMatrix(factor(as.integer(nn_pred)), factor(Default_test$default))
print(nn_conf_matrix)

# Confusion Matrix for Logistic Regression
logit_conf_matrix <- confusionMatrix(factor(logit_pred_class), factor(Default_test$default))
print(logit_conf_matrix)
```


\newpage
<!-- EXERCISE 5 -->
# Exercise 5
Now we perform document classification on the  \textit{IMDb} data set, which is available as part of the \textbf{torchdatasets} package. We limit the dictionary size to the 10,000 most frequently-used words and tokens. Again, we use James et al. (2021, Chapter 10). We begin by loading the data and creating a imdb_tain and imdb_test object. Each element of imdb_train is a vector of numbers between 1 and 10000 (the document), referring to the words found in the dictionary. Next we write a function to one-hot encode each document in a list of documents, and return a binary matrix in sparse-matrix format. To construct the sparse matrix, one supplies just the entries that are nonzero. In the last line we call the function sparseMatrix() and supply the row indices corresponding to each document and the column indices corresponding to the words in each document, since we omit the values they are taken to be all ones. Words that appear more than once in any given document still get recorded as a one.



```{r}
#https://www.casact.org/sites/default/files/2022-12/James-G.-et-al.-2nd-edition-Springer-2021.pdf
library(torch)
library(torchdatasets)
library(luz)

gen_path<-"~/"  #Specific to the computer
max_features <- c(500, 1000, 3000, 5000, 10000)
# Function to create a directory and set it as the working directory

# Define the base path where directories will be created

# Function to create a directory and set it as the working directory
set_working_directory <- function(path) {
  if (!dir.exists(path)) {
    dir.create(path, recursive = TRUE)
  }
  setwd(path)
}


set.seed(1)

for (f in max_features) {
#f <- 500

# Load the IMDb dataset from torchdatasets

imdb_train <- imdb_dataset(
  root = ".", 
  download = TRUE,
  split="train",
  num_words = f
)

imdb_test <- imdb_dataset(
  root = ".", 
  download = TRUE,
  split="test",
  num_words = f
)

imdb_train[1]$x[1:12]

library(Matrix)
one_hot <- function(sequences, dimension) {
   seqlen <- sapply(sequences, length)
   n <- length(seqlen)
   rowind <- rep(1:n, seqlen)
   colind <- unlist(sequences)
   sparseMatrix(i = rowind, j = colind,
      dims = c(n, dimension))
}

# collect all values into a list
train <- seq_along(imdb_train) %>% 
  lapply(function(i) imdb_train[i]) %>% 
  purrr::transpose()
test <- seq_along(imdb_test) %>% 
  lapply(function(i) imdb_test[i]) %>% 
  purrr::transpose()

# num_words + padding + start + oov token = 10000 + 3
x_train_1h <- one_hot(train$x, f + 3)
x_test_1h <- one_hot(test$x, f + 3)
dim(x_train_1h)

#nnzero(x_train_1h) / (25000 * (10000 + 3))
#Only 1.3% of the entries are nonzero, so this amounts to considerable savings in memory. We create a validation set of size 2,000, leaving 23,000 for training.

ival <- sample(seq(along = train$y), (f*0.2))
itrain <- seq_along(train$y)[-ival]

#Next we fit a fully-connected neural network with two hidden layers, each with 16 units and ReLU activation.

model <- nn_module(
  initialize = function(input_size = f + 3) {
    self$dense1 <- nn_linear(input_size, 16)
    self$relu <- nn_relu()
    self$dense2 <- nn_linear(16, 16)
    self$output <- nn_linear(16, 1)
  },
  forward = function(x) {
    x %>% 
      self$dense1() %>% 
      self$relu() %>% 
      self$dense2() %>% 
      self$relu() %>% 
      self$output() %>% 
      torch_flatten(start_dim = 1)
  }
)
model <- model %>% 
  setup(
    loss = nn_bce_with_logits_loss(),
    optimizer = optim_rmsprop,
    metrics = list(luz_metric_binary_accuracy_with_logits())
  ) %>% 
  set_opt_hparams(lr = 0.001)

# Fit the model with training and validation data

fitted <- model %>% 
  fit(
    # we transform the training and validation data into torch tensors
    list(
      torch_tensor(as.matrix(x_train_1h[itrain,]), dtype = torch_float()), 
      torch_tensor(unlist(train$y[itrain]))
    ),
    valid_data = list(
      torch_tensor(as.matrix(x_train_1h[ival, ]), dtype = torch_float()), 
      torch_tensor(unlist(train$y[ival]))
    ),
    dataloader_options = list(batch_size = 512),
    epochs = 10
  )

p_fitted <- plot(fitted) 


# Fit the model with test data as validation data to get test accuracy
fitted_test <- model %>% 
  fit(
    list(
      torch_tensor(as.matrix(x_train_1h[itrain, ]), dtype = torch_float()), 
      torch_tensor(unlist(train$y[itrain]), dtype = torch_float())
    ),
    valid_data = list(
      torch_tensor(as.matrix(x_test_1h), dtype = torch_float()), 
      torch_tensor(unlist(test$y), dtype = torch_float())
    ),
    dataloader_options = list(batch_size = 512),
    epochs = 10
  )

p_fittedtest <-plot(fitted_test)
# Helper function to extract metrics
extract_metric <- function(metric_list, metric_name) {
  sapply(metric_list, function(epoch) epoch[[metric_name]])}
  
  
# Extract metrics
train_loss <- extract_metric(fitted$records$metrics$train, "loss")
val_loss <- extract_metric(fitted$records$metrics$valid, "loss")
test_loss <- extract_metric(fitted_test$records$metrics$valid, "loss")


train_acc <- extract_metric(fitted$records$metrics$train, "acc")
val_acc <- extract_metric(fitted$records$metrics$valid, "acc")
test_acc <- extract_metric(fitted_test$records$metrics$valid, "acc")  



# Create data frame for plotting
df <- data.frame(
  Epoch = rep(1:10, 3),
  Metric = c(rep("Train", 10), rep("Validation", 10), rep("Test", 10)),
  Loss = c(train_loss, val_loss, test_loss),
  Accuracy = c(train_acc, val_acc, test_acc)
)

library(ggplot2)
# Plotting the results
gg_loss <- ggplot(df, aes(x = Epoch)) +
  geom_line(aes(y = Loss, color = Metric), na.rm = TRUE) +
  labs(y = "Loss") +
  scale_color_manual(values = c("blue", "red", "orange")) +
  theme_minimal() +
  ggtitle("Loss per Epoch")

gg_acc <- ggplot(df, aes(x = Epoch)) +
  geom_line(aes(y = Accuracy, color = Metric)) +
  labs(y = "Accuracy") +
  scale_color_manual(values = c("blue", "red", "orange")) +
  theme_minimal() +
  ggtitle("Accuracy per Epoch")



  # Save ggplot2 plots

  ggsave(filename = paste0("gg_loss_", f, ".png"), plot = gg_loss 
         + theme(plot.background = element_rect(fill = "white")))
  ggsave(filename = paste0("gg_acc_", f, ".png"), plot = gg_acc
         + theme(plot.background = element_rect(fill = "white")))
  
 # Save base R plots
  png(filename = paste0("p_fitted_", f, ".png"))
  p_fitted
  dev.off()
  
  png(filename = paste0("p_fittedtest_", f, ".png"))
  p_fittedtest
  dev.off()
  
 objects_to_keep <- c("gg_loss", "gg_acc","df","p_fittedtest","p_fitted","f")
#rm(list = setdiff(ls(), objects_to_keep))

 
 rm(list = setdiff(ls(), objects_to_keep))
  # Save data frame
  save.image(file = paste0(f, ".RData"))

}
```

```{r}

load("1000.RData")
p_fitted
p_fittedtest
gg_acc
gg_loss
```

Consider the effects of varying the dictionary size. Try the values 500, 1000, 3000, 5000, and 10,000, and compare the results.
```{r}

load("500.RData")
p_fitted
p_fittedtest
gg_acc
gg_loss
```

```{r}

load("3000.RData")
p_fitted
p_fittedtest
gg_acc
gg_loss 
```

```{r}

load("5000.RData")
p_fitted
p_fittedtest
gg_acc
gg_loss  
```

```{r}

load("10000.RData")
 p_fitted
p_fittedtest
gg_acc
gg_loss 
```