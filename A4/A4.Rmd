---
title: "Statistical Learning (5454) - Assignment 4"  
author: "Matthias Hochholzer, Lukas Pirnbacher, Anne Valder"
date: "Due: 2024-06-10"
output: 
  pdf_document: 
    keep_tex: yes
header-includes:
   - \usepackage{titlesec}
   - \titleformat*{\section}{\normalfont\Large\bfseries\flushleft}
   - \titleformat*{\subsection}{\normalfont\large\bfseries\flushleft}
   - \titleformat*{\subsubsection}{\normalfont\normalsize\bfseries\flushleft}
   - \usepackage{amsmath}
   - \newcommand*{\defeq}{\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}=}
   - \newcommand*{\eqdef}{=\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}}
                     
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)
```


```{r,include=FALSE}
# Clear memory
rm(list=ls())
```

```{r, echo=FALSE, include=FALSE, warning=FALSE}
library(nnet)
library(ISLR2)
library(torch)
library(luz)
library(caret)
library(ggplot2)
library(MASS)
library(ElemStatLearn)

```


<!-- EXERCISE 1 -->
# Exercise 1

We generate data from the additive error model $Y = f(X_1,X_2) + \epsilon$, where $f(X_1, X_2)$ is a sum of sigmoids, i.e.
$$ f(X_1,X_2) = \sigma(a_1^\top X_1) + \sigma(a_2^\top X_2), $$
with $a_1 = (3, 3)'$, $a_2 = (3,-3)'$ and bivariate standard Gaussian variables $X_j$, $j = 1, 2$. The variance of the independent Gaussian error $\epsilon$ is chosen such that the signal-to-noise ratio as measured by the respective variances equals four. We generate a training set of size 100 and a test sample of size 10,000.

```{r}
set.seed(123)

n_train <- 100
n_test <- 10000

# helper for sigmoid
sigmoid <- function(x){
  1/(1+exp(-x))
}

# data generating function
generate_data <- function(n, a1 = c(3,3), a2 = c(3,-3)){
  p1 <- length(a1)
  p2 <- length(a2)
  
  X1 <- mvrnorm(n, mu = rep(0,p1), Sigma = diag(p1))
  X2 <- mvrnorm(n, mu = rep(0,p2), Sigma = diag(p2))
  X <- cbind(X1, X2)
  
  fX <- sigmoid(X1%*%a1) + sigmoid(X2%*%a2)
  
  epsilon <- rnorm(n, mean = 0, sd = sqrt(var(fX)/4)) # SNR=4
  Y <- fX + epsilon
  
  list(X = X, Y = Y)
}

# Generate the train and test set
train_data <- generate_data(n_train)
test_data <- generate_data(n_test)

x_train <- train_data$X
y_train <- train_data$Y
x_test <- test_data$X
y_test <- test_data$Y


```

We then fit neural networks with weight decay of 0.0005 and vary the number of hidden units from 0 to 10. We record the average test error $$\mathbb{E}_{\text{Test}}(Y - \hat{f}(X_1,X_2))^2$$ 
for each of 10 random starting weights.
  
```{r }
# set seed for torch
torch_manual_seed(123)
  
# define the neural network
net <- nn_module(
  initialize = function(hidden_units){
    self$hidden_units <- hidden_units # number of hidden units
    if(hidden_units>0){
      self$hidden <- nn_linear(4, hidden_units) # hidden layer
      self$activation <- nn_sigmoid() # sigmoid activation function
      self$output <- nn_linear(hidden_units, 1) # output layer
    } else {
      self$output <- nn_linear(4, 1) # no hidden units --> just output layer
    }
  },
  forward = function(x){
    if(self$hidden_units > 0){
      x %>%
        self$hidden() %>%
        self$activation() %>%
        self$output()
    } else {
      x %>% self$output()
    }
  }
)
  
# set up for estimation
net <- net %>%
  setup(
    loss = nn_mse_loss(), # MSE loss
    optimizer = optim_rmsprop # choose optimizer
  ) %>%
  set_opt_hparams(weight_decay = 0.0005) # set weight decay
  
# set up for exercise  
hidden_units_list <- 0:10
n_repeats <- 10 # iteration for different starting weights
  
# containers for results
results <- data.frame()
errors_list <- vector("list", length(hidden_units_list))
names(errors_list) <- paste0('h=',hidden_units_list)
  
  
  
for (h in hidden_units_list) {
    
  #cat(sprintf("Training with %d hidden units\n", h))
  test_errors <- numeric(n_repeats)
    
  for (rep in 1:n_repeats) {
      
    fitted <- net %>%
      set_hparams(hidden_units = h) %>%
      fit(
        data = list(x_train, y_train),
        epochs = 30 # in-sample MSE quickly drops for only a few epochs 
      )
      
    pred <- as.matrix(predict(fitted, x_test)) # predictions on test data
    mse <- mean((y_test-pred)^2) # average test error
      
    test_errors[rep] <- mse
      
   }
    
   errors_list[[paste0("h=",h)]] <- test_errors
    
   mean_test_error <- mean(test_errors)
   sd_test_error <- sd(test_errors)
    
   results <- rbind(results, data.frame(hidden_units = h, 
                                        mean_test_error = mean_test_error, 
                                        sd_test_error = sd_test_error))
}
  
  
print(results)    
  
```

Let us now visualize the results and interpret them.

```{r}
ggplot(results, aes(x = hidden_units, y = mean_test_error)) +
  geom_point() +
  geom_line() +
  geom_errorbar(
    aes(ymin = mean_test_error - sd_test_error, ymax = mean_test_error + sd_test_error), 
    width = 0.2) +
  labs(title = "Test Error vs. Number of Hidden Units",
       x = "Number of Hidden Units",
       y = "Mean Test Error") +
  theme_minimal()

```  

Out of all the models under consideration, the neural network with a single hidden unit performs worst. It has the largest mean average test error and also the largest variation in the average test error  for different random starting weights. Somewhat surprisingly, the linear model, i.e. the neural network with no hidden units, performs almost as good as the neural network with two hidden units. While the mean average test errors of these two models are similar, the average test error is much less sensitive to different starting weights in case of the linear model. All neural networks with 3 or more hidden units have a similar performance, both in terms of the mean average test error and the standard deviation of the average test error for different random starting weights. Overall, we conclude that choosing a larger number of hidden units and imposing shrinkage via weight decay appears to be better than having too few hidden units in the first place.


<!-- EXERCISE 2 -->
\newpage
# Exercise 2

The data sets `zip.train` and `zip.test` from package \textbf{ElemStatLearn} contain information on the gray color values of the pixels on a $16 \times 16$ pixel image of hand-written digits. We first visualize for each digit one randomly selected observation.

```{r include=FALSE}
set.seed(1234)

data("zip.train")
data("zip.test")

# helper to select observations
findRows <- function(zip, n) {
 # find  n (random) rows with zip representing 0,1,2,...,9
 res <- vector(length=10, mode="list")
 names(res) <- 0:9
 ind <- zip[,1]
 for (j in 0:9) {
    res[[j+1]] <- sample(which(ind==j), n, replace = FALSE) 
    }
 return(res) 
 }

# make plot
digits <- vector(length=10, mode="list")
names(digits) <- 0:9
rows <- findRows(zip.train, 1) # get one observation for each digit
for (j in 0:9) {
    digits[[j+1]] <- zip2image(zip.train, rows[[j+1]])
}
im <- do.call("rbind", digits)



```

```{r}
image(im, col=gray(256:0/256), zlim=c(0,1), xlab="", ylab="" ) 
```


We now fit a multinomial logistic regression model to the training data and evaluate it on the training and the test data. Before fitting the model, however, we transform the data such that the regressors are scaled on the unit interval.

```{r include=FALSE}

helper_transform <- function(x){
  maxs <- apply(x, 2, max)
  mins <- apply(x, 2, min)
  
  output <- sweep(sweep(x, 2, mins, "-"), 2, maxs - mins, "/")
  return(output)
}

zip_train_df <- as.data.frame(zip.train)
zip_test_df <- as.data.frame(zip.test)

# scale rhs variables to [0,1]
zip_train_df[,2:ncol(zip_train_df)] <- helper_transform(zip_train_df[,2:ncol(zip_train_df)])
zip_test_df[,2:ncol(zip_test_df)] <- helper_transform(zip_test_df[,2:ncol(zip_test_df)])

# turn dependent variable into factor
zip_train_df[,1] <- as.factor(zip_train_df[,1])
zip_test_df[,1] <- as.factor(zip_test_df[,1])

colnames(zip_train_df) <- c("y", paste0("x",1:(ncol(zip_train_df)-1)))
colnames(zip_test_df) <- c("y", paste0("x",1:(ncol(zip_test_df)-1)))

# fit the model
multinom_fit <- multinom(y~., data = zip_train_df, MaxNWts = 10000)


```

We now determine the overall misclassification rate on the training and the test data and the digit-specific misclassification rates on the test data. 

```{r}
# misclassification rate on training data
mcr_train <- mean(predict(multinom_fit) != zip_train_df$y)
print(paste0("misclassification rate (training data): ", round(100*mcr_train,2),"%"))

# misclassification rate on test data
mcr_test <- mean(predict(multinom_fit, newdata = zip_test_df) != zip_test_df$y)
print(paste0("misclassification rate (test data): ", round(100*mcr_test,2),"%"))

# digit-specific misclassification rate on test data
misclassified <- which(predict(multinom_fit, newdata = zip_test_df) != zip_test_df$y)
mcr_digit <- paste0(round(100*table(zip_test_df[misclassified,"y"])/table(zip_test_df$y),2),"%")
names(mcr_digit) <- 0:9
print("digit-specific misclassification rates (test data):")
print(mcr_digit)

```
The misclassification rate on the training data is exceptionally low, while the one on the test data is fairly high. The digits 8,4 and 2 are particularly difficult to classify, whereas the model does a good job in classifying 0,1, and 9. 

The substantial discrepancy in performance between training and test data suggests that overfitting may be an issue. Hence, we add a positive weight decay of 0.05 when fitting the multinomial logistic regression model in order to regularize it.


```{r include=FALSE}
multinom_fit2 <- multinom(y~., data = zip_train_df, MaxNWts = 10000, 
                                     decay = 0.05)
```


```{r}


# misclassification rate on training data
mcr_train <- mean(predict(multinom_fit2) != zip_train_df$y)
print(paste0("misclassification rate (training data): ", round(100*mcr_train,2),"%"))

# misclassification rate on test data
mcr_test <- mean(predict(multinom_fit2, newdata = zip_test_df) != zip_test_df$y)
print(paste0("misclassification rate (test data): ", round(100*mcr_test,2),"%"))

# digit-specific misclassification rate on test data
misclassified <- which(predict(multinom_fit2, newdata = zip_test_df) != zip_test_df$y)
mcr_digit <- paste0(round(100*table(zip_test_df[misclassified,"y"])/table(zip_test_df$y),2),"%")
names(mcr_digit) <- 0:9
print("digit-specific misclassification rates (test data):")
print(mcr_digit)
```
Adding weight decay slightly increases the misclassification rate on the training data, but reduces the misclassification rate on the test data. The improved classification performance is particularly pronounced for the "difficult" digits (8 and 4). Overall, adding a Ridge penalty to the loss function, i.e. adding the weight decay, improves the out-of-sample performance of our model by mitigating the overfitting problem.

\newpage
<!-- EXERCISE 3 -->
# Exercise 3

We continue using the data sets `zip.train` and `zip.test` from package \textbf{ElemStatLearn}. However, we now only use a subset of size 320 from `zip.train`, with an equal number of observations for each digit to fit a multinomial logistic regression model and a neural network. We use the remaining observations from `zip.train` and the test data to evaluate the fitted models.

Given the small sample size of the training data, overfitting is most likely an issue. We therefore visualize the performance on the test data in dependence of the training epochs (\# epochs $\in \{10,20,30,50,100,150,200\}$) when fitting the models.

```{r include=FALSE}
set.seed(1234)

# select observations for new training data
train_rows <- unlist(findRows(zip.train,n=32),use.names = FALSE)
zip_new_train_df <- as.data.frame(zip.train[train_rows,])

# combine "zip.test" and other observations from "zip.test"
zip_new_test_df <- as.data.frame(rbind(zip.train[-train_rows,], zip.test))

# scale rhs variables to [0,1]
zip_new_train_df[,2:ncol(zip_new_train_df)] <- helper_transform(
  zip_new_train_df[,2:ncol(zip_new_train_df)])
zip_new_test_df[,2:ncol(zip_new_test_df)] <- helper_transform(
  zip_new_test_df[,2:ncol(zip_new_test_df)])

# turn dependent variable into factor
zip_new_train_df[,1] <- as.factor(zip_new_train_df[,1])
zip_new_test_df[,1] <- as.factor(zip_new_test_df[,1])

colnames(zip_new_train_df) <- c("y", paste0("x",1:(ncol(zip_new_train_df)-1)))
colnames(zip_new_test_df) <- c("y", paste0("x",1:(ncol(zip_new_test_df)-1)))

# list of training epochs
epochs_list <- list(10,20,30,50,100,150,200)
names(epochs_list) <- paste0("e=", epochs_list)

# multinomial logistic regression
mcrs_multinom <- lapply(epochs_list, function(e){
  fit <- multinom(y ~ ., data = zip_new_train_df, maxit = e, MaxNWts = 10000)
  c(train = mean(predict(fit) != zip_new_train_df$y),
    test = mean(predict(fit, newdata = zip_new_test_df) != zip_new_test_df$y))
})

mcrs_multinom <- do.call(rbind,mcrs_multinom)


# neural network with 5 hidden units
mcrs_nn5 <- lapply(epochs_list, function(e){
  fit <- nnet(y ~ ., data = zip_new_train_df, size = 5, maxit = e, 
              MaxNWts = 10000, skip = TRUE)
  c(train = mean(predict(fit, type = "class") != zip_new_train_df$y),
    test = mean(predict(fit, newdata = zip_new_test_df, type = "class") != zip_new_test_df$y))
})

mcrs_nn5 <- do.call(rbind,mcrs_nn5)

# neural network with 10 hidden units
mcrs_nn10 <- lapply(epochs_list, function(e){
  fit <- nnet(y ~ ., data = zip_new_train_df, size = 10, maxit = e, 
              MaxNWts = 10000, skip = TRUE)
  c(train = mean(predict(fit, type = "class") != zip_new_train_df$y),
    test = mean(predict(fit, newdata = zip_new_test_df, type = "class") != zip_new_test_df$y))
})

mcrs_nn10 <- do.call(rbind,mcrs_nn10)

# neural network with 30 hidden units
mcrs_nn20 <- lapply(epochs_list, function(e){
  fit <- nnet(y ~ ., data = zip_new_train_df, size = 20, maxit = e,  
              MaxNWts = 10000, skip = TRUE)
  c(train = mean(predict(fit, type = "class") != zip_new_train_df$y),
    test = mean(predict(fit, newdata = zip_new_test_df, type = "class") != zip_new_test_df$y))
})

mcrs_nn20 <- do.call(rbind,mcrs_nn20)





```


```{r out.width="90%"}
# make plots
matplot(epochs_list, mcrs_multinom, type="b",pch=19,lty=1, xaxt = 'n',
        xlab = "epochs", ylab = "misclassification rate", main = "Multinomial Logit")
axis(1,epochs_list, cex.axis = 0.8)
legend("right", c("train", "test"), lty = 1, col = 1:2, box.lwd = 0)

```


```{r out.width="90%"}


matplot(epochs_list, mcrs_nn5, type="b",pch=19,lty=1, xaxt = 'n',
        xlab = "epochs", ylab = "misclassification rate", main = "NN (5 hidden units)")
axis(1,epochs_list, cex.axis = 0.8)
legend("right", c("train", "test"), lty = 1, col = 1:2, box.lwd = 0)




```


```{r out.width="90%"}

matplot(epochs_list, mcrs_nn10, type="b",pch=19,lty=1, xaxt = 'n',
        xlab = "epochs", ylab = "misclassification rate", main = "NN (10 hidden units)")
axis(1,epochs_list, cex.axis = 0.8)
legend("right", c("train", "test"), lty = 1, col = 1:2, box.lwd = 0)



```

```{r out.width="90%"}

matplot(epochs_list, mcrs_nn20, type="b",pch=19,lty=1, xaxt = 'n',
        xlab = "epochs", ylab = "misclassification rate", main = "NN (20 hidden units)")
axis(1,epochs_list, cex.axis = 0.8)
legend("right", c("train", "test"), lty = 1, col = 1:2, box.lwd = 0)

```


The plots are in line with our assumption that overfitting is an issue in this exercise. For all models (multinomial logit, neural networks with 5, 10 and 20 hidden units) the misclassification rate on the test data increases as the number of training epochs exceeds 30. Hence, in the absence of a more explicit form of regularization (e.g. a positive weight decay) stopping the optimization routine early can be helpful to improve out-of-sample performance.

\newpage
<!-- EXERCISE 4 -->
# Exercise 4

In the following we will estimate a predictive model for the `Default` data from the \textbf{ISLR2} package. We fit a neural network using a single hidden layer with 10 units and dropout regularization.

```{r include=FALSE}
rm(list=ls())

library(pROC)

data("Default", package = "ISLR2") # Load data
head(Default)

# Converting "default" and "student"  binary numeric variables for the neural network model.
Default$default <- ifelse(Default$default == "Yes", 1, 0)
Default$student <- ifelse(Default$student == "Yes", 1, 0)

# separating the data into train and test (66:33 split)
set.seed(123)
torch_manual_seed(123)

n <- nrow(Default)
ntest <- trunc(n/3)
testid <- sample(1:n, ntest)

Default_test <- Default[testid,]
Default_train <- Default[-testid, ]


### --- Logistic Regression --- ###

logit_model <- glm(default ~ ., data = Default_train, family = binomial(link = "logit"))

# Predicting on test data
logit_pred <- predict(logit_model, newdata = Default_test, type = "response")
logit_pred_class <- ifelse(logit_pred > 0.5,1,0)
y_test <- Default[testid,]$default
accuracy_logistic <- mean(y_test == logit_pred_class)



### --- Neural Network --- ###

net <- nn_module(
  initialize = function(input_size) {
    self$linear1 <- nn_linear(input_size, 10)# hidden layer with 10 hidden units
    self$linear2 <- nn_linear(10, 1) # output layer
    self$activation1 <- nn_relu() # RELU activation function for hidden layer
    self$activation2 <- nn_sigmoid() # sigmoid activation function for output layer (binary logit)
    self$dropout <- nn_dropout(p = 0.4) # randomly remove 40% of observations in hidden layer
  },
  forward = function(x) {
    x %>% 
      self$linear1() %>%
      self$activation1() %>%
      self$dropout() %>%
      self$linear2() %>%
      self$activation2()
  }
) %>%
  setup( # setup for estimation
    loss = nn_bce_loss(),
    optimizer = optim_rmsprop,
    metrics = list(luz_metric_binary_accuracy())
  )
# Note: we don't need a (sigmoid) transformation function, since the loss function
#       we employ combines a sigmoid layer with a binary cross entropy loss

  
# transform the data (scale to unit interval)
y_train <- as.matrix(Default_train$default)
x_train <- Default_train[,-1]
maxs <- apply(x_train, 2, max)
mins <- apply(x_train, 2, min)
x_train <- as.matrix(sweep(sweep(x_train, 2, mins, "-"), 2, maxs-mins, "/"))

y_test <- as.matrix(Default_test$default)
x_test <- Default_test[,-1]
maxs <- apply(x_test, 2, max)
mins <- apply(x_test, 2, min)
x_test <- as.matrix(sweep(sweep(x_test, 2, mins, "-"), 2, maxs-mins, "/"))

# fit the model
fitted <- net %>%
      set_hparams(input_size = ncol(x_train)) %>%
      fit(
        data = list(x_train, y_train),
        valid_data = list(x_test, y_test),
        epochs = 50 # in-sample MSE quickly drops for only a few epochs 
      ) 


# Predicting on test data
nn_pred_prob <- as.matrix((predict(fitted, x_test)))
nn_pred <- ifelse(nn_pred_prob > 0.5, 1,0)
accuracy_nn <- mean(y_test == nn_pred)

# # Plot training and validation loss
# plot(1:epochs, train_loss, type = "l", col = "red", ylim = range(c(train_loss, valid_loss)), 
#      xlab = "Epoch", ylab = "Loss", main = "Training and Validation Loss")
# lines(1:epochs, valid_loss, col = "green")
# legend("topright", legend = c("Train", "Validation"), col = c("red", "green"), lwd = 2)

```

```{r}
plot(fitted)
```


The linear logistic regression model performs very well on the test data and has a classification accuracy of `r round(100*accuracy_logistic,2)`\%. Our neural network also performs well and has a slightly lower accuracy of `r round(100*accuracy_nn, 2)`\%. Looking at the plots above, we can see that the value of the loss function decreases as the number of training epochs increases - for both the training and the test data. Given that the loss function evaluated at the test data does not increase as the number of epochs grows larger, we find no evidence for overfitting. The classification performance slightly improves as the number of training epochs increases. However, it does not improve monotonically and the marginal gains are rather negligible. 

We now compare the classification performance of the two models more closely, by looking at their ROC curve and confusion matrix. In the ROC plot we observe that the curves for the linear logistic regression model and the neural network follow each other closely. The confusion matrices, however, highlight some subtle differences between the two models. The logistic regression model predicts almost twice as many defaults (50 in total, 13 of them incorrectly) as the neural network (27 in total, 3 of them incorrectly). 

```{r }

# Plot ROC Curves and Calculate AUC
roc_nn <- roc(Default_test$default, as.numeric(nn_pred_prob))
roc_logit <- roc(Default_test$default, logit_pred)

# Plot ROC curve
plot(roc_nn, col = "blue", main = "ROC Curves for Neural Network and Logistic Regression")
lines(roc_logit, col = "red")
legend("bottomright", legend = c("Neural Network", "Logistic Regression"), 
       col = c("blue", "red"), lwd = 2)

# Print AUC values
cat("AUC for Neural Network:", auc(roc_nn), "\n")
cat("AUC for Logistic Regression:", auc(roc_logit), "\n")


# comparing model performance
# Confusion Matrix for Neural Network
nn_conf_matrix <- confusionMatrix(factor(as.integer(nn_pred)), factor(Default_test$default))
print("Neural Network:")
print(nn_conf_matrix)

# Confusion Matrix for Logistic Regression
logit_conf_matrix <- confusionMatrix(factor(logit_pred_class), factor(Default_test$default))
print("Logistic Regression:")
print(logit_conf_matrix)
```


\newpage
<!-- EXERCISE 5 -->
# Exercise 5
Now we perform document classification on the  `IMDb` data set, which is available as part of the \textbf{torchdatasets} package. We limit the dictionary size to the 10,000 most frequently-used words and tokens. Again, we use James et al. (2021, Chapter 10  torch version). We begin by loading the data and creating a `imdb_tain` and `imdb_test` object. Each element of `imdb_train` is a vector of numbers between 1 and 10000 (the document), referring to the words found in the dictionary. Next we write a function to one-hot encode each document in a list of documents, and return a binary matrix in sparse-matrix format. To construct the sparse matrix, one supplies just the entries that are nonzero. In the last line we call the function `sparseMatrix()` and supply the row indices corresponding to each document and the column indices corresponding to the words in each document, since we omit the values they are taken to be all ones. Words that appear more than once in any given document still get recorded as a one. Next we fit a fully-connected neural network with two hidden layers, each with 16 units and ReLU activation.

```{r eval=FALSE, include=FALSE}
#https://www.casact.org/sites/default/files/2022-12/James-G.-et-al.-2nd-edition-Springer-2021.pdf
library(torch)
library(torchdatasets)
library(luz)

gen_path<-"~/"  #Specific to the computer
max_features <- c(500, 1000, 3000, 5000, 10000)
# Function to create a directory and set it as the working directory

# Define the base path where directories will be created

# Function to create a directory and set it as the working directory
set_working_directory <- function(path) {
  if (!dir.exists(path)) {
    dir.create(path, recursive = TRUE)
  }
  setwd(path)
}


set.seed(1)

for (f in max_features) {

# Load the IMDb dataset from torchdatasets
imdb_train <- imdb_dataset(
  root = ".", 
  download = TRUE,
  split="train",
  num_words = f
)

imdb_test <- imdb_dataset(
  root = ".", 
  download = TRUE,
  split="test",
  num_words = f
)

imdb_train[1]$x[1:12]

library(Matrix)
one_hot <- function(sequences, dimension) {
   seqlen <- sapply(sequences, length)
   n <- length(seqlen)
   rowind <- rep(1:n, seqlen)
   colind <- unlist(sequences)
   sparseMatrix(i = rowind, j = colind,
      dims = c(n, dimension))
}

# collect all values into a list
train <- seq_along(imdb_train) %>% 
  lapply(function(i) imdb_train[i]) %>% 
  purrr::transpose()
test <- seq_along(imdb_test) %>% 
  lapply(function(i) imdb_test[i]) %>% 
  purrr::transpose()

# num_words + padding + start + oov token = f + 3
x_train_1h <- one_hot(train$x, f + 3)
x_test_1h <- one_hot(test$x, f + 3)
dim(x_train_1h)

#nnzero(x_train_1h) / (25000 * (10000 + 3))
#Only 1.3% of the entries are nonzero, so this amounts to considerable savings in memory. We create a validation set of size 2,000, leaving 23,000 for training.

ival <- sample(seq(along = train$y), (f*0.2))
itrain <- seq_along(train$y)[-ival]

#Next we fit a fully-connected neural network with two hidden layers, each with 16 units and ReLU activation.

model <- nn_module(
  initialize = function(input_size = f + 3) {
    self$dense1 <- nn_linear(input_size, 16)
    self$relu <- nn_relu()
    self$dense2 <- nn_linear(16, 16)
    self$output <- nn_linear(16, 1)
  },
  forward = function(x) {
    x %>% 
      self$dense1() %>% 
      self$relu() %>% 
      self$dense2() %>% 
      self$relu() %>% 
      self$output() %>% 
      torch_flatten(start_dim = 1)
  }
)
model <- model %>% 
  setup(
    loss = nn_bce_with_logits_loss(),
    optimizer = optim_rmsprop,
    metrics = list(luz_metric_binary_accuracy_with_logits())
  ) %>% 
  set_opt_hparams(lr = 0.001)

# Fit the model with training and validation data

fitted <- model %>% 
  fit(
    # we transform the training and validation data into torch tensors
    list(
      torch_tensor(as.matrix(x_train_1h[itrain,]), dtype = torch_float()), 
      torch_tensor(unlist(train$y[itrain]))
    ),
    valid_data = list(
      torch_tensor(as.matrix(x_train_1h[ival, ]), dtype = torch_float()), 
      torch_tensor(unlist(train$y[ival]))
    ),
    dataloader_options = list(batch_size = 512),
    epochs = 10
  )

p_fitted <- plot(fitted) 


# Fit the model with test data as validation data to get test accuracy
fitted_test <- model %>% 
  fit(
    list(
      torch_tensor(as.matrix(x_train_1h[itrain, ]), dtype = torch_float()), 
      torch_tensor(unlist(train$y[itrain]), dtype = torch_float())
    ),
    valid_data = list(
      torch_tensor(as.matrix(x_test_1h), dtype = torch_float()), 
      torch_tensor(unlist(test$y), dtype = torch_float())
    ),
    dataloader_options = list(batch_size = 512),
    epochs = 10
  )

p_fittedtest <-plot(fitted_test)
# Helper function to extract metrics
extract_metric <- function(metric_list, metric_name) {
  sapply(metric_list, function(epoch) epoch[[metric_name]])}
  
  
# Extract metrics
train_loss <- extract_metric(fitted$records$metrics$train, "loss")
val_loss <- extract_metric(fitted$records$metrics$valid, "loss")
test_loss <- extract_metric(fitted_test$records$metrics$valid, "loss")


train_acc <- extract_metric(fitted$records$metrics$train, "acc")
val_acc <- extract_metric(fitted$records$metrics$valid, "acc")
test_acc <- extract_metric(fitted_test$records$metrics$valid, "acc")  



# Create data frame for plotting
df <- data.frame(
  Epoch = rep(1:10, 3),
  Metric = c(rep("Train", 10), rep("Validation", 10), rep("Test", 10)),
  Loss = c(train_loss, val_loss, test_loss),
  Accuracy = c(train_acc, val_acc, test_acc)
)

library(ggplot2)
# Plotting the results
gg_loss <- ggplot(df, aes(x = Epoch)) +
  geom_line(aes(y = Loss, color = Metric), na.rm = TRUE) +
  labs(y = "Loss") +
  scale_color_manual(values = c("blue", "red", "orange")) +
  theme_minimal() +
  ggtitle("Loss per Epoch")

gg_acc <- ggplot(df, aes(x = Epoch)) +
  geom_line(aes(y = Accuracy, color = Metric)) +
  labs(y = "Accuracy") +
  scale_color_manual(values = c("blue", "red", "orange")) +
  theme_minimal() +
  ggtitle("Accuracy per Epoch")



  # Save ggplot2 plots

  ggsave(filename = paste0("gg_loss_", f, ".png"), plot = gg_loss 
         + theme(plot.background = element_rect(fill = "white")))
  ggsave(filename = paste0("gg_acc_", f, ".png"), plot = gg_acc
         + theme(plot.background = element_rect(fill = "white")))
  
 # Save base R plots
  png(filename = paste0("p_fitted_", f, ".png"))
  p_fitted
  dev.off()
  
  png(filename = paste0("p_fittedtest_", f, ".png"))
  p_fittedtest
  dev.off()
  
 objects_to_keep <- c("gg_loss", "gg_acc","df","p_fittedtest","p_fitted","f")
#rm(list = setdiff(ls(), objects_to_keep))

 
 rm(list = setdiff(ls(), objects_to_keep))
  # Save data frame
  save.image(file = paste0(f, ".RData"))

}
```
After fitting the fully-connected neural network we can now look at the results with the dictionary size 1000. We look at how the accuracy and the loss of our train, test and validation set evolve over 10 epochs. The accuracy graph displays that for the dictionary size of 1000 the accuracy is highest for the validation set (above 0.88), followed by the train set and lowest for the test set. Also for the train set the accuracy increased sharply after the first 2 epochs. For test and train it starts higher up but has higher variability. Considering the loss, we see a similar development for our train set: For the first 2 epochs the loss is rather high and then drops down and remains at more stable level. The loss for the test and validation sets starts at a lower point again, with the validation loss being below the test loss.  

```{r}
library(gridExtra)

load("1000.RData")
#p_fitted
#p_fittedtest
grid.arrange(gg_acc, gg_loss, ncol = 2)



```
We then vary the dictionary size and try out values 500, 1000, 3000, 5000, 10,000 and consider the effects of this varying dictionary size. We observe that the larger the dictionary size the higher the accuracy (above 0.95 for dictionary size 10,000) and the lower the loss for the training set. Moreover, the higher the accuracy of the training set, the (slightly) higher the accuracy of the validation and test sets. In addition, from the loss plots we can observe that the higher the dictionary size, the more the loss varies between training, test and validation set. Furthermore, the loss for the test set increases strongly with more epochs, while the loss for the training data set decreases (with more epochs).

```{r}

load("500.RData")
#p_fitted
#p_fittedtest
grid.arrange(gg_acc, gg_loss, ncol = 2)

```

```{r}

load("3000.RData")
#p_fitted
#p_fittedtest
grid.arrange(gg_acc, gg_loss, ncol = 2)
 
```

```{r}

load("5000.RData")
#p_fitted
#p_fittedtest
grid.arrange(gg_acc, gg_loss, ncol = 2)

```

```{r}

load("10000.RData")
#p_fitted
#p_fittedtest
grid.arrange(gg_acc, gg_loss, ncol = 2)

```