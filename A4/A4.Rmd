---
title: "Statistical Learning (5454) - Assignment 4"  
author: "Matthias Hochholzer, Lukas Pirnbacher, Anne Valder"
date: "Due: 2024-06-10"
output: 
  pdf_document: 
    keep_tex: yes
header-includes:
   - \usepackage{titlesec}
   - \titleformat*{\section}{\normalfont\Large\bfseries\flushleft}
   - \titleformat*{\subsection}{\normalfont\large\bfseries\flushleft}
   - \titleformat*{\subsubsection}{\normalfont\normalsize\bfseries\flushleft}
   - \usepackage{amsmath}
   - \newcommand*{\defeq}{\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}=}
   - \newcommand*{\eqdef}{=\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}}
                     
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)
```


```{r,include=FALSE}
# Clear memory
rm(list=ls())
```

```{r, echo=FALSE, include=FALSE, warning=FALSE}
library("nnet")
library(ISLR2)
library(torch)
library(caret)
library(ggplot2)
library(MASS)

```



<!-- EXERCISE 1 -->
# Exercise 1

We generate data from the following additive error model $Y = f(X_1,X_2) + \epsilon$. The sum of sigmoids is
$$ Y = \sigma(a_1^\top X_1) + \sigma(a_2^\top X_2) + \epsilon, $$
with $a_1 = (3, 3)$, $a_2 = (3,−3)$.
– Each $X_j$ , $j = 1, 2$, is a standard Gaussian variate with $p = 2$.
– $\epsilon$ is an independent Gaussian error with variance chosen such that the signal-to-noise ratio as measured
by the respective variances equals four.

We Generate a training set of size 100 and a test sample of size 10,000.
```{r}
set.seed(123)

a1 <- c(3,3)
a2 <- c(3,-3)
n_train <- 100
n_test <- 10000
p <- 2

# X1,X2 richtig generiret?
# variance von f(...)?
# learning rate?

# Generate data
generate_data <- function(n, a1, a2, p) {
  X <- mvrnorm(n, mu = rep(0, p), Sigma = diag(p))
  epsilon <- rnorm(n, mean = 0, sd = sqrt(var(X %*% a1) / 4)) # SNR=4
  Y <- 1 / (1 + exp(-X %*% a1)) + 1 / (1 + exp(-X %*% a2)) + epsilon
  list(X = X, Y = Y)
}

# Generatethe train and test set
train_data <- generate_data(n_train, a1, a2, p)
test_data <- generate_data(n_test, a1, a2, p)

x_train <- train_data$X
y_train <- train_data$Y
x_test <- test_data$X
y_test <- test_data$Y
```

We then fit a neural networks with weight decay of 0.0005 and we vary the number of hidden units from 0 to 10 and record the average test error $E_{Test}(Y − \hat{f}(X_1,X_2))^2$ for each of 10 random starting weights. I.e. for each of the settings with a different number of hidden units we fit a neural network 10 times with different initial values.

```{r, warning=F}
# Define the neural network model
net <- nn_module(
  initialize = function(hidden_units) {
    self$hidden_units <- hidden_units
    if (hidden_units > 0) {
      self$fc1 <- nn_linear(p, hidden_units)
      self$fc2 <- nn_linear(hidden_units, 1)
    } else {
      self$fc <- nn_linear(p, 1)
    }
  },
  forward = function(x) {
    if (self$hidden_units > 0) {
      x %>%
        self$fc1() %>%
        nnf_relu() %>%
        self$fc2()
    } else {
      x %>% self$fc()
    }
  }
)


hidden_units_list <- 0:5
n_repeats <- 10
results <- data.frame()

for (hidden_units in hidden_units_list) {
  cat(sprintf("Training with %d hidden units\n", hidden_units))
  test_errors <- numeric(n_repeats)
  
  for (rep in 1:n_repeats) {
    model <- net(hidden_units)
    optimizer <- optim_adam(model$parameters, lr = 0.001, weight_decay = 0.0005)
    criterion <- nn_mse_loss()
    
    x_train_tensor <- torch_tensor(as.matrix(x_train), dtype = torch_float())
    y_train_tensor <- torch_tensor(y_train, dtype = torch_float())#$unsqueeze(2)
    x_test_tensor <- torch_tensor(as.matrix(x_test), dtype = torch_float())
    y_test_tensor <- torch_tensor(y_test, dtype = torch_float())#$unsqueeze(2)
    
    # Training loop
    model$train()
    for (epoch in 1:200) {
      optimizer$zero_grad()
      output <- model(x_train_tensor)
      loss <- criterion(output, y_train_tensor)
      loss$backward()
      optimizer$step()
    }
    
    # Evaluate the model on the test data
    model$eval()
    with_no_grad({
      output_test <- model(x_test_tensor)
      test_error <- criterion(output_test, y_test_tensor)$item()
    })
    
    test_errors[rep] <- test_error
  }
  
  mean_test_error <- mean(test_errors)
  sd_test_error <- sd(test_errors)
  
  results <- rbind(results, data.frame(hidden_units = hidden_units, mean_test_error = mean_test_error, sd_test_error = sd_test_error))
}

print(results)

```

Let's now visualize the results and interpret them.

```{r}
ggplot(results, aes(x = hidden_units, y = mean_test_error)) +
  geom_point() +
  geom_line() +
  geom_errorbar(aes(ymin = mean_test_error - sd_test_error, ymax = mean_test_error + sd_test_error), width = 0.2) +
  labs(title = "Test Error vs. Number of Hidden Units",
       x = "Number of Hidden Units",
       y = "Mean Test Error") +
  theme_minimal()

```

```{r}

```

\newpage
<!-- EXERCISE 2 -->

# Exercise 2

The data sets \texit{zip.train} and \textit{zip.test} from package \textbf{ElemStatLearn} contain the information on the gray color
values of the pixels on a $16 \times 16$ pixel image of hand-written digits. We first visualize for each digit one randomly selected observation.

```{r}

```

we then fit a multinomial logistic regression model to the training data and evaluate it on the training and the test data. We determine the overall misclassification rate on the training and the test data and the digit-specific misclassification rates on the test data. 

```{r}

```

Which digits are the most difficult and the easiest to classify?

We add a positive weight decay of 0.05 when fitting the multinomial logistic regression model to the training data and evaluate the model on the training and the test data. We determine the overall misclassification rate on the training and the test data. 

```{r}

```


Explain why it makes sense to also include weight decay when fitting this multinomial logistic regression model.

\newpage
<!-- EXERCISE 3 -->
# Exercise 3

The data sets \textit{zip.train} and \textit{zip.test} from package \textbf{ElemStatLearn} contain the information on the gray color values of the pixels on a $16 \times 16$ pixel image of hand-written digits. Use only a subset from \textit{zip.train} of size 320 observations with an equal number of observations for each
digit to fit a multinomial logistic regression model and a neural network.

```{r}

```

Use all remaining training observations and the test data set to evaluate the fitted models.

```{r}

```

For this small training data overfitting is an issue. Visualize the performance on the test data in dependence of the training epochs when fitting the models.

```{r}

```



\newpage
<!-- EXERCISE 4 -->
# Exercise 4

In the following we will estimate a predictive model for the Default data from the \textbf{ISLR2} pacakge. We fit a neural network using a single hidden layer with 10 units and dropout regularization. We use the hint and use James et al. (2021, Chapter 10).

```{r}
rm(list=ls())

data("Default", package = "ISLR2") # Load data
head(Default)
#Converting the default column to a binary numeric variable for the neural network model.

Default$default <- ifelse(Default$default == "Yes", 1, 0)
Default$student <- ifelse(Default$student == "Yes", 1, 0)

# seperating the data into train and test
set.seed(123)
n <- nrow(Default)
ntest <- trunc(n/3)
testid <- sample(1:n, ntest)

Default_test <- Default[testid,]
Default_train <- Default[-testid, ]

# Logistic regression model
logit_model <- glm(default ~ ., data = Default_train, family = binomial)
# Predicting on test data
logit_pred <- predict(logit_model, newdata = Default_test, type = "response")
logit_pred_class <- ifelse(logit_pred > 0.5,1,0)
y_test <- Default[testid,]$default
accuracy_logistic <- mean(y_test == logit_pred_class)

# scaling the data for neural networks to ensure that the model converges more efficiently

maxs <- apply(Default[, -1], 2, max)
mins <- apply(Default[, -1], 2, min)
x <-  sweep(sweep(Default[,-1], 2, mins, "-"), 2, maxs - mins, "/")

#Convert the data to torch tensors
x_train <- torch_tensor(as.matrix(x[-testid, ]), dtype = torch_float())
y_train <- torch_tensor(matrix(Default[-testid,1]), dtype = torch_float())$unsqueeze(2)
x_test <- torch_tensor(as.matrix(x[testid, ]), dtype = torch_float())
y_test <- torch_tensor(matrix(Default[testid,1]), dtype = torch_float())$unsqueeze(2)


# Neural network model
#torch::install_torch()


## Define the neural network model
model_nn <- nn_module(
  initialize = function() {
    self$fc1 <- nn_linear(ncol(x_train) , 10)
    self$dropout <- nn_dropout(p = 0.4)
    self$fc2 <- nn_linear(10, 1)
  },
  forward = function(x) {
    x %>% 
      self$fc1() %>% 
      nnf_relu() %>% 
      self$dropout() %>% 
      self$fc2() %>% 
      nnf_sigmoid()
  }
)

net <- model_nn()
optimizer <- optim_adam(net$parameters, lr = 0.001)


# Training the model and recording loss
epochs <- 10
train_loss <- numeric(epochs)
valid_loss <- numeric(epochs)

for (epoch in 1:epochs) {
  net$train()
  optimizer$zero_grad()
  output <- net(x_train)
  loss <- nnf_binary_cross_entropy(output, y_train)
  loss$backward()
  optimizer$step()
  train_loss[epoch] <- loss$item()
  
  # Calculate validation loss
  net$eval()
  with_no_grad({
    output_valid <- net(x_test) #nnf_binary_cross_entropy
    loss_valid <- nnf_binary_cross_entropy(output_valid, y_test)
    valid_loss[epoch] <- loss_valid$item()
  })
  
  if (epoch %% 10 == 0) {
    cat(sprintf("Epoch: %d, Train Loss: %3f, Validation Loss: %3f\n", epoch, train_loss[epoch], valid_loss[epoch]))
  }
}

  
# Predicting on test data
nn_pred_prob <- net(x_test)
nn_pred <- ifelse(nn_pred_prob > 0.5, 1,0)
y_test <- Default[testid,]$default
accuracy_nn <- mean(y_test == nn_pred)

# Plot training and validation loss
plot(1:epochs, train_loss, type = "l", col = "red", ylim = range(c(train_loss, valid_loss)), 
     xlab = "Epoch", ylab = "Loss", main = "Training and Validation Loss")
lines(1:epochs, valid_loss, col = "green")
legend("topright", legend = c("Train", "Validation"), col = c("red", "green"), lwd = 2)

```

We ompare the classification performance of this model with that of linear logistic regression.

```{r}
# Plot ROC Curves and Calculate AUC
library(pROC)

roc_nn <- roc(Default_test$default, as.numeric(nn_pred_prob))
roc_logit <- roc(Default_test$default, logit_pred)

# Plot ROC curve
plot(roc_nn, col = "blue", main = "ROC Curves for Neural Network and Logistic Regression")
lines(roc_logit, col = "red")
legend("bottomright", legend = c("Neural Network", "Logistic Regression"), col = c("blue", "red"), lwd = 2)

# Print AUC values
cat("AUC for Neural Network:", auc(roc_nn), "\n")
cat("AUC for Logistic Regression:", auc(roc_logit), "\n")


# comparing model performance
# Confusion Matrix for Neural Network
nn_conf_matrix <- confusionMatrix(factor(as.integer(nn_pred)), factor(Default_test$default))
print(nn_conf_matrix)

# Confusion Matrix for Logistic Regression
logit_conf_matrix <- confusionMatrix(factor(logit_pred_class), factor(Default_test$default))
print(logit_conf_matrix)
```


\newpage
<!-- EXERCISE 5 -->
# Exercise 5

Consider the \textit{IMDb} dataset from the \textbf{keras} package to perform document classification. Restrict the vocabulary to the most frequently-used words and tokens. Again, we use the hint and use James et al. (2021, Chapter 10).

```{r}
#https://www.casact.org/sites/default/files/2022-12/James-G.-et-al.-2nd-edition-Springer-2021.pdf
library(torch)
library(torchdatasets)
library(luz)

set.seed(1)
max_features <- 500
imdb_train <- imdb_dataset(
  root = ".", 
  download = TRUE,
  split="train",
  num_words = max_features
)
imdb_test <- imdb_dataset(
  root = ".", 
  download = TRUE,
  split="test",
  num_words = max_features
)

imdb_train[1]$x[1:12]

word_index <- imdb_train$vocabulary
decode_review <- function(text, word_index) {
   word <- names(word_index)
   idx <- unlist(word_index, use.names = FALSE)
   word <- c("<PAD>", "<START>", "<UNK>", word)
   words <- word[text]
   paste(words, collapse = " ")
}
decode_review(imdb_train[1]$x[1:12], word_index)

library(Matrix)
one_hot <- function(sequences, dimension) {
   seqlen <- sapply(sequences, length)
   n <- length(seqlen)
   rowind <- rep(1:n, seqlen)
   colind <- unlist(sequences)
   sparseMatrix(i = rowind, j = colind,
      dims = c(n, dimension))
}
train <- seq_along(imdb_train) %>% 
  lapply(function(i) imdb_train[i]) %>% 
  purrr::transpose()
test <- seq_along(imdb_test) %>% 
  lapply(function(i) imdb_test[i]) %>% 
  purrr::transpose()

# num_words + padding + start + oov token = 10000 + 3
x_train_1h <- one_hot(train$x, 10000 + 3)
x_test_1h <- one_hot(test$x, 10000 + 3)
dim(x_train_1h)


set.seed(3)
ival <- sample(seq(along = train$y), 2000)
itrain <- seq_along(train$y)[-ival]
```

Fit a fully-connected neural network with two hidden layers, each with 16 units and ReLU activation to the data with dictionary size 1000.

```{r}
model <- nn_module(
  initialize = function(input_size = 10000 + 3) {
    self$dense1 <- nn_linear(input_size, 16)
    self$relu <- nn_relu()
    self$dense2 <- nn_linear(16, 16)
    self$output <- nn_linear(16, 1)
  },
  forward = function(x) {
    x %>% 
      self$dense1() %>% 
      self$relu() %>% 
      self$dense2() %>% 
      self$relu() %>% 
      self$output() %>% 
      torch_flatten(start_dim = 1)
  }
)
model <- model %>% 
  setup(
    loss = nn_bce_with_logits_loss(),
    optimizer = optim_rmsprop,
    metrics = list(luz_metric_binary_accuracy_with_logits())
  ) %>% 
  set_opt_hparams(lr = 0.001)

fitted <- model %>% 
  fit(
    # we transform the training and validation data into torch tensors
    list(
      torch_tensor(as.matrix(x_train_1h[itrain,]), dtype = torch_float()), 
      torch_tensor(unlist(train$y[itrain]))
    ),
    valid_data = list(
      torch_tensor(as.matrix(x_train_1h[ival, ]), dtype = torch_float()), 
      torch_tensor(unlist(train$y[ival]))
    ),
    dataloader_options = list(batch_size = 512),
    epochs = 10
  )

plot(fitted) 


fitted <- model %>% 
  fit(
    list(
      torch_tensor(as.matrix(x_train_1h[itrain,]), dtype = torch_float()), 
      torch_tensor(unlist(train$y[itrain]))
    ),
    valid_data = list(
      torch_tensor(as.matrix(x_test_1h), dtype = torch_float()), 
      torch_tensor(unlist(test$y))
    ),
    dataloader_options = list(batch_size = 512),
    epochs = 10
  )
```

Consider the effects of varying the dictionary size. Try the values 500, 1000, 3000, 5000, and 10,000, and compare the results.
```{r}

```


