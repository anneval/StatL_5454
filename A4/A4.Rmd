---
title: "Statistical Learning (5454) - Assignment 4"  
author: "Matthias Hochholzer, Lukas Pirnbacher, Anne Valder"
date: "Due: 2024-06-10"
output: 
  pdf_document: 
    keep_tex: yes
header-includes:
   - \usepackage{titlesec}
   - \titleformat*{\section}{\normalfont\Large\bfseries\flushleft}
   - \titleformat*{\subsection}{\normalfont\large\bfseries\flushleft}
   - \titleformat*{\subsubsection}{\normalfont\normalsize\bfseries\flushleft}
   - \usepackage{amsmath}
   - \newcommand*{\defeq}{\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}=}
   - \newcommand*{\eqdef}{=\mathrel{\vcenter{\baselineskip0.5ex \lineskiplimit0pt
                     \hbox{\scriptsize.}\hbox{\scriptsize.}}}}
                     
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE)
```


```{r,include=FALSE}
# Clear memory
rm(list=ls())
```

```{r, echo=FALSE, include=FALSE, warning=FALSE}

```



<!-- EXERCISE 1 -->
# Exercise 1

We generate data from the following additive error model $Y = f(X_1,X_2) + \epsilon$. The sum of sigmoids is
$$ Y = \sigma(a_1^\top X_1) + \sigma(a_2^\top X_2) + \epsilon, $$
with $a_1 = (3, 3)$, $a_2 = (3,−3)$.
– Each $X_j$ , $j = 1, 2$, is a standard Gaussian variate with $p = 2$.
– $\epsilon$ is an independent Gaussian error with variance chosen such that the signal-to-noise ratio as measured
by the respective variances equals four.

We Generate a training set of size 100 and a test sample of size 10,000.
```{r}

```


we then fit a neural networks with weight decay of 0.0005 and we vary the number of hidden units from 0 to 10 and
record the average test error $E_{Test}(Y − \hat{f}(X_1,X_2))^2$ for each of 10 random starting weights. I.e. for each of the settings with a different number of hidden units we fit a neural network 10 times with different initial values.

```{r}

```

Let's now visualize the results and interpret them.

```{r}

```


\newpage
<!-- EXERCISE 2 -->

# Exercise 2

The data sets \texit{zip.train} and \textit{zip.test} from package \textbf{ElemStatLearn} contain the information on the gray color
values of the pixels on a $16 \times 16$ pixel image of hand-written digits. We first visualize for each digit one randomly selected observation.

```{r}

```

we then fit a multinomial logistic regression model to the training data and evaluate it on the training and the test data. We determine the overall misclassification rate on the training and the test data and the digit-specific misclassification rates on the test data. 

```{r}

```

Which digits are the most difficult and the easiest to classify?

We add a positive weight decay of 0.05 when fitting the multinomial logistic regression model to the training data and evaluate the model on the training and the test data. We determine the overall misclassification rate on the training and the test data. 

```{r}

```


Explain why it makes sense to also include weight decay when fitting this multinomial logistic regression model.

\newpage
<!-- EXERCISE 3 -->
# Exercise 3

The data sets \textit{zip.train} and \textit{zip.test} from package \textbf{ElemStatLearn} contain the information on the gray color values of the pixels on a $16 \times 16$ pixel image of hand-written digits. Use only a subset from \textit{zip.train} of size 320 observations with an equal number of observations for each
digit to fit a multinomial logistic regression model and a neural network.

```{r}

```

Use all remaining training observations and the test data set to evaluate the fitted models.

```{r}

```

For this small training data overfitting is an issue. Visualize the performance on the test data in dependence of the training epochs when fitting the models.

```{r}

```



\newpage
<!-- EXERCISE 4 -->
# Exercise 4

In the following we will estimate a predictive model for the Default data from the \textbf{ISLR2} pacakge. We fit a neural network using a single hidden layer with 10 units and dropout regularization. We use the hint and use James et al. (2021, Chapter 10).

```{r}

```

We ompare the classification performance of this model with that of linear logistic regression.
```{r}

```


\newpage
<!-- EXERCISE 5 -->
# Exercise 5

Consider the \textit{IMDb} dataset from the \textbf{keras} package to perform document classification. Restrict the vocabulary to the most frequently-used words and tokens. Again, we use the hint and use James et al. (2021, Chapter 10).

```{r}

```

Fit a fully-connected neural network with two hidden layers, each with 16 units and ReLU activation to the data with dictionary size 1000.

```{r}

```

Consider the effects of varying the dictionary size. Try the values 500, 1000, 3000, 5000, and 10,000, and compare the results.
```{r}

```


